// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package v1

import (
	"context"
	"reflect"

	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfig struct {
	// The number of the accelerator cards of this type exposed to this instance.
	AcceleratorCount *int `pulumi:"acceleratorCount"`
	// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes).Examples: https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
	AcceleratorTypeUri *string `pulumi:"acceleratorTypeUri"`
}

// AcceleratorConfigInput is an input type that accepts AcceleratorConfigArgs and AcceleratorConfigOutput values.
// You can construct a concrete instance of `AcceleratorConfigInput` via:
//
//          AcceleratorConfigArgs{...}
type AcceleratorConfigInput interface {
	pulumi.Input

	ToAcceleratorConfigOutput() AcceleratorConfigOutput
	ToAcceleratorConfigOutputWithContext(context.Context) AcceleratorConfigOutput
}

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfigArgs struct {
	// The number of the accelerator cards of this type exposed to this instance.
	AcceleratorCount pulumi.IntPtrInput `pulumi:"acceleratorCount"`
	// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes).Examples: https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
	AcceleratorTypeUri pulumi.StringPtrInput `pulumi:"acceleratorTypeUri"`
}

func (AcceleratorConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*AcceleratorConfig)(nil)).Elem()
}

func (i AcceleratorConfigArgs) ToAcceleratorConfigOutput() AcceleratorConfigOutput {
	return i.ToAcceleratorConfigOutputWithContext(context.Background())
}

func (i AcceleratorConfigArgs) ToAcceleratorConfigOutputWithContext(ctx context.Context) AcceleratorConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AcceleratorConfigOutput)
}

// AcceleratorConfigArrayInput is an input type that accepts AcceleratorConfigArray and AcceleratorConfigArrayOutput values.
// You can construct a concrete instance of `AcceleratorConfigArrayInput` via:
//
//          AcceleratorConfigArray{ AcceleratorConfigArgs{...} }
type AcceleratorConfigArrayInput interface {
	pulumi.Input

	ToAcceleratorConfigArrayOutput() AcceleratorConfigArrayOutput
	ToAcceleratorConfigArrayOutputWithContext(context.Context) AcceleratorConfigArrayOutput
}

type AcceleratorConfigArray []AcceleratorConfigInput

func (AcceleratorConfigArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]AcceleratorConfig)(nil)).Elem()
}

func (i AcceleratorConfigArray) ToAcceleratorConfigArrayOutput() AcceleratorConfigArrayOutput {
	return i.ToAcceleratorConfigArrayOutputWithContext(context.Background())
}

func (i AcceleratorConfigArray) ToAcceleratorConfigArrayOutputWithContext(ctx context.Context) AcceleratorConfigArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AcceleratorConfigArrayOutput)
}

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfigOutput struct{ *pulumi.OutputState }

func (AcceleratorConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AcceleratorConfig)(nil)).Elem()
}

func (o AcceleratorConfigOutput) ToAcceleratorConfigOutput() AcceleratorConfigOutput {
	return o
}

func (o AcceleratorConfigOutput) ToAcceleratorConfigOutputWithContext(ctx context.Context) AcceleratorConfigOutput {
	return o
}

// The number of the accelerator cards of this type exposed to this instance.
func (o AcceleratorConfigOutput) AcceleratorCount() pulumi.IntPtrOutput {
	return o.ApplyT(func(v AcceleratorConfig) *int { return v.AcceleratorCount }).(pulumi.IntPtrOutput)
}

// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes).Examples: https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
func (o AcceleratorConfigOutput) AcceleratorTypeUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v AcceleratorConfig) *string { return v.AcceleratorTypeUri }).(pulumi.StringPtrOutput)
}

type AcceleratorConfigArrayOutput struct{ *pulumi.OutputState }

func (AcceleratorConfigArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]AcceleratorConfig)(nil)).Elem()
}

func (o AcceleratorConfigArrayOutput) ToAcceleratorConfigArrayOutput() AcceleratorConfigArrayOutput {
	return o
}

func (o AcceleratorConfigArrayOutput) ToAcceleratorConfigArrayOutputWithContext(ctx context.Context) AcceleratorConfigArrayOutput {
	return o
}

func (o AcceleratorConfigArrayOutput) Index(i pulumi.IntInput) AcceleratorConfigOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) AcceleratorConfig {
		return vs[0].([]AcceleratorConfig)[vs[1].(int)]
	}).(AcceleratorConfigOutput)
}

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfigResponse struct {
	// The number of the accelerator cards of this type exposed to this instance.
	AcceleratorCount int `pulumi:"acceleratorCount"`
	// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes).Examples: https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
	AcceleratorTypeUri string `pulumi:"acceleratorTypeUri"`
}

// Specifies the type and number of accelerator cards attached to the instances of an instance. See GPUs on Compute Engine (https://cloud.google.com/compute/docs/gpus/).
type AcceleratorConfigResponseOutput struct{ *pulumi.OutputState }

func (AcceleratorConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AcceleratorConfigResponse)(nil)).Elem()
}

func (o AcceleratorConfigResponseOutput) ToAcceleratorConfigResponseOutput() AcceleratorConfigResponseOutput {
	return o
}

func (o AcceleratorConfigResponseOutput) ToAcceleratorConfigResponseOutputWithContext(ctx context.Context) AcceleratorConfigResponseOutput {
	return o
}

// The number of the accelerator cards of this type exposed to this instance.
func (o AcceleratorConfigResponseOutput) AcceleratorCount() pulumi.IntOutput {
	return o.ApplyT(func(v AcceleratorConfigResponse) int { return v.AcceleratorCount }).(pulumi.IntOutput)
}

// Full URL, partial URI, or short name of the accelerator type resource to expose to this instance. See Compute Engine AcceleratorTypes (https://cloud.google.com/compute/docs/reference/beta/acceleratorTypes).Examples: https://www.googleapis.com/compute/beta/projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 projects/[project_id]/zones/us-east1-a/acceleratorTypes/nvidia-tesla-k80 nvidia-tesla-k80Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the accelerator type resource, for example, nvidia-tesla-k80.
func (o AcceleratorConfigResponseOutput) AcceleratorTypeUri() pulumi.StringOutput {
	return o.ApplyT(func(v AcceleratorConfigResponse) string { return v.AcceleratorTypeUri }).(pulumi.StringOutput)
}

type AcceleratorConfigResponseArrayOutput struct{ *pulumi.OutputState }

func (AcceleratorConfigResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]AcceleratorConfigResponse)(nil)).Elem()
}

func (o AcceleratorConfigResponseArrayOutput) ToAcceleratorConfigResponseArrayOutput() AcceleratorConfigResponseArrayOutput {
	return o
}

func (o AcceleratorConfigResponseArrayOutput) ToAcceleratorConfigResponseArrayOutputWithContext(ctx context.Context) AcceleratorConfigResponseArrayOutput {
	return o
}

func (o AcceleratorConfigResponseArrayOutput) Index(i pulumi.IntInput) AcceleratorConfigResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) AcceleratorConfigResponse {
		return vs[0].([]AcceleratorConfigResponse)[vs[1].(int)]
	}).(AcceleratorConfigResponseOutput)
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfig struct {
	// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
	PolicyUri *string `pulumi:"policyUri"`
}

// AutoscalingConfigInput is an input type that accepts AutoscalingConfigArgs and AutoscalingConfigOutput values.
// You can construct a concrete instance of `AutoscalingConfigInput` via:
//
//          AutoscalingConfigArgs{...}
type AutoscalingConfigInput interface {
	pulumi.Input

	ToAutoscalingConfigOutput() AutoscalingConfigOutput
	ToAutoscalingConfigOutputWithContext(context.Context) AutoscalingConfigOutput
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfigArgs struct {
	// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
	PolicyUri pulumi.StringPtrInput `pulumi:"policyUri"`
}

func (AutoscalingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*AutoscalingConfig)(nil)).Elem()
}

func (i AutoscalingConfigArgs) ToAutoscalingConfigOutput() AutoscalingConfigOutput {
	return i.ToAutoscalingConfigOutputWithContext(context.Background())
}

func (i AutoscalingConfigArgs) ToAutoscalingConfigOutputWithContext(ctx context.Context) AutoscalingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AutoscalingConfigOutput)
}

func (i AutoscalingConfigArgs) ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput {
	return i.ToAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i AutoscalingConfigArgs) ToAutoscalingConfigPtrOutputWithContext(ctx context.Context) AutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AutoscalingConfigOutput).ToAutoscalingConfigPtrOutputWithContext(ctx)
}

// AutoscalingConfigPtrInput is an input type that accepts AutoscalingConfigArgs, AutoscalingConfigPtr and AutoscalingConfigPtrOutput values.
// You can construct a concrete instance of `AutoscalingConfigPtrInput` via:
//
//          AutoscalingConfigArgs{...}
//
//  or:
//
//          nil
type AutoscalingConfigPtrInput interface {
	pulumi.Input

	ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput
	ToAutoscalingConfigPtrOutputWithContext(context.Context) AutoscalingConfigPtrOutput
}

type autoscalingConfigPtrType AutoscalingConfigArgs

func AutoscalingConfigPtr(v *AutoscalingConfigArgs) AutoscalingConfigPtrInput {
	return (*autoscalingConfigPtrType)(v)
}

func (*autoscalingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**AutoscalingConfig)(nil)).Elem()
}

func (i *autoscalingConfigPtrType) ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput {
	return i.ToAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i *autoscalingConfigPtrType) ToAutoscalingConfigPtrOutputWithContext(ctx context.Context) AutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(AutoscalingConfigPtrOutput)
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfigOutput struct{ *pulumi.OutputState }

func (AutoscalingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AutoscalingConfig)(nil)).Elem()
}

func (o AutoscalingConfigOutput) ToAutoscalingConfigOutput() AutoscalingConfigOutput {
	return o
}

func (o AutoscalingConfigOutput) ToAutoscalingConfigOutputWithContext(ctx context.Context) AutoscalingConfigOutput {
	return o
}

func (o AutoscalingConfigOutput) ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput {
	return o.ToAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (o AutoscalingConfigOutput) ToAutoscalingConfigPtrOutputWithContext(ctx context.Context) AutoscalingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v AutoscalingConfig) *AutoscalingConfig {
		return &v
	}).(AutoscalingConfigPtrOutput)
}

// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
func (o AutoscalingConfigOutput) PolicyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v AutoscalingConfig) *string { return v.PolicyUri }).(pulumi.StringPtrOutput)
}

type AutoscalingConfigPtrOutput struct{ *pulumi.OutputState }

func (AutoscalingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**AutoscalingConfig)(nil)).Elem()
}

func (o AutoscalingConfigPtrOutput) ToAutoscalingConfigPtrOutput() AutoscalingConfigPtrOutput {
	return o
}

func (o AutoscalingConfigPtrOutput) ToAutoscalingConfigPtrOutputWithContext(ctx context.Context) AutoscalingConfigPtrOutput {
	return o
}

func (o AutoscalingConfigPtrOutput) Elem() AutoscalingConfigOutput {
	return o.ApplyT(func(v *AutoscalingConfig) AutoscalingConfig {
		if v != nil {
			return *v
		}
		var ret AutoscalingConfig
		return ret
	}).(AutoscalingConfigOutput)
}

// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
func (o AutoscalingConfigPtrOutput) PolicyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *AutoscalingConfig) *string {
		if v == nil {
			return nil
		}
		return v.PolicyUri
	}).(pulumi.StringPtrOutput)
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfigResponse struct {
	// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
	PolicyUri string `pulumi:"policyUri"`
}

// Autoscaling Policy config associated with the cluster.
type AutoscalingConfigResponseOutput struct{ *pulumi.OutputState }

func (AutoscalingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*AutoscalingConfigResponse)(nil)).Elem()
}

func (o AutoscalingConfigResponseOutput) ToAutoscalingConfigResponseOutput() AutoscalingConfigResponseOutput {
	return o
}

func (o AutoscalingConfigResponseOutput) ToAutoscalingConfigResponseOutputWithContext(ctx context.Context) AutoscalingConfigResponseOutput {
	return o
}

// Optional. The autoscaling policy used by the cluster.Only resource names including projectid and location (region) are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id] projects/[project_id]/locations/[dataproc_region]/autoscalingPolicies/[policy_id]Note that the policy must be in the same project and Dataproc region.
func (o AutoscalingConfigResponseOutput) PolicyUri() pulumi.StringOutput {
	return o.ApplyT(func(v AutoscalingConfigResponse) string { return v.PolicyUri }).(pulumi.StringOutput)
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithm struct {
	// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
	CooldownPeriod *string `pulumi:"cooldownPeriod"`
	// Optional. Spark Standalone autoscaling configuration
	SparkStandaloneConfig *SparkStandaloneAutoscalingConfig `pulumi:"sparkStandaloneConfig"`
	// Optional. YARN autoscaling configuration.
	YarnConfig *BasicYarnAutoscalingConfig `pulumi:"yarnConfig"`
}

// BasicAutoscalingAlgorithmInput is an input type that accepts BasicAutoscalingAlgorithmArgs and BasicAutoscalingAlgorithmOutput values.
// You can construct a concrete instance of `BasicAutoscalingAlgorithmInput` via:
//
//          BasicAutoscalingAlgorithmArgs{...}
type BasicAutoscalingAlgorithmInput interface {
	pulumi.Input

	ToBasicAutoscalingAlgorithmOutput() BasicAutoscalingAlgorithmOutput
	ToBasicAutoscalingAlgorithmOutputWithContext(context.Context) BasicAutoscalingAlgorithmOutput
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithmArgs struct {
	// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
	CooldownPeriod pulumi.StringPtrInput `pulumi:"cooldownPeriod"`
	// Optional. Spark Standalone autoscaling configuration
	SparkStandaloneConfig SparkStandaloneAutoscalingConfigPtrInput `pulumi:"sparkStandaloneConfig"`
	// Optional. YARN autoscaling configuration.
	YarnConfig BasicYarnAutoscalingConfigPtrInput `pulumi:"yarnConfig"`
}

func (BasicAutoscalingAlgorithmArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicAutoscalingAlgorithm)(nil)).Elem()
}

func (i BasicAutoscalingAlgorithmArgs) ToBasicAutoscalingAlgorithmOutput() BasicAutoscalingAlgorithmOutput {
	return i.ToBasicAutoscalingAlgorithmOutputWithContext(context.Background())
}

func (i BasicAutoscalingAlgorithmArgs) ToBasicAutoscalingAlgorithmOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicAutoscalingAlgorithmOutput)
}

func (i BasicAutoscalingAlgorithmArgs) ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput {
	return i.ToBasicAutoscalingAlgorithmPtrOutputWithContext(context.Background())
}

func (i BasicAutoscalingAlgorithmArgs) ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicAutoscalingAlgorithmOutput).ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx)
}

// BasicAutoscalingAlgorithmPtrInput is an input type that accepts BasicAutoscalingAlgorithmArgs, BasicAutoscalingAlgorithmPtr and BasicAutoscalingAlgorithmPtrOutput values.
// You can construct a concrete instance of `BasicAutoscalingAlgorithmPtrInput` via:
//
//          BasicAutoscalingAlgorithmArgs{...}
//
//  or:
//
//          nil
type BasicAutoscalingAlgorithmPtrInput interface {
	pulumi.Input

	ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput
	ToBasicAutoscalingAlgorithmPtrOutputWithContext(context.Context) BasicAutoscalingAlgorithmPtrOutput
}

type basicAutoscalingAlgorithmPtrType BasicAutoscalingAlgorithmArgs

func BasicAutoscalingAlgorithmPtr(v *BasicAutoscalingAlgorithmArgs) BasicAutoscalingAlgorithmPtrInput {
	return (*basicAutoscalingAlgorithmPtrType)(v)
}

func (*basicAutoscalingAlgorithmPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**BasicAutoscalingAlgorithm)(nil)).Elem()
}

func (i *basicAutoscalingAlgorithmPtrType) ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput {
	return i.ToBasicAutoscalingAlgorithmPtrOutputWithContext(context.Background())
}

func (i *basicAutoscalingAlgorithmPtrType) ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicAutoscalingAlgorithmPtrOutput)
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithmOutput struct{ *pulumi.OutputState }

func (BasicAutoscalingAlgorithmOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicAutoscalingAlgorithm)(nil)).Elem()
}

func (o BasicAutoscalingAlgorithmOutput) ToBasicAutoscalingAlgorithmOutput() BasicAutoscalingAlgorithmOutput {
	return o
}

func (o BasicAutoscalingAlgorithmOutput) ToBasicAutoscalingAlgorithmOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmOutput {
	return o
}

func (o BasicAutoscalingAlgorithmOutput) ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput {
	return o.ToBasicAutoscalingAlgorithmPtrOutputWithContext(context.Background())
}

func (o BasicAutoscalingAlgorithmOutput) ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v BasicAutoscalingAlgorithm) *BasicAutoscalingAlgorithm {
		return &v
	}).(BasicAutoscalingAlgorithmPtrOutput)
}

// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
func (o BasicAutoscalingAlgorithmOutput) CooldownPeriod() pulumi.StringPtrOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithm) *string { return v.CooldownPeriod }).(pulumi.StringPtrOutput)
}

// Optional. Spark Standalone autoscaling configuration
func (o BasicAutoscalingAlgorithmOutput) SparkStandaloneConfig() SparkStandaloneAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithm) *SparkStandaloneAutoscalingConfig { return v.SparkStandaloneConfig }).(SparkStandaloneAutoscalingConfigPtrOutput)
}

// Optional. YARN autoscaling configuration.
func (o BasicAutoscalingAlgorithmOutput) YarnConfig() BasicYarnAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithm) *BasicYarnAutoscalingConfig { return v.YarnConfig }).(BasicYarnAutoscalingConfigPtrOutput)
}

type BasicAutoscalingAlgorithmPtrOutput struct{ *pulumi.OutputState }

func (BasicAutoscalingAlgorithmPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**BasicAutoscalingAlgorithm)(nil)).Elem()
}

func (o BasicAutoscalingAlgorithmPtrOutput) ToBasicAutoscalingAlgorithmPtrOutput() BasicAutoscalingAlgorithmPtrOutput {
	return o
}

func (o BasicAutoscalingAlgorithmPtrOutput) ToBasicAutoscalingAlgorithmPtrOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmPtrOutput {
	return o
}

func (o BasicAutoscalingAlgorithmPtrOutput) Elem() BasicAutoscalingAlgorithmOutput {
	return o.ApplyT(func(v *BasicAutoscalingAlgorithm) BasicAutoscalingAlgorithm {
		if v != nil {
			return *v
		}
		var ret BasicAutoscalingAlgorithm
		return ret
	}).(BasicAutoscalingAlgorithmOutput)
}

// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
func (o BasicAutoscalingAlgorithmPtrOutput) CooldownPeriod() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *BasicAutoscalingAlgorithm) *string {
		if v == nil {
			return nil
		}
		return v.CooldownPeriod
	}).(pulumi.StringPtrOutput)
}

// Optional. Spark Standalone autoscaling configuration
func (o BasicAutoscalingAlgorithmPtrOutput) SparkStandaloneConfig() SparkStandaloneAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v *BasicAutoscalingAlgorithm) *SparkStandaloneAutoscalingConfig {
		if v == nil {
			return nil
		}
		return v.SparkStandaloneConfig
	}).(SparkStandaloneAutoscalingConfigPtrOutput)
}

// Optional. YARN autoscaling configuration.
func (o BasicAutoscalingAlgorithmPtrOutput) YarnConfig() BasicYarnAutoscalingConfigPtrOutput {
	return o.ApplyT(func(v *BasicAutoscalingAlgorithm) *BasicYarnAutoscalingConfig {
		if v == nil {
			return nil
		}
		return v.YarnConfig
	}).(BasicYarnAutoscalingConfigPtrOutput)
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithmResponse struct {
	// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
	CooldownPeriod string `pulumi:"cooldownPeriod"`
	// Optional. Spark Standalone autoscaling configuration
	SparkStandaloneConfig SparkStandaloneAutoscalingConfigResponse `pulumi:"sparkStandaloneConfig"`
	// Optional. YARN autoscaling configuration.
	YarnConfig BasicYarnAutoscalingConfigResponse `pulumi:"yarnConfig"`
}

// Basic algorithm for autoscaling.
type BasicAutoscalingAlgorithmResponseOutput struct{ *pulumi.OutputState }

func (BasicAutoscalingAlgorithmResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicAutoscalingAlgorithmResponse)(nil)).Elem()
}

func (o BasicAutoscalingAlgorithmResponseOutput) ToBasicAutoscalingAlgorithmResponseOutput() BasicAutoscalingAlgorithmResponseOutput {
	return o
}

func (o BasicAutoscalingAlgorithmResponseOutput) ToBasicAutoscalingAlgorithmResponseOutputWithContext(ctx context.Context) BasicAutoscalingAlgorithmResponseOutput {
	return o
}

// Optional. Duration between scaling events. A scaling period starts after the update operation from the previous event has completed.Bounds: 2m, 1d. Default: 2m.
func (o BasicAutoscalingAlgorithmResponseOutput) CooldownPeriod() pulumi.StringOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithmResponse) string { return v.CooldownPeriod }).(pulumi.StringOutput)
}

// Optional. Spark Standalone autoscaling configuration
func (o BasicAutoscalingAlgorithmResponseOutput) SparkStandaloneConfig() SparkStandaloneAutoscalingConfigResponseOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithmResponse) SparkStandaloneAutoscalingConfigResponse {
		return v.SparkStandaloneConfig
	}).(SparkStandaloneAutoscalingConfigResponseOutput)
}

// Optional. YARN autoscaling configuration.
func (o BasicAutoscalingAlgorithmResponseOutput) YarnConfig() BasicYarnAutoscalingConfigResponseOutput {
	return o.ApplyT(func(v BasicAutoscalingAlgorithmResponse) BasicYarnAutoscalingConfigResponse { return v.YarnConfig }).(BasicYarnAutoscalingConfigResponseOutput)
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfig struct {
	// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout string `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleDownFactor float64 `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction *float64 `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleUpFactor float64 `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction *float64 `pulumi:"scaleUpMinWorkerFraction"`
}

// BasicYarnAutoscalingConfigInput is an input type that accepts BasicYarnAutoscalingConfigArgs and BasicYarnAutoscalingConfigOutput values.
// You can construct a concrete instance of `BasicYarnAutoscalingConfigInput` via:
//
//          BasicYarnAutoscalingConfigArgs{...}
type BasicYarnAutoscalingConfigInput interface {
	pulumi.Input

	ToBasicYarnAutoscalingConfigOutput() BasicYarnAutoscalingConfigOutput
	ToBasicYarnAutoscalingConfigOutputWithContext(context.Context) BasicYarnAutoscalingConfigOutput
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfigArgs struct {
	// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout pulumi.StringInput `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleDownFactor pulumi.Float64Input `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction pulumi.Float64PtrInput `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleUpFactor pulumi.Float64Input `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction pulumi.Float64PtrInput `pulumi:"scaleUpMinWorkerFraction"`
}

func (BasicYarnAutoscalingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicYarnAutoscalingConfig)(nil)).Elem()
}

func (i BasicYarnAutoscalingConfigArgs) ToBasicYarnAutoscalingConfigOutput() BasicYarnAutoscalingConfigOutput {
	return i.ToBasicYarnAutoscalingConfigOutputWithContext(context.Background())
}

func (i BasicYarnAutoscalingConfigArgs) ToBasicYarnAutoscalingConfigOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicYarnAutoscalingConfigOutput)
}

func (i BasicYarnAutoscalingConfigArgs) ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput {
	return i.ToBasicYarnAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i BasicYarnAutoscalingConfigArgs) ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicYarnAutoscalingConfigOutput).ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx)
}

// BasicYarnAutoscalingConfigPtrInput is an input type that accepts BasicYarnAutoscalingConfigArgs, BasicYarnAutoscalingConfigPtr and BasicYarnAutoscalingConfigPtrOutput values.
// You can construct a concrete instance of `BasicYarnAutoscalingConfigPtrInput` via:
//
//          BasicYarnAutoscalingConfigArgs{...}
//
//  or:
//
//          nil
type BasicYarnAutoscalingConfigPtrInput interface {
	pulumi.Input

	ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput
	ToBasicYarnAutoscalingConfigPtrOutputWithContext(context.Context) BasicYarnAutoscalingConfigPtrOutput
}

type basicYarnAutoscalingConfigPtrType BasicYarnAutoscalingConfigArgs

func BasicYarnAutoscalingConfigPtr(v *BasicYarnAutoscalingConfigArgs) BasicYarnAutoscalingConfigPtrInput {
	return (*basicYarnAutoscalingConfigPtrType)(v)
}

func (*basicYarnAutoscalingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**BasicYarnAutoscalingConfig)(nil)).Elem()
}

func (i *basicYarnAutoscalingConfigPtrType) ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput {
	return i.ToBasicYarnAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i *basicYarnAutoscalingConfigPtrType) ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BasicYarnAutoscalingConfigPtrOutput)
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfigOutput struct{ *pulumi.OutputState }

func (BasicYarnAutoscalingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicYarnAutoscalingConfig)(nil)).Elem()
}

func (o BasicYarnAutoscalingConfigOutput) ToBasicYarnAutoscalingConfigOutput() BasicYarnAutoscalingConfigOutput {
	return o
}

func (o BasicYarnAutoscalingConfigOutput) ToBasicYarnAutoscalingConfigOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigOutput {
	return o
}

func (o BasicYarnAutoscalingConfigOutput) ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput {
	return o.ToBasicYarnAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (o BasicYarnAutoscalingConfigOutput) ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v BasicYarnAutoscalingConfig) *BasicYarnAutoscalingConfig {
		return &v
	}).(BasicYarnAutoscalingConfigPtrOutput)
}

// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o BasicYarnAutoscalingConfigOutput) GracefulDecommissionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) string { return v.GracefulDecommissionTimeout }).(pulumi.StringOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigOutput) ScaleDownFactor() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) float64 { return v.ScaleDownFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigOutput) ScaleDownMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) *float64 { return v.ScaleDownMinWorkerFraction }).(pulumi.Float64PtrOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigOutput) ScaleUpFactor() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) float64 { return v.ScaleUpFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigOutput) ScaleUpMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v BasicYarnAutoscalingConfig) *float64 { return v.ScaleUpMinWorkerFraction }).(pulumi.Float64PtrOutput)
}

type BasicYarnAutoscalingConfigPtrOutput struct{ *pulumi.OutputState }

func (BasicYarnAutoscalingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**BasicYarnAutoscalingConfig)(nil)).Elem()
}

func (o BasicYarnAutoscalingConfigPtrOutput) ToBasicYarnAutoscalingConfigPtrOutput() BasicYarnAutoscalingConfigPtrOutput {
	return o
}

func (o BasicYarnAutoscalingConfigPtrOutput) ToBasicYarnAutoscalingConfigPtrOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigPtrOutput {
	return o
}

func (o BasicYarnAutoscalingConfigPtrOutput) Elem() BasicYarnAutoscalingConfigOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) BasicYarnAutoscalingConfig {
		if v != nil {
			return *v
		}
		var ret BasicYarnAutoscalingConfig
		return ret
	}).(BasicYarnAutoscalingConfigOutput)
}

// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o BasicYarnAutoscalingConfigPtrOutput) GracefulDecommissionTimeout() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *string {
		if v == nil {
			return nil
		}
		return &v.GracefulDecommissionTimeout
	}).(pulumi.StringPtrOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigPtrOutput) ScaleDownFactor() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return &v.ScaleDownFactor
	}).(pulumi.Float64PtrOutput)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigPtrOutput) ScaleDownMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return v.ScaleDownMinWorkerFraction
	}).(pulumi.Float64PtrOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigPtrOutput) ScaleUpFactor() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return &v.ScaleUpFactor
	}).(pulumi.Float64PtrOutput)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigPtrOutput) ScaleUpMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *BasicYarnAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return v.ScaleUpMinWorkerFraction
	}).(pulumi.Float64PtrOutput)
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfigResponse struct {
	// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout string `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleDownFactor float64 `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction float64 `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
	ScaleUpFactor float64 `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction float64 `pulumi:"scaleUpMinWorkerFraction"`
}

// Basic autoscaling configurations for YARN.
type BasicYarnAutoscalingConfigResponseOutput struct{ *pulumi.OutputState }

func (BasicYarnAutoscalingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BasicYarnAutoscalingConfigResponse)(nil)).Elem()
}

func (o BasicYarnAutoscalingConfigResponseOutput) ToBasicYarnAutoscalingConfigResponseOutput() BasicYarnAutoscalingConfigResponseOutput {
	return o
}

func (o BasicYarnAutoscalingConfigResponseOutput) ToBasicYarnAutoscalingConfigResponseOutputWithContext(ctx context.Context) BasicYarnAutoscalingConfigResponseOutput {
	return o
}

// Timeout for YARN graceful decommissioning of Node Managers. Specifies the duration to wait for jobs to complete before forcefully removing workers (and potentially interrupting jobs). Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o BasicYarnAutoscalingConfigResponseOutput) GracefulDecommissionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) string { return v.GracefulDecommissionTimeout }).(pulumi.StringOutput)
}

// Fraction of average YARN pending memory in the last cooldown period for which to remove workers. A scale-down factor of 1 will result in scaling down so that there is no available memory remaining after the update (more aggressive scaling). A scale-down factor of 0 disables removing workers, which can be beneficial for autoscaling a single job. See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigResponseOutput) ScaleDownFactor() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) float64 { return v.ScaleDownFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigResponseOutput) ScaleDownMinWorkerFraction() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) float64 { return v.ScaleDownMinWorkerFraction }).(pulumi.Float64Output)
}

// Fraction of average YARN pending memory in the last cooldown period for which to add workers. A scale-up factor of 1.0 will result in scaling up so that there is no pending memory remaining after the update (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling). See How autoscaling works (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling#how_autoscaling_works) for more information.Bounds: 0.0, 1.0.
func (o BasicYarnAutoscalingConfigResponseOutput) ScaleUpFactor() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) float64 { return v.ScaleUpFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o BasicYarnAutoscalingConfigResponseOutput) ScaleUpMinWorkerFraction() pulumi.Float64Output {
	return o.ApplyT(func(v BasicYarnAutoscalingConfigResponse) float64 { return v.ScaleUpMinWorkerFraction }).(pulumi.Float64Output)
}

// Associates members, or principals, with a role.
type Binding struct {
	// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
	Condition *Expr `pulumi:"condition"`
	// Specifies the principals requesting access for a Cloud Platform resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com.
	Members []string `pulumi:"members"`
	// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
	Role *string `pulumi:"role"`
}

// BindingInput is an input type that accepts BindingArgs and BindingOutput values.
// You can construct a concrete instance of `BindingInput` via:
//
//          BindingArgs{...}
type BindingInput interface {
	pulumi.Input

	ToBindingOutput() BindingOutput
	ToBindingOutputWithContext(context.Context) BindingOutput
}

// Associates members, or principals, with a role.
type BindingArgs struct {
	// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
	Condition ExprPtrInput `pulumi:"condition"`
	// Specifies the principals requesting access for a Cloud Platform resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com.
	Members pulumi.StringArrayInput `pulumi:"members"`
	// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
	Role pulumi.StringPtrInput `pulumi:"role"`
}

func (BindingArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*Binding)(nil)).Elem()
}

func (i BindingArgs) ToBindingOutput() BindingOutput {
	return i.ToBindingOutputWithContext(context.Background())
}

func (i BindingArgs) ToBindingOutputWithContext(ctx context.Context) BindingOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BindingOutput)
}

// BindingArrayInput is an input type that accepts BindingArray and BindingArrayOutput values.
// You can construct a concrete instance of `BindingArrayInput` via:
//
//          BindingArray{ BindingArgs{...} }
type BindingArrayInput interface {
	pulumi.Input

	ToBindingArrayOutput() BindingArrayOutput
	ToBindingArrayOutputWithContext(context.Context) BindingArrayOutput
}

type BindingArray []BindingInput

func (BindingArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]Binding)(nil)).Elem()
}

func (i BindingArray) ToBindingArrayOutput() BindingArrayOutput {
	return i.ToBindingArrayOutputWithContext(context.Background())
}

func (i BindingArray) ToBindingArrayOutputWithContext(ctx context.Context) BindingArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(BindingArrayOutput)
}

// Associates members, or principals, with a role.
type BindingOutput struct{ *pulumi.OutputState }

func (BindingOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*Binding)(nil)).Elem()
}

func (o BindingOutput) ToBindingOutput() BindingOutput {
	return o
}

func (o BindingOutput) ToBindingOutputWithContext(ctx context.Context) BindingOutput {
	return o
}

// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
func (o BindingOutput) Condition() ExprPtrOutput {
	return o.ApplyT(func(v Binding) *Expr { return v.Condition }).(ExprPtrOutput)
}

// Specifies the principals requesting access for a Cloud Platform resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com.
func (o BindingOutput) Members() pulumi.StringArrayOutput {
	return o.ApplyT(func(v Binding) []string { return v.Members }).(pulumi.StringArrayOutput)
}

// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
func (o BindingOutput) Role() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Binding) *string { return v.Role }).(pulumi.StringPtrOutput)
}

type BindingArrayOutput struct{ *pulumi.OutputState }

func (BindingArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]Binding)(nil)).Elem()
}

func (o BindingArrayOutput) ToBindingArrayOutput() BindingArrayOutput {
	return o
}

func (o BindingArrayOutput) ToBindingArrayOutputWithContext(ctx context.Context) BindingArrayOutput {
	return o
}

func (o BindingArrayOutput) Index(i pulumi.IntInput) BindingOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) Binding {
		return vs[0].([]Binding)[vs[1].(int)]
	}).(BindingOutput)
}

// Associates members, or principals, with a role.
type BindingResponse struct {
	// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
	Condition ExprResponse `pulumi:"condition"`
	// Specifies the principals requesting access for a Cloud Platform resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com.
	Members []string `pulumi:"members"`
	// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
	Role string `pulumi:"role"`
}

// Associates members, or principals, with a role.
type BindingResponseOutput struct{ *pulumi.OutputState }

func (BindingResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*BindingResponse)(nil)).Elem()
}

func (o BindingResponseOutput) ToBindingResponseOutput() BindingResponseOutput {
	return o
}

func (o BindingResponseOutput) ToBindingResponseOutputWithContext(ctx context.Context) BindingResponseOutput {
	return o
}

// The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
func (o BindingResponseOutput) Condition() ExprResponseOutput {
	return o.ApplyT(func(v BindingResponse) ExprResponse { return v.Condition }).(ExprResponseOutput)
}

// Specifies the principals requesting access for a Cloud Platform resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a service account. For example, my-other-app@appspot.gserviceaccount.com. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com.
func (o BindingResponseOutput) Members() pulumi.StringArrayOutput {
	return o.ApplyT(func(v BindingResponse) []string { return v.Members }).(pulumi.StringArrayOutput)
}

// Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
func (o BindingResponseOutput) Role() pulumi.StringOutput {
	return o.ApplyT(func(v BindingResponse) string { return v.Role }).(pulumi.StringOutput)
}

type BindingResponseArrayOutput struct{ *pulumi.OutputState }

func (BindingResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]BindingResponse)(nil)).Elem()
}

func (o BindingResponseArrayOutput) ToBindingResponseArrayOutput() BindingResponseArrayOutput {
	return o
}

func (o BindingResponseArrayOutput) ToBindingResponseArrayOutputWithContext(ctx context.Context) BindingResponseArrayOutput {
	return o
}

func (o BindingResponseArrayOutput) Index(i pulumi.IntInput) BindingResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) BindingResponse {
		return vs[0].([]BindingResponse)[vs[1].(int)]
	}).(BindingResponseOutput)
}

// The cluster config.
type ClusterConfig struct {
	// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
	AutoscalingConfig *AutoscalingConfig `pulumi:"autoscalingConfig"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	ConfigBucket *string `pulumi:"configBucket"`
	// Optional. Encryption settings for the cluster.
	EncryptionConfig *EncryptionConfig `pulumi:"encryptionConfig"`
	// Optional. Port/endpoint configuration for this cluster
	EndpointConfig *EndpointConfig `pulumi:"endpointConfig"`
	// Optional. The shared Compute Engine config settings for all instances in a cluster.
	GceClusterConfig *GceClusterConfig `pulumi:"gceClusterConfig"`
	// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
	GkeClusterConfig *GkeClusterConfig `pulumi:"gkeClusterConfig"`
	// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
	InitializationActions []NodeInitializationAction `pulumi:"initializationActions"`
	// Optional. Lifecycle setting for the cluster.
	LifecycleConfig *LifecycleConfig `pulumi:"lifecycleConfig"`
	// Optional. The Compute Engine config settings for the cluster's master instance.
	MasterConfig *InstanceGroupConfig `pulumi:"masterConfig"`
	// Optional. Metastore configuration.
	MetastoreConfig *MetastoreConfig `pulumi:"metastoreConfig"`
	// Optional. The Compute Engine config settings for a cluster's secondary worker instances
	SecondaryWorkerConfig *InstanceGroupConfig `pulumi:"secondaryWorkerConfig"`
	// Optional. Security settings for the cluster.
	SecurityConfig *SecurityConfig `pulumi:"securityConfig"`
	// Optional. The config settings for cluster software.
	SoftwareConfig *SoftwareConfig `pulumi:"softwareConfig"`
	// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	TempBucket *string `pulumi:"tempBucket"`
	// Optional. The Compute Engine config settings for the cluster's worker instances.
	WorkerConfig *InstanceGroupConfig `pulumi:"workerConfig"`
}

// ClusterConfigInput is an input type that accepts ClusterConfigArgs and ClusterConfigOutput values.
// You can construct a concrete instance of `ClusterConfigInput` via:
//
//          ClusterConfigArgs{...}
type ClusterConfigInput interface {
	pulumi.Input

	ToClusterConfigOutput() ClusterConfigOutput
	ToClusterConfigOutputWithContext(context.Context) ClusterConfigOutput
}

// The cluster config.
type ClusterConfigArgs struct {
	// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
	AutoscalingConfig AutoscalingConfigPtrInput `pulumi:"autoscalingConfig"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	ConfigBucket pulumi.StringPtrInput `pulumi:"configBucket"`
	// Optional. Encryption settings for the cluster.
	EncryptionConfig EncryptionConfigPtrInput `pulumi:"encryptionConfig"`
	// Optional. Port/endpoint configuration for this cluster
	EndpointConfig EndpointConfigPtrInput `pulumi:"endpointConfig"`
	// Optional. The shared Compute Engine config settings for all instances in a cluster.
	GceClusterConfig GceClusterConfigPtrInput `pulumi:"gceClusterConfig"`
	// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
	GkeClusterConfig GkeClusterConfigPtrInput `pulumi:"gkeClusterConfig"`
	// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
	InitializationActions NodeInitializationActionArrayInput `pulumi:"initializationActions"`
	// Optional. Lifecycle setting for the cluster.
	LifecycleConfig LifecycleConfigPtrInput `pulumi:"lifecycleConfig"`
	// Optional. The Compute Engine config settings for the cluster's master instance.
	MasterConfig InstanceGroupConfigPtrInput `pulumi:"masterConfig"`
	// Optional. Metastore configuration.
	MetastoreConfig MetastoreConfigPtrInput `pulumi:"metastoreConfig"`
	// Optional. The Compute Engine config settings for a cluster's secondary worker instances
	SecondaryWorkerConfig InstanceGroupConfigPtrInput `pulumi:"secondaryWorkerConfig"`
	// Optional. Security settings for the cluster.
	SecurityConfig SecurityConfigPtrInput `pulumi:"securityConfig"`
	// Optional. The config settings for cluster software.
	SoftwareConfig SoftwareConfigPtrInput `pulumi:"softwareConfig"`
	// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	TempBucket pulumi.StringPtrInput `pulumi:"tempBucket"`
	// Optional. The Compute Engine config settings for the cluster's worker instances.
	WorkerConfig InstanceGroupConfigPtrInput `pulumi:"workerConfig"`
}

func (ClusterConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterConfig)(nil)).Elem()
}

func (i ClusterConfigArgs) ToClusterConfigOutput() ClusterConfigOutput {
	return i.ToClusterConfigOutputWithContext(context.Background())
}

func (i ClusterConfigArgs) ToClusterConfigOutputWithContext(ctx context.Context) ClusterConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterConfigOutput)
}

func (i ClusterConfigArgs) ToClusterConfigPtrOutput() ClusterConfigPtrOutput {
	return i.ToClusterConfigPtrOutputWithContext(context.Background())
}

func (i ClusterConfigArgs) ToClusterConfigPtrOutputWithContext(ctx context.Context) ClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterConfigOutput).ToClusterConfigPtrOutputWithContext(ctx)
}

// ClusterConfigPtrInput is an input type that accepts ClusterConfigArgs, ClusterConfigPtr and ClusterConfigPtrOutput values.
// You can construct a concrete instance of `ClusterConfigPtrInput` via:
//
//          ClusterConfigArgs{...}
//
//  or:
//
//          nil
type ClusterConfigPtrInput interface {
	pulumi.Input

	ToClusterConfigPtrOutput() ClusterConfigPtrOutput
	ToClusterConfigPtrOutputWithContext(context.Context) ClusterConfigPtrOutput
}

type clusterConfigPtrType ClusterConfigArgs

func ClusterConfigPtr(v *ClusterConfigArgs) ClusterConfigPtrInput {
	return (*clusterConfigPtrType)(v)
}

func (*clusterConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ClusterConfig)(nil)).Elem()
}

func (i *clusterConfigPtrType) ToClusterConfigPtrOutput() ClusterConfigPtrOutput {
	return i.ToClusterConfigPtrOutputWithContext(context.Background())
}

func (i *clusterConfigPtrType) ToClusterConfigPtrOutputWithContext(ctx context.Context) ClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterConfigPtrOutput)
}

// The cluster config.
type ClusterConfigOutput struct{ *pulumi.OutputState }

func (ClusterConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterConfig)(nil)).Elem()
}

func (o ClusterConfigOutput) ToClusterConfigOutput() ClusterConfigOutput {
	return o
}

func (o ClusterConfigOutput) ToClusterConfigOutputWithContext(ctx context.Context) ClusterConfigOutput {
	return o
}

func (o ClusterConfigOutput) ToClusterConfigPtrOutput() ClusterConfigPtrOutput {
	return o.ToClusterConfigPtrOutputWithContext(context.Background())
}

func (o ClusterConfigOutput) ToClusterConfigPtrOutputWithContext(ctx context.Context) ClusterConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ClusterConfig) *ClusterConfig {
		return &v
	}).(ClusterConfigPtrOutput)
}

// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
func (o ClusterConfigOutput) AutoscalingConfig() AutoscalingConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *AutoscalingConfig { return v.AutoscalingConfig }).(AutoscalingConfigPtrOutput)
}

// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigOutput) ConfigBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *string { return v.ConfigBucket }).(pulumi.StringPtrOutput)
}

// Optional. Encryption settings for the cluster.
func (o ClusterConfigOutput) EncryptionConfig() EncryptionConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *EncryptionConfig { return v.EncryptionConfig }).(EncryptionConfigPtrOutput)
}

// Optional. Port/endpoint configuration for this cluster
func (o ClusterConfigOutput) EndpointConfig() EndpointConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *EndpointConfig { return v.EndpointConfig }).(EndpointConfigPtrOutput)
}

// Optional. The shared Compute Engine config settings for all instances in a cluster.
func (o ClusterConfigOutput) GceClusterConfig() GceClusterConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *GceClusterConfig { return v.GceClusterConfig }).(GceClusterConfigPtrOutput)
}

// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
func (o ClusterConfigOutput) GkeClusterConfig() GkeClusterConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *GkeClusterConfig { return v.GkeClusterConfig }).(GkeClusterConfigPtrOutput)
}

// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
func (o ClusterConfigOutput) InitializationActions() NodeInitializationActionArrayOutput {
	return o.ApplyT(func(v ClusterConfig) []NodeInitializationAction { return v.InitializationActions }).(NodeInitializationActionArrayOutput)
}

// Optional. Lifecycle setting for the cluster.
func (o ClusterConfigOutput) LifecycleConfig() LifecycleConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *LifecycleConfig { return v.LifecycleConfig }).(LifecycleConfigPtrOutput)
}

// Optional. The Compute Engine config settings for the cluster's master instance.
func (o ClusterConfigOutput) MasterConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *InstanceGroupConfig { return v.MasterConfig }).(InstanceGroupConfigPtrOutput)
}

// Optional. Metastore configuration.
func (o ClusterConfigOutput) MetastoreConfig() MetastoreConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *MetastoreConfig { return v.MetastoreConfig }).(MetastoreConfigPtrOutput)
}

// Optional. The Compute Engine config settings for a cluster's secondary worker instances
func (o ClusterConfigOutput) SecondaryWorkerConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *InstanceGroupConfig { return v.SecondaryWorkerConfig }).(InstanceGroupConfigPtrOutput)
}

// Optional. Security settings for the cluster.
func (o ClusterConfigOutput) SecurityConfig() SecurityConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *SecurityConfig { return v.SecurityConfig }).(SecurityConfigPtrOutput)
}

// Optional. The config settings for cluster software.
func (o ClusterConfigOutput) SoftwareConfig() SoftwareConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *SoftwareConfig { return v.SoftwareConfig }).(SoftwareConfigPtrOutput)
}

// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigOutput) TempBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *string { return v.TempBucket }).(pulumi.StringPtrOutput)
}

// Optional. The Compute Engine config settings for the cluster's worker instances.
func (o ClusterConfigOutput) WorkerConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v ClusterConfig) *InstanceGroupConfig { return v.WorkerConfig }).(InstanceGroupConfigPtrOutput)
}

type ClusterConfigPtrOutput struct{ *pulumi.OutputState }

func (ClusterConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ClusterConfig)(nil)).Elem()
}

func (o ClusterConfigPtrOutput) ToClusterConfigPtrOutput() ClusterConfigPtrOutput {
	return o
}

func (o ClusterConfigPtrOutput) ToClusterConfigPtrOutputWithContext(ctx context.Context) ClusterConfigPtrOutput {
	return o
}

func (o ClusterConfigPtrOutput) Elem() ClusterConfigOutput {
	return o.ApplyT(func(v *ClusterConfig) ClusterConfig {
		if v != nil {
			return *v
		}
		var ret ClusterConfig
		return ret
	}).(ClusterConfigOutput)
}

// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
func (o ClusterConfigPtrOutput) AutoscalingConfig() AutoscalingConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *AutoscalingConfig {
		if v == nil {
			return nil
		}
		return v.AutoscalingConfig
	}).(AutoscalingConfigPtrOutput)
}

// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigPtrOutput) ConfigBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.ConfigBucket
	}).(pulumi.StringPtrOutput)
}

// Optional. Encryption settings for the cluster.
func (o ClusterConfigPtrOutput) EncryptionConfig() EncryptionConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *EncryptionConfig {
		if v == nil {
			return nil
		}
		return v.EncryptionConfig
	}).(EncryptionConfigPtrOutput)
}

// Optional. Port/endpoint configuration for this cluster
func (o ClusterConfigPtrOutput) EndpointConfig() EndpointConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *EndpointConfig {
		if v == nil {
			return nil
		}
		return v.EndpointConfig
	}).(EndpointConfigPtrOutput)
}

// Optional. The shared Compute Engine config settings for all instances in a cluster.
func (o ClusterConfigPtrOutput) GceClusterConfig() GceClusterConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *GceClusterConfig {
		if v == nil {
			return nil
		}
		return v.GceClusterConfig
	}).(GceClusterConfigPtrOutput)
}

// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
func (o ClusterConfigPtrOutput) GkeClusterConfig() GkeClusterConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *GkeClusterConfig {
		if v == nil {
			return nil
		}
		return v.GkeClusterConfig
	}).(GkeClusterConfigPtrOutput)
}

// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
func (o ClusterConfigPtrOutput) InitializationActions() NodeInitializationActionArrayOutput {
	return o.ApplyT(func(v *ClusterConfig) []NodeInitializationAction {
		if v == nil {
			return nil
		}
		return v.InitializationActions
	}).(NodeInitializationActionArrayOutput)
}

// Optional. Lifecycle setting for the cluster.
func (o ClusterConfigPtrOutput) LifecycleConfig() LifecycleConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *LifecycleConfig {
		if v == nil {
			return nil
		}
		return v.LifecycleConfig
	}).(LifecycleConfigPtrOutput)
}

// Optional. The Compute Engine config settings for the cluster's master instance.
func (o ClusterConfigPtrOutput) MasterConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *InstanceGroupConfig {
		if v == nil {
			return nil
		}
		return v.MasterConfig
	}).(InstanceGroupConfigPtrOutput)
}

// Optional. Metastore configuration.
func (o ClusterConfigPtrOutput) MetastoreConfig() MetastoreConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *MetastoreConfig {
		if v == nil {
			return nil
		}
		return v.MetastoreConfig
	}).(MetastoreConfigPtrOutput)
}

// Optional. The Compute Engine config settings for a cluster's secondary worker instances
func (o ClusterConfigPtrOutput) SecondaryWorkerConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *InstanceGroupConfig {
		if v == nil {
			return nil
		}
		return v.SecondaryWorkerConfig
	}).(InstanceGroupConfigPtrOutput)
}

// Optional. Security settings for the cluster.
func (o ClusterConfigPtrOutput) SecurityConfig() SecurityConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *SecurityConfig {
		if v == nil {
			return nil
		}
		return v.SecurityConfig
	}).(SecurityConfigPtrOutput)
}

// Optional. The config settings for cluster software.
func (o ClusterConfigPtrOutput) SoftwareConfig() SoftwareConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *SoftwareConfig {
		if v == nil {
			return nil
		}
		return v.SoftwareConfig
	}).(SoftwareConfigPtrOutput)
}

// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigPtrOutput) TempBucket() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.TempBucket
	}).(pulumi.StringPtrOutput)
}

// Optional. The Compute Engine config settings for the cluster's worker instances.
func (o ClusterConfigPtrOutput) WorkerConfig() InstanceGroupConfigPtrOutput {
	return o.ApplyT(func(v *ClusterConfig) *InstanceGroupConfig {
		if v == nil {
			return nil
		}
		return v.WorkerConfig
	}).(InstanceGroupConfigPtrOutput)
}

// The cluster config.
type ClusterConfigResponse struct {
	// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
	AutoscalingConfig AutoscalingConfigResponse `pulumi:"autoscalingConfig"`
	// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	ConfigBucket string `pulumi:"configBucket"`
	// Optional. Encryption settings for the cluster.
	EncryptionConfig EncryptionConfigResponse `pulumi:"encryptionConfig"`
	// Optional. Port/endpoint configuration for this cluster
	EndpointConfig EndpointConfigResponse `pulumi:"endpointConfig"`
	// Optional. The shared Compute Engine config settings for all instances in a cluster.
	GceClusterConfig GceClusterConfigResponse `pulumi:"gceClusterConfig"`
	// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
	GkeClusterConfig GkeClusterConfigResponse `pulumi:"gkeClusterConfig"`
	// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
	InitializationActions []NodeInitializationActionResponse `pulumi:"initializationActions"`
	// Optional. Lifecycle setting for the cluster.
	LifecycleConfig LifecycleConfigResponse `pulumi:"lifecycleConfig"`
	// Optional. The Compute Engine config settings for the cluster's master instance.
	MasterConfig InstanceGroupConfigResponse `pulumi:"masterConfig"`
	// Optional. Metastore configuration.
	MetastoreConfig MetastoreConfigResponse `pulumi:"metastoreConfig"`
	// Optional. The Compute Engine config settings for a cluster's secondary worker instances
	SecondaryWorkerConfig InstanceGroupConfigResponse `pulumi:"secondaryWorkerConfig"`
	// Optional. Security settings for the cluster.
	SecurityConfig SecurityConfigResponse `pulumi:"securityConfig"`
	// Optional. The config settings for cluster software.
	SoftwareConfig SoftwareConfigResponse `pulumi:"softwareConfig"`
	// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
	TempBucket string `pulumi:"tempBucket"`
	// Optional. The Compute Engine config settings for the cluster's worker instances.
	WorkerConfig InstanceGroupConfigResponse `pulumi:"workerConfig"`
}

// The cluster config.
type ClusterConfigResponseOutput struct{ *pulumi.OutputState }

func (ClusterConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterConfigResponse)(nil)).Elem()
}

func (o ClusterConfigResponseOutput) ToClusterConfigResponseOutput() ClusterConfigResponseOutput {
	return o
}

func (o ClusterConfigResponseOutput) ToClusterConfigResponseOutputWithContext(ctx context.Context) ClusterConfigResponseOutput {
	return o
}

// Optional. Autoscaling config for the policy associated with the cluster. Cluster does not autoscale if this field is unset.
func (o ClusterConfigResponseOutput) AutoscalingConfig() AutoscalingConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) AutoscalingConfigResponse { return v.AutoscalingConfig }).(AutoscalingConfigResponseOutput)
}

// Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigResponseOutput) ConfigBucket() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterConfigResponse) string { return v.ConfigBucket }).(pulumi.StringOutput)
}

// Optional. Encryption settings for the cluster.
func (o ClusterConfigResponseOutput) EncryptionConfig() EncryptionConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) EncryptionConfigResponse { return v.EncryptionConfig }).(EncryptionConfigResponseOutput)
}

// Optional. Port/endpoint configuration for this cluster
func (o ClusterConfigResponseOutput) EndpointConfig() EndpointConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) EndpointConfigResponse { return v.EndpointConfig }).(EndpointConfigResponseOutput)
}

// Optional. The shared Compute Engine config settings for all instances in a cluster.
func (o ClusterConfigResponseOutput) GceClusterConfig() GceClusterConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) GceClusterConfigResponse { return v.GceClusterConfig }).(GceClusterConfigResponseOutput)
}

// Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to Kubernetes. Setting this is considered mutually exclusive with Compute Engine-based options such as gce_cluster_config, master_config, worker_config, secondary_worker_config, and autoscaling_config.
func (o ClusterConfigResponseOutput) GkeClusterConfig() GkeClusterConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) GkeClusterConfigResponse { return v.GkeClusterConfig }).(GkeClusterConfigResponseOutput)
}

// Optional. Commands to execute on each node after config is completed. By default, executables are run on master and all worker nodes. You can test a node's role metadata to run an executable on a master or worker node, as shown below using curl (you can also use wget): ROLE=$(curl -H Metadata-Flavor:Google http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" == 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
func (o ClusterConfigResponseOutput) InitializationActions() NodeInitializationActionResponseArrayOutput {
	return o.ApplyT(func(v ClusterConfigResponse) []NodeInitializationActionResponse { return v.InitializationActions }).(NodeInitializationActionResponseArrayOutput)
}

// Optional. Lifecycle setting for the cluster.
func (o ClusterConfigResponseOutput) LifecycleConfig() LifecycleConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) LifecycleConfigResponse { return v.LifecycleConfig }).(LifecycleConfigResponseOutput)
}

// Optional. The Compute Engine config settings for the cluster's master instance.
func (o ClusterConfigResponseOutput) MasterConfig() InstanceGroupConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) InstanceGroupConfigResponse { return v.MasterConfig }).(InstanceGroupConfigResponseOutput)
}

// Optional. Metastore configuration.
func (o ClusterConfigResponseOutput) MetastoreConfig() MetastoreConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) MetastoreConfigResponse { return v.MetastoreConfig }).(MetastoreConfigResponseOutput)
}

// Optional. The Compute Engine config settings for a cluster's secondary worker instances
func (o ClusterConfigResponseOutput) SecondaryWorkerConfig() InstanceGroupConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) InstanceGroupConfigResponse { return v.SecondaryWorkerConfig }).(InstanceGroupConfigResponseOutput)
}

// Optional. Security settings for the cluster.
func (o ClusterConfigResponseOutput) SecurityConfig() SecurityConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) SecurityConfigResponse { return v.SecurityConfig }).(SecurityConfigResponseOutput)
}

// Optional. The config settings for cluster software.
func (o ClusterConfigResponseOutput) SoftwareConfig() SoftwareConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) SoftwareConfigResponse { return v.SoftwareConfig }).(SoftwareConfigResponseOutput)
}

// Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the Compute Engine zone where your cluster is deployed, and then create and manage this project-level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL (or none) if you specify a bucket (see Dataproc staging and temp buckets (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
func (o ClusterConfigResponseOutput) TempBucket() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterConfigResponse) string { return v.TempBucket }).(pulumi.StringOutput)
}

// Optional. The Compute Engine config settings for the cluster's worker instances.
func (o ClusterConfigResponseOutput) WorkerConfig() InstanceGroupConfigResponseOutput {
	return o.ApplyT(func(v ClusterConfigResponse) InstanceGroupConfigResponse { return v.WorkerConfig }).(InstanceGroupConfigResponseOutput)
}

// Contains cluster daemon metrics, such as HDFS and YARN stats.Beta Feature: This report is available for testing purposes only. It may be changed before final release.
type ClusterMetricsResponse struct {
	// The HDFS metrics.
	HdfsMetrics map[string]string `pulumi:"hdfsMetrics"`
	// The YARN metrics.
	YarnMetrics map[string]string `pulumi:"yarnMetrics"`
}

// Contains cluster daemon metrics, such as HDFS and YARN stats.Beta Feature: This report is available for testing purposes only. It may be changed before final release.
type ClusterMetricsResponseOutput struct{ *pulumi.OutputState }

func (ClusterMetricsResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterMetricsResponse)(nil)).Elem()
}

func (o ClusterMetricsResponseOutput) ToClusterMetricsResponseOutput() ClusterMetricsResponseOutput {
	return o
}

func (o ClusterMetricsResponseOutput) ToClusterMetricsResponseOutputWithContext(ctx context.Context) ClusterMetricsResponseOutput {
	return o
}

// The HDFS metrics.
func (o ClusterMetricsResponseOutput) HdfsMetrics() pulumi.StringMapOutput {
	return o.ApplyT(func(v ClusterMetricsResponse) map[string]string { return v.HdfsMetrics }).(pulumi.StringMapOutput)
}

// The YARN metrics.
func (o ClusterMetricsResponseOutput) YarnMetrics() pulumi.StringMapOutput {
	return o.ApplyT(func(v ClusterMetricsResponse) map[string]string { return v.YarnMetrics }).(pulumi.StringMapOutput)
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelector struct {
	// The cluster labels. Cluster must have all labels to match.
	ClusterLabels map[string]string `pulumi:"clusterLabels"`
	// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
	Zone *string `pulumi:"zone"`
}

// ClusterSelectorInput is an input type that accepts ClusterSelectorArgs and ClusterSelectorOutput values.
// You can construct a concrete instance of `ClusterSelectorInput` via:
//
//          ClusterSelectorArgs{...}
type ClusterSelectorInput interface {
	pulumi.Input

	ToClusterSelectorOutput() ClusterSelectorOutput
	ToClusterSelectorOutputWithContext(context.Context) ClusterSelectorOutput
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelectorArgs struct {
	// The cluster labels. Cluster must have all labels to match.
	ClusterLabels pulumi.StringMapInput `pulumi:"clusterLabels"`
	// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
	Zone pulumi.StringPtrInput `pulumi:"zone"`
}

func (ClusterSelectorArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterSelector)(nil)).Elem()
}

func (i ClusterSelectorArgs) ToClusterSelectorOutput() ClusterSelectorOutput {
	return i.ToClusterSelectorOutputWithContext(context.Background())
}

func (i ClusterSelectorArgs) ToClusterSelectorOutputWithContext(ctx context.Context) ClusterSelectorOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterSelectorOutput)
}

func (i ClusterSelectorArgs) ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput {
	return i.ToClusterSelectorPtrOutputWithContext(context.Background())
}

func (i ClusterSelectorArgs) ToClusterSelectorPtrOutputWithContext(ctx context.Context) ClusterSelectorPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterSelectorOutput).ToClusterSelectorPtrOutputWithContext(ctx)
}

// ClusterSelectorPtrInput is an input type that accepts ClusterSelectorArgs, ClusterSelectorPtr and ClusterSelectorPtrOutput values.
// You can construct a concrete instance of `ClusterSelectorPtrInput` via:
//
//          ClusterSelectorArgs{...}
//
//  or:
//
//          nil
type ClusterSelectorPtrInput interface {
	pulumi.Input

	ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput
	ToClusterSelectorPtrOutputWithContext(context.Context) ClusterSelectorPtrOutput
}

type clusterSelectorPtrType ClusterSelectorArgs

func ClusterSelectorPtr(v *ClusterSelectorArgs) ClusterSelectorPtrInput {
	return (*clusterSelectorPtrType)(v)
}

func (*clusterSelectorPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ClusterSelector)(nil)).Elem()
}

func (i *clusterSelectorPtrType) ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput {
	return i.ToClusterSelectorPtrOutputWithContext(context.Background())
}

func (i *clusterSelectorPtrType) ToClusterSelectorPtrOutputWithContext(ctx context.Context) ClusterSelectorPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ClusterSelectorPtrOutput)
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelectorOutput struct{ *pulumi.OutputState }

func (ClusterSelectorOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterSelector)(nil)).Elem()
}

func (o ClusterSelectorOutput) ToClusterSelectorOutput() ClusterSelectorOutput {
	return o
}

func (o ClusterSelectorOutput) ToClusterSelectorOutputWithContext(ctx context.Context) ClusterSelectorOutput {
	return o
}

func (o ClusterSelectorOutput) ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput {
	return o.ToClusterSelectorPtrOutputWithContext(context.Background())
}

func (o ClusterSelectorOutput) ToClusterSelectorPtrOutputWithContext(ctx context.Context) ClusterSelectorPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ClusterSelector) *ClusterSelector {
		return &v
	}).(ClusterSelectorPtrOutput)
}

// The cluster labels. Cluster must have all labels to match.
func (o ClusterSelectorOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v ClusterSelector) map[string]string { return v.ClusterLabels }).(pulumi.StringMapOutput)
}

// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
func (o ClusterSelectorOutput) Zone() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ClusterSelector) *string { return v.Zone }).(pulumi.StringPtrOutput)
}

type ClusterSelectorPtrOutput struct{ *pulumi.OutputState }

func (ClusterSelectorPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ClusterSelector)(nil)).Elem()
}

func (o ClusterSelectorPtrOutput) ToClusterSelectorPtrOutput() ClusterSelectorPtrOutput {
	return o
}

func (o ClusterSelectorPtrOutput) ToClusterSelectorPtrOutputWithContext(ctx context.Context) ClusterSelectorPtrOutput {
	return o
}

func (o ClusterSelectorPtrOutput) Elem() ClusterSelectorOutput {
	return o.ApplyT(func(v *ClusterSelector) ClusterSelector {
		if v != nil {
			return *v
		}
		var ret ClusterSelector
		return ret
	}).(ClusterSelectorOutput)
}

// The cluster labels. Cluster must have all labels to match.
func (o ClusterSelectorPtrOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *ClusterSelector) map[string]string {
		if v == nil {
			return nil
		}
		return v.ClusterLabels
	}).(pulumi.StringMapOutput)
}

// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
func (o ClusterSelectorPtrOutput) Zone() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ClusterSelector) *string {
		if v == nil {
			return nil
		}
		return v.Zone
	}).(pulumi.StringPtrOutput)
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelectorResponse struct {
	// The cluster labels. Cluster must have all labels to match.
	ClusterLabels map[string]string `pulumi:"clusterLabels"`
	// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
	Zone string `pulumi:"zone"`
}

// A selector that chooses target cluster for jobs based on metadata.
type ClusterSelectorResponseOutput struct{ *pulumi.OutputState }

func (ClusterSelectorResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterSelectorResponse)(nil)).Elem()
}

func (o ClusterSelectorResponseOutput) ToClusterSelectorResponseOutput() ClusterSelectorResponseOutput {
	return o
}

func (o ClusterSelectorResponseOutput) ToClusterSelectorResponseOutputWithContext(ctx context.Context) ClusterSelectorResponseOutput {
	return o
}

// The cluster labels. Cluster must have all labels to match.
func (o ClusterSelectorResponseOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v ClusterSelectorResponse) map[string]string { return v.ClusterLabels }).(pulumi.StringMapOutput)
}

// Optional. The zone where workflow process executes. This parameter does not affect the selection of the cluster.If unspecified, the zone of the first cluster matching the selector is used.
func (o ClusterSelectorResponseOutput) Zone() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterSelectorResponse) string { return v.Zone }).(pulumi.StringOutput)
}

// The status of a cluster and its instances.
type ClusterStatusResponse struct {
	// Optional. Output only. Details of cluster's state.
	Detail string `pulumi:"detail"`
	// The cluster's state.
	State string `pulumi:"state"`
	// Time when this state was entered (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	StateStartTime string `pulumi:"stateStartTime"`
	// Additional state information that includes status reported by the agent.
	Substate string `pulumi:"substate"`
}

// The status of a cluster and its instances.
type ClusterStatusResponseOutput struct{ *pulumi.OutputState }

func (ClusterStatusResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ClusterStatusResponse)(nil)).Elem()
}

func (o ClusterStatusResponseOutput) ToClusterStatusResponseOutput() ClusterStatusResponseOutput {
	return o
}

func (o ClusterStatusResponseOutput) ToClusterStatusResponseOutputWithContext(ctx context.Context) ClusterStatusResponseOutput {
	return o
}

// Optional. Output only. Details of cluster's state.
func (o ClusterStatusResponseOutput) Detail() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterStatusResponse) string { return v.Detail }).(pulumi.StringOutput)
}

// The cluster's state.
func (o ClusterStatusResponseOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterStatusResponse) string { return v.State }).(pulumi.StringOutput)
}

// Time when this state was entered (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o ClusterStatusResponseOutput) StateStartTime() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterStatusResponse) string { return v.StateStartTime }).(pulumi.StringOutput)
}

// Additional state information that includes status reported by the agent.
func (o ClusterStatusResponseOutput) Substate() pulumi.StringOutput {
	return o.ApplyT(func(v ClusterStatusResponse) string { return v.Substate }).(pulumi.StringOutput)
}

type ClusterStatusResponseArrayOutput struct{ *pulumi.OutputState }

func (ClusterStatusResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]ClusterStatusResponse)(nil)).Elem()
}

func (o ClusterStatusResponseArrayOutput) ToClusterStatusResponseArrayOutput() ClusterStatusResponseArrayOutput {
	return o
}

func (o ClusterStatusResponseArrayOutput) ToClusterStatusResponseArrayOutputWithContext(ctx context.Context) ClusterStatusResponseArrayOutput {
	return o
}

func (o ClusterStatusResponseArrayOutput) Index(i pulumi.IntInput) ClusterStatusResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) ClusterStatusResponse {
		return vs[0].([]ClusterStatusResponse)[vs[1].(int)]
	}).(ClusterStatusResponseOutput)
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfig struct {
	// Optional. Defines whether the instance should have confidential compute enabled.
	EnableConfidentialCompute *bool `pulumi:"enableConfidentialCompute"`
}

// ConfidentialInstanceConfigInput is an input type that accepts ConfidentialInstanceConfigArgs and ConfidentialInstanceConfigOutput values.
// You can construct a concrete instance of `ConfidentialInstanceConfigInput` via:
//
//          ConfidentialInstanceConfigArgs{...}
type ConfidentialInstanceConfigInput interface {
	pulumi.Input

	ToConfidentialInstanceConfigOutput() ConfidentialInstanceConfigOutput
	ToConfidentialInstanceConfigOutputWithContext(context.Context) ConfidentialInstanceConfigOutput
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfigArgs struct {
	// Optional. Defines whether the instance should have confidential compute enabled.
	EnableConfidentialCompute pulumi.BoolPtrInput `pulumi:"enableConfidentialCompute"`
}

func (ConfidentialInstanceConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ConfidentialInstanceConfig)(nil)).Elem()
}

func (i ConfidentialInstanceConfigArgs) ToConfidentialInstanceConfigOutput() ConfidentialInstanceConfigOutput {
	return i.ToConfidentialInstanceConfigOutputWithContext(context.Background())
}

func (i ConfidentialInstanceConfigArgs) ToConfidentialInstanceConfigOutputWithContext(ctx context.Context) ConfidentialInstanceConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ConfidentialInstanceConfigOutput)
}

func (i ConfidentialInstanceConfigArgs) ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput {
	return i.ToConfidentialInstanceConfigPtrOutputWithContext(context.Background())
}

func (i ConfidentialInstanceConfigArgs) ToConfidentialInstanceConfigPtrOutputWithContext(ctx context.Context) ConfidentialInstanceConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ConfidentialInstanceConfigOutput).ToConfidentialInstanceConfigPtrOutputWithContext(ctx)
}

// ConfidentialInstanceConfigPtrInput is an input type that accepts ConfidentialInstanceConfigArgs, ConfidentialInstanceConfigPtr and ConfidentialInstanceConfigPtrOutput values.
// You can construct a concrete instance of `ConfidentialInstanceConfigPtrInput` via:
//
//          ConfidentialInstanceConfigArgs{...}
//
//  or:
//
//          nil
type ConfidentialInstanceConfigPtrInput interface {
	pulumi.Input

	ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput
	ToConfidentialInstanceConfigPtrOutputWithContext(context.Context) ConfidentialInstanceConfigPtrOutput
}

type confidentialInstanceConfigPtrType ConfidentialInstanceConfigArgs

func ConfidentialInstanceConfigPtr(v *ConfidentialInstanceConfigArgs) ConfidentialInstanceConfigPtrInput {
	return (*confidentialInstanceConfigPtrType)(v)
}

func (*confidentialInstanceConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ConfidentialInstanceConfig)(nil)).Elem()
}

func (i *confidentialInstanceConfigPtrType) ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput {
	return i.ToConfidentialInstanceConfigPtrOutputWithContext(context.Background())
}

func (i *confidentialInstanceConfigPtrType) ToConfidentialInstanceConfigPtrOutputWithContext(ctx context.Context) ConfidentialInstanceConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ConfidentialInstanceConfigPtrOutput)
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfigOutput struct{ *pulumi.OutputState }

func (ConfidentialInstanceConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ConfidentialInstanceConfig)(nil)).Elem()
}

func (o ConfidentialInstanceConfigOutput) ToConfidentialInstanceConfigOutput() ConfidentialInstanceConfigOutput {
	return o
}

func (o ConfidentialInstanceConfigOutput) ToConfidentialInstanceConfigOutputWithContext(ctx context.Context) ConfidentialInstanceConfigOutput {
	return o
}

func (o ConfidentialInstanceConfigOutput) ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput {
	return o.ToConfidentialInstanceConfigPtrOutputWithContext(context.Background())
}

func (o ConfidentialInstanceConfigOutput) ToConfidentialInstanceConfigPtrOutputWithContext(ctx context.Context) ConfidentialInstanceConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ConfidentialInstanceConfig) *ConfidentialInstanceConfig {
		return &v
	}).(ConfidentialInstanceConfigPtrOutput)
}

// Optional. Defines whether the instance should have confidential compute enabled.
func (o ConfidentialInstanceConfigOutput) EnableConfidentialCompute() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v ConfidentialInstanceConfig) *bool { return v.EnableConfidentialCompute }).(pulumi.BoolPtrOutput)
}

type ConfidentialInstanceConfigPtrOutput struct{ *pulumi.OutputState }

func (ConfidentialInstanceConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ConfidentialInstanceConfig)(nil)).Elem()
}

func (o ConfidentialInstanceConfigPtrOutput) ToConfidentialInstanceConfigPtrOutput() ConfidentialInstanceConfigPtrOutput {
	return o
}

func (o ConfidentialInstanceConfigPtrOutput) ToConfidentialInstanceConfigPtrOutputWithContext(ctx context.Context) ConfidentialInstanceConfigPtrOutput {
	return o
}

func (o ConfidentialInstanceConfigPtrOutput) Elem() ConfidentialInstanceConfigOutput {
	return o.ApplyT(func(v *ConfidentialInstanceConfig) ConfidentialInstanceConfig {
		if v != nil {
			return *v
		}
		var ret ConfidentialInstanceConfig
		return ret
	}).(ConfidentialInstanceConfigOutput)
}

// Optional. Defines whether the instance should have confidential compute enabled.
func (o ConfidentialInstanceConfigPtrOutput) EnableConfidentialCompute() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *ConfidentialInstanceConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableConfidentialCompute
	}).(pulumi.BoolPtrOutput)
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfigResponse struct {
	// Optional. Defines whether the instance should have confidential compute enabled.
	EnableConfidentialCompute bool `pulumi:"enableConfidentialCompute"`
}

// Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs)
type ConfidentialInstanceConfigResponseOutput struct{ *pulumi.OutputState }

func (ConfidentialInstanceConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ConfidentialInstanceConfigResponse)(nil)).Elem()
}

func (o ConfidentialInstanceConfigResponseOutput) ToConfidentialInstanceConfigResponseOutput() ConfidentialInstanceConfigResponseOutput {
	return o
}

func (o ConfidentialInstanceConfigResponseOutput) ToConfidentialInstanceConfigResponseOutputWithContext(ctx context.Context) ConfidentialInstanceConfigResponseOutput {
	return o
}

// Optional. Defines whether the instance should have confidential compute enabled.
func (o ConfidentialInstanceConfigResponseOutput) EnableConfidentialCompute() pulumi.BoolOutput {
	return o.ApplyT(func(v ConfidentialInstanceConfigResponse) bool { return v.EnableConfidentialCompute }).(pulumi.BoolOutput)
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfig struct {
	// Optional. Size in GB of the boot disk (default is 500GB).
	BootDiskSizeGb *int `pulumi:"bootDiskSizeGb"`
	// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
	BootDiskType *string `pulumi:"bootDiskType"`
	// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
	LocalSsdInterface *string `pulumi:"localSsdInterface"`
	// Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
	NumLocalSsds *int `pulumi:"numLocalSsds"`
}

// DiskConfigInput is an input type that accepts DiskConfigArgs and DiskConfigOutput values.
// You can construct a concrete instance of `DiskConfigInput` via:
//
//          DiskConfigArgs{...}
type DiskConfigInput interface {
	pulumi.Input

	ToDiskConfigOutput() DiskConfigOutput
	ToDiskConfigOutputWithContext(context.Context) DiskConfigOutput
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfigArgs struct {
	// Optional. Size in GB of the boot disk (default is 500GB).
	BootDiskSizeGb pulumi.IntPtrInput `pulumi:"bootDiskSizeGb"`
	// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
	BootDiskType pulumi.StringPtrInput `pulumi:"bootDiskType"`
	// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
	LocalSsdInterface pulumi.StringPtrInput `pulumi:"localSsdInterface"`
	// Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
	NumLocalSsds pulumi.IntPtrInput `pulumi:"numLocalSsds"`
}

func (DiskConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*DiskConfig)(nil)).Elem()
}

func (i DiskConfigArgs) ToDiskConfigOutput() DiskConfigOutput {
	return i.ToDiskConfigOutputWithContext(context.Background())
}

func (i DiskConfigArgs) ToDiskConfigOutputWithContext(ctx context.Context) DiskConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DiskConfigOutput)
}

func (i DiskConfigArgs) ToDiskConfigPtrOutput() DiskConfigPtrOutput {
	return i.ToDiskConfigPtrOutputWithContext(context.Background())
}

func (i DiskConfigArgs) ToDiskConfigPtrOutputWithContext(ctx context.Context) DiskConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DiskConfigOutput).ToDiskConfigPtrOutputWithContext(ctx)
}

// DiskConfigPtrInput is an input type that accepts DiskConfigArgs, DiskConfigPtr and DiskConfigPtrOutput values.
// You can construct a concrete instance of `DiskConfigPtrInput` via:
//
//          DiskConfigArgs{...}
//
//  or:
//
//          nil
type DiskConfigPtrInput interface {
	pulumi.Input

	ToDiskConfigPtrOutput() DiskConfigPtrOutput
	ToDiskConfigPtrOutputWithContext(context.Context) DiskConfigPtrOutput
}

type diskConfigPtrType DiskConfigArgs

func DiskConfigPtr(v *DiskConfigArgs) DiskConfigPtrInput {
	return (*diskConfigPtrType)(v)
}

func (*diskConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**DiskConfig)(nil)).Elem()
}

func (i *diskConfigPtrType) ToDiskConfigPtrOutput() DiskConfigPtrOutput {
	return i.ToDiskConfigPtrOutputWithContext(context.Background())
}

func (i *diskConfigPtrType) ToDiskConfigPtrOutputWithContext(ctx context.Context) DiskConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DiskConfigPtrOutput)
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfigOutput struct{ *pulumi.OutputState }

func (DiskConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*DiskConfig)(nil)).Elem()
}

func (o DiskConfigOutput) ToDiskConfigOutput() DiskConfigOutput {
	return o
}

func (o DiskConfigOutput) ToDiskConfigOutputWithContext(ctx context.Context) DiskConfigOutput {
	return o
}

func (o DiskConfigOutput) ToDiskConfigPtrOutput() DiskConfigPtrOutput {
	return o.ToDiskConfigPtrOutputWithContext(context.Background())
}

func (o DiskConfigOutput) ToDiskConfigPtrOutputWithContext(ctx context.Context) DiskConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v DiskConfig) *DiskConfig {
		return &v
	}).(DiskConfigPtrOutput)
}

// Optional. Size in GB of the boot disk (default is 500GB).
func (o DiskConfigOutput) BootDiskSizeGb() pulumi.IntPtrOutput {
	return o.ApplyT(func(v DiskConfig) *int { return v.BootDiskSizeGb }).(pulumi.IntPtrOutput)
}

// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
func (o DiskConfigOutput) BootDiskType() pulumi.StringPtrOutput {
	return o.ApplyT(func(v DiskConfig) *string { return v.BootDiskType }).(pulumi.StringPtrOutput)
}

// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
func (o DiskConfigOutput) LocalSsdInterface() pulumi.StringPtrOutput {
	return o.ApplyT(func(v DiskConfig) *string { return v.LocalSsdInterface }).(pulumi.StringPtrOutput)
}

// Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
func (o DiskConfigOutput) NumLocalSsds() pulumi.IntPtrOutput {
	return o.ApplyT(func(v DiskConfig) *int { return v.NumLocalSsds }).(pulumi.IntPtrOutput)
}

type DiskConfigPtrOutput struct{ *pulumi.OutputState }

func (DiskConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**DiskConfig)(nil)).Elem()
}

func (o DiskConfigPtrOutput) ToDiskConfigPtrOutput() DiskConfigPtrOutput {
	return o
}

func (o DiskConfigPtrOutput) ToDiskConfigPtrOutputWithContext(ctx context.Context) DiskConfigPtrOutput {
	return o
}

func (o DiskConfigPtrOutput) Elem() DiskConfigOutput {
	return o.ApplyT(func(v *DiskConfig) DiskConfig {
		if v != nil {
			return *v
		}
		var ret DiskConfig
		return ret
	}).(DiskConfigOutput)
}

// Optional. Size in GB of the boot disk (default is 500GB).
func (o DiskConfigPtrOutput) BootDiskSizeGb() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *DiskConfig) *int {
		if v == nil {
			return nil
		}
		return v.BootDiskSizeGb
	}).(pulumi.IntPtrOutput)
}

// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
func (o DiskConfigPtrOutput) BootDiskType() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *DiskConfig) *string {
		if v == nil {
			return nil
		}
		return v.BootDiskType
	}).(pulumi.StringPtrOutput)
}

// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
func (o DiskConfigPtrOutput) LocalSsdInterface() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *DiskConfig) *string {
		if v == nil {
			return nil
		}
		return v.LocalSsdInterface
	}).(pulumi.StringPtrOutput)
}

// Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
func (o DiskConfigPtrOutput) NumLocalSsds() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *DiskConfig) *int {
		if v == nil {
			return nil
		}
		return v.NumLocalSsds
	}).(pulumi.IntPtrOutput)
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfigResponse struct {
	// Optional. Size in GB of the boot disk (default is 500GB).
	BootDiskSizeGb int `pulumi:"bootDiskSizeGb"`
	// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
	BootDiskType string `pulumi:"bootDiskType"`
	// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
	LocalSsdInterface string `pulumi:"localSsdInterface"`
	// Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
	NumLocalSsds int `pulumi:"numLocalSsds"`
}

// Specifies the config of disk options for a group of VM instances.
type DiskConfigResponseOutput struct{ *pulumi.OutputState }

func (DiskConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*DiskConfigResponse)(nil)).Elem()
}

func (o DiskConfigResponseOutput) ToDiskConfigResponseOutput() DiskConfigResponseOutput {
	return o
}

func (o DiskConfigResponseOutput) ToDiskConfigResponseOutputWithContext(ctx context.Context) DiskConfigResponseOutput {
	return o
}

// Optional. Size in GB of the boot disk (default is 500GB).
func (o DiskConfigResponseOutput) BootDiskSizeGb() pulumi.IntOutput {
	return o.ApplyT(func(v DiskConfigResponse) int { return v.BootDiskSizeGb }).(pulumi.IntOutput)
}

// Optional. Type of the boot disk (default is "pd-standard"). Valid values: "pd-balanced" (Persistent Disk Balanced Solid State Drive), "pd-ssd" (Persistent Disk Solid State Drive), or "pd-standard" (Persistent Disk Hard Disk Drive). See Disk types (https://cloud.google.com/compute/docs/disks#disk-types).
func (o DiskConfigResponseOutput) BootDiskType() pulumi.StringOutput {
	return o.ApplyT(func(v DiskConfigResponse) string { return v.BootDiskType }).(pulumi.StringOutput)
}

// Optional. Interface type of local SSDs (default is "scsi"). Valid values: "scsi" (Small Computer System Interface), "nvme" (Non-Volatile Memory Express). See local SSD performance (https://cloud.google.com/compute/docs/disks/local-ssd#performance).
func (o DiskConfigResponseOutput) LocalSsdInterface() pulumi.StringOutput {
	return o.ApplyT(func(v DiskConfigResponse) string { return v.LocalSsdInterface }).(pulumi.StringOutput)
}

// Optional. Number of attached SSDs, from 0 to 4 (default is 0). If SSDs are not attached, the boot disk is used to store runtime logs and HDFS (https://hadoop.apache.org/docs/r1.2.1/hdfs_user_guide.html) data. If one or more SSDs are attached, this runtime bulk data is spread across them, and the boot disk contains only basic config and installed binaries.
func (o DiskConfigResponseOutput) NumLocalSsds() pulumi.IntOutput {
	return o.ApplyT(func(v DiskConfigResponse) int { return v.NumLocalSsds }).(pulumi.IntOutput)
}

// Encryption settings for the cluster.
type EncryptionConfig struct {
	// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
	GcePdKmsKeyName *string `pulumi:"gcePdKmsKeyName"`
}

// EncryptionConfigInput is an input type that accepts EncryptionConfigArgs and EncryptionConfigOutput values.
// You can construct a concrete instance of `EncryptionConfigInput` via:
//
//          EncryptionConfigArgs{...}
type EncryptionConfigInput interface {
	pulumi.Input

	ToEncryptionConfigOutput() EncryptionConfigOutput
	ToEncryptionConfigOutputWithContext(context.Context) EncryptionConfigOutput
}

// Encryption settings for the cluster.
type EncryptionConfigArgs struct {
	// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
	GcePdKmsKeyName pulumi.StringPtrInput `pulumi:"gcePdKmsKeyName"`
}

func (EncryptionConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*EncryptionConfig)(nil)).Elem()
}

func (i EncryptionConfigArgs) ToEncryptionConfigOutput() EncryptionConfigOutput {
	return i.ToEncryptionConfigOutputWithContext(context.Background())
}

func (i EncryptionConfigArgs) ToEncryptionConfigOutputWithContext(ctx context.Context) EncryptionConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EncryptionConfigOutput)
}

func (i EncryptionConfigArgs) ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput {
	return i.ToEncryptionConfigPtrOutputWithContext(context.Background())
}

func (i EncryptionConfigArgs) ToEncryptionConfigPtrOutputWithContext(ctx context.Context) EncryptionConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EncryptionConfigOutput).ToEncryptionConfigPtrOutputWithContext(ctx)
}

// EncryptionConfigPtrInput is an input type that accepts EncryptionConfigArgs, EncryptionConfigPtr and EncryptionConfigPtrOutput values.
// You can construct a concrete instance of `EncryptionConfigPtrInput` via:
//
//          EncryptionConfigArgs{...}
//
//  or:
//
//          nil
type EncryptionConfigPtrInput interface {
	pulumi.Input

	ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput
	ToEncryptionConfigPtrOutputWithContext(context.Context) EncryptionConfigPtrOutput
}

type encryptionConfigPtrType EncryptionConfigArgs

func EncryptionConfigPtr(v *EncryptionConfigArgs) EncryptionConfigPtrInput {
	return (*encryptionConfigPtrType)(v)
}

func (*encryptionConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**EncryptionConfig)(nil)).Elem()
}

func (i *encryptionConfigPtrType) ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput {
	return i.ToEncryptionConfigPtrOutputWithContext(context.Background())
}

func (i *encryptionConfigPtrType) ToEncryptionConfigPtrOutputWithContext(ctx context.Context) EncryptionConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EncryptionConfigPtrOutput)
}

// Encryption settings for the cluster.
type EncryptionConfigOutput struct{ *pulumi.OutputState }

func (EncryptionConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EncryptionConfig)(nil)).Elem()
}

func (o EncryptionConfigOutput) ToEncryptionConfigOutput() EncryptionConfigOutput {
	return o
}

func (o EncryptionConfigOutput) ToEncryptionConfigOutputWithContext(ctx context.Context) EncryptionConfigOutput {
	return o
}

func (o EncryptionConfigOutput) ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput {
	return o.ToEncryptionConfigPtrOutputWithContext(context.Background())
}

func (o EncryptionConfigOutput) ToEncryptionConfigPtrOutputWithContext(ctx context.Context) EncryptionConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v EncryptionConfig) *EncryptionConfig {
		return &v
	}).(EncryptionConfigPtrOutput)
}

// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
func (o EncryptionConfigOutput) GcePdKmsKeyName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v EncryptionConfig) *string { return v.GcePdKmsKeyName }).(pulumi.StringPtrOutput)
}

type EncryptionConfigPtrOutput struct{ *pulumi.OutputState }

func (EncryptionConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**EncryptionConfig)(nil)).Elem()
}

func (o EncryptionConfigPtrOutput) ToEncryptionConfigPtrOutput() EncryptionConfigPtrOutput {
	return o
}

func (o EncryptionConfigPtrOutput) ToEncryptionConfigPtrOutputWithContext(ctx context.Context) EncryptionConfigPtrOutput {
	return o
}

func (o EncryptionConfigPtrOutput) Elem() EncryptionConfigOutput {
	return o.ApplyT(func(v *EncryptionConfig) EncryptionConfig {
		if v != nil {
			return *v
		}
		var ret EncryptionConfig
		return ret
	}).(EncryptionConfigOutput)
}

// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
func (o EncryptionConfigPtrOutput) GcePdKmsKeyName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *EncryptionConfig) *string {
		if v == nil {
			return nil
		}
		return v.GcePdKmsKeyName
	}).(pulumi.StringPtrOutput)
}

// Encryption settings for the cluster.
type EncryptionConfigResponse struct {
	// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
	GcePdKmsKeyName string `pulumi:"gcePdKmsKeyName"`
}

// Encryption settings for the cluster.
type EncryptionConfigResponseOutput struct{ *pulumi.OutputState }

func (EncryptionConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EncryptionConfigResponse)(nil)).Elem()
}

func (o EncryptionConfigResponseOutput) ToEncryptionConfigResponseOutput() EncryptionConfigResponseOutput {
	return o
}

func (o EncryptionConfigResponseOutput) ToEncryptionConfigResponseOutputWithContext(ctx context.Context) EncryptionConfigResponseOutput {
	return o
}

// Optional. The Cloud KMS key name to use for PD disk encryption for all instances in the cluster.
func (o EncryptionConfigResponseOutput) GcePdKmsKeyName() pulumi.StringOutput {
	return o.ApplyT(func(v EncryptionConfigResponse) string { return v.GcePdKmsKeyName }).(pulumi.StringOutput)
}

// Endpoint config for this cluster
type EndpointConfig struct {
	// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
	EnableHttpPortAccess *bool `pulumi:"enableHttpPortAccess"`
}

// EndpointConfigInput is an input type that accepts EndpointConfigArgs and EndpointConfigOutput values.
// You can construct a concrete instance of `EndpointConfigInput` via:
//
//          EndpointConfigArgs{...}
type EndpointConfigInput interface {
	pulumi.Input

	ToEndpointConfigOutput() EndpointConfigOutput
	ToEndpointConfigOutputWithContext(context.Context) EndpointConfigOutput
}

// Endpoint config for this cluster
type EndpointConfigArgs struct {
	// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
	EnableHttpPortAccess pulumi.BoolPtrInput `pulumi:"enableHttpPortAccess"`
}

func (EndpointConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*EndpointConfig)(nil)).Elem()
}

func (i EndpointConfigArgs) ToEndpointConfigOutput() EndpointConfigOutput {
	return i.ToEndpointConfigOutputWithContext(context.Background())
}

func (i EndpointConfigArgs) ToEndpointConfigOutputWithContext(ctx context.Context) EndpointConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EndpointConfigOutput)
}

func (i EndpointConfigArgs) ToEndpointConfigPtrOutput() EndpointConfigPtrOutput {
	return i.ToEndpointConfigPtrOutputWithContext(context.Background())
}

func (i EndpointConfigArgs) ToEndpointConfigPtrOutputWithContext(ctx context.Context) EndpointConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EndpointConfigOutput).ToEndpointConfigPtrOutputWithContext(ctx)
}

// EndpointConfigPtrInput is an input type that accepts EndpointConfigArgs, EndpointConfigPtr and EndpointConfigPtrOutput values.
// You can construct a concrete instance of `EndpointConfigPtrInput` via:
//
//          EndpointConfigArgs{...}
//
//  or:
//
//          nil
type EndpointConfigPtrInput interface {
	pulumi.Input

	ToEndpointConfigPtrOutput() EndpointConfigPtrOutput
	ToEndpointConfigPtrOutputWithContext(context.Context) EndpointConfigPtrOutput
}

type endpointConfigPtrType EndpointConfigArgs

func EndpointConfigPtr(v *EndpointConfigArgs) EndpointConfigPtrInput {
	return (*endpointConfigPtrType)(v)
}

func (*endpointConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**EndpointConfig)(nil)).Elem()
}

func (i *endpointConfigPtrType) ToEndpointConfigPtrOutput() EndpointConfigPtrOutput {
	return i.ToEndpointConfigPtrOutputWithContext(context.Background())
}

func (i *endpointConfigPtrType) ToEndpointConfigPtrOutputWithContext(ctx context.Context) EndpointConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EndpointConfigPtrOutput)
}

// Endpoint config for this cluster
type EndpointConfigOutput struct{ *pulumi.OutputState }

func (EndpointConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EndpointConfig)(nil)).Elem()
}

func (o EndpointConfigOutput) ToEndpointConfigOutput() EndpointConfigOutput {
	return o
}

func (o EndpointConfigOutput) ToEndpointConfigOutputWithContext(ctx context.Context) EndpointConfigOutput {
	return o
}

func (o EndpointConfigOutput) ToEndpointConfigPtrOutput() EndpointConfigPtrOutput {
	return o.ToEndpointConfigPtrOutputWithContext(context.Background())
}

func (o EndpointConfigOutput) ToEndpointConfigPtrOutputWithContext(ctx context.Context) EndpointConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v EndpointConfig) *EndpointConfig {
		return &v
	}).(EndpointConfigPtrOutput)
}

// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
func (o EndpointConfigOutput) EnableHttpPortAccess() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v EndpointConfig) *bool { return v.EnableHttpPortAccess }).(pulumi.BoolPtrOutput)
}

type EndpointConfigPtrOutput struct{ *pulumi.OutputState }

func (EndpointConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**EndpointConfig)(nil)).Elem()
}

func (o EndpointConfigPtrOutput) ToEndpointConfigPtrOutput() EndpointConfigPtrOutput {
	return o
}

func (o EndpointConfigPtrOutput) ToEndpointConfigPtrOutputWithContext(ctx context.Context) EndpointConfigPtrOutput {
	return o
}

func (o EndpointConfigPtrOutput) Elem() EndpointConfigOutput {
	return o.ApplyT(func(v *EndpointConfig) EndpointConfig {
		if v != nil {
			return *v
		}
		var ret EndpointConfig
		return ret
	}).(EndpointConfigOutput)
}

// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
func (o EndpointConfigPtrOutput) EnableHttpPortAccess() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *EndpointConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableHttpPortAccess
	}).(pulumi.BoolPtrOutput)
}

// Endpoint config for this cluster
type EndpointConfigResponse struct {
	// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
	EnableHttpPortAccess bool `pulumi:"enableHttpPortAccess"`
	// The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
	HttpPorts map[string]string `pulumi:"httpPorts"`
}

// Endpoint config for this cluster
type EndpointConfigResponseOutput struct{ *pulumi.OutputState }

func (EndpointConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EndpointConfigResponse)(nil)).Elem()
}

func (o EndpointConfigResponseOutput) ToEndpointConfigResponseOutput() EndpointConfigResponseOutput {
	return o
}

func (o EndpointConfigResponseOutput) ToEndpointConfigResponseOutputWithContext(ctx context.Context) EndpointConfigResponseOutput {
	return o
}

// Optional. If true, enable http access to specific ports on the cluster from external sources. Defaults to false.
func (o EndpointConfigResponseOutput) EnableHttpPortAccess() pulumi.BoolOutput {
	return o.ApplyT(func(v EndpointConfigResponse) bool { return v.EnableHttpPortAccess }).(pulumi.BoolOutput)
}

// The map of port descriptions to URLs. Will only be populated if enable_http_port_access is true.
func (o EndpointConfigResponseOutput) HttpPorts() pulumi.StringMapOutput {
	return o.ApplyT(func(v EndpointConfigResponse) map[string]string { return v.HttpPorts }).(pulumi.StringMapOutput)
}

// Environment configuration for a workload.
type EnvironmentConfig struct {
	// Optional. Execution configuration for a workload.
	ExecutionConfig *ExecutionConfig `pulumi:"executionConfig"`
	// Optional. Peripherals configuration that workload has access to.
	PeripheralsConfig *PeripheralsConfig `pulumi:"peripheralsConfig"`
}

// EnvironmentConfigInput is an input type that accepts EnvironmentConfigArgs and EnvironmentConfigOutput values.
// You can construct a concrete instance of `EnvironmentConfigInput` via:
//
//          EnvironmentConfigArgs{...}
type EnvironmentConfigInput interface {
	pulumi.Input

	ToEnvironmentConfigOutput() EnvironmentConfigOutput
	ToEnvironmentConfigOutputWithContext(context.Context) EnvironmentConfigOutput
}

// Environment configuration for a workload.
type EnvironmentConfigArgs struct {
	// Optional. Execution configuration for a workload.
	ExecutionConfig ExecutionConfigPtrInput `pulumi:"executionConfig"`
	// Optional. Peripherals configuration that workload has access to.
	PeripheralsConfig PeripheralsConfigPtrInput `pulumi:"peripheralsConfig"`
}

func (EnvironmentConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*EnvironmentConfig)(nil)).Elem()
}

func (i EnvironmentConfigArgs) ToEnvironmentConfigOutput() EnvironmentConfigOutput {
	return i.ToEnvironmentConfigOutputWithContext(context.Background())
}

func (i EnvironmentConfigArgs) ToEnvironmentConfigOutputWithContext(ctx context.Context) EnvironmentConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EnvironmentConfigOutput)
}

func (i EnvironmentConfigArgs) ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput {
	return i.ToEnvironmentConfigPtrOutputWithContext(context.Background())
}

func (i EnvironmentConfigArgs) ToEnvironmentConfigPtrOutputWithContext(ctx context.Context) EnvironmentConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EnvironmentConfigOutput).ToEnvironmentConfigPtrOutputWithContext(ctx)
}

// EnvironmentConfigPtrInput is an input type that accepts EnvironmentConfigArgs, EnvironmentConfigPtr and EnvironmentConfigPtrOutput values.
// You can construct a concrete instance of `EnvironmentConfigPtrInput` via:
//
//          EnvironmentConfigArgs{...}
//
//  or:
//
//          nil
type EnvironmentConfigPtrInput interface {
	pulumi.Input

	ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput
	ToEnvironmentConfigPtrOutputWithContext(context.Context) EnvironmentConfigPtrOutput
}

type environmentConfigPtrType EnvironmentConfigArgs

func EnvironmentConfigPtr(v *EnvironmentConfigArgs) EnvironmentConfigPtrInput {
	return (*environmentConfigPtrType)(v)
}

func (*environmentConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**EnvironmentConfig)(nil)).Elem()
}

func (i *environmentConfigPtrType) ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput {
	return i.ToEnvironmentConfigPtrOutputWithContext(context.Background())
}

func (i *environmentConfigPtrType) ToEnvironmentConfigPtrOutputWithContext(ctx context.Context) EnvironmentConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(EnvironmentConfigPtrOutput)
}

// Environment configuration for a workload.
type EnvironmentConfigOutput struct{ *pulumi.OutputState }

func (EnvironmentConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EnvironmentConfig)(nil)).Elem()
}

func (o EnvironmentConfigOutput) ToEnvironmentConfigOutput() EnvironmentConfigOutput {
	return o
}

func (o EnvironmentConfigOutput) ToEnvironmentConfigOutputWithContext(ctx context.Context) EnvironmentConfigOutput {
	return o
}

func (o EnvironmentConfigOutput) ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput {
	return o.ToEnvironmentConfigPtrOutputWithContext(context.Background())
}

func (o EnvironmentConfigOutput) ToEnvironmentConfigPtrOutputWithContext(ctx context.Context) EnvironmentConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v EnvironmentConfig) *EnvironmentConfig {
		return &v
	}).(EnvironmentConfigPtrOutput)
}

// Optional. Execution configuration for a workload.
func (o EnvironmentConfigOutput) ExecutionConfig() ExecutionConfigPtrOutput {
	return o.ApplyT(func(v EnvironmentConfig) *ExecutionConfig { return v.ExecutionConfig }).(ExecutionConfigPtrOutput)
}

// Optional. Peripherals configuration that workload has access to.
func (o EnvironmentConfigOutput) PeripheralsConfig() PeripheralsConfigPtrOutput {
	return o.ApplyT(func(v EnvironmentConfig) *PeripheralsConfig { return v.PeripheralsConfig }).(PeripheralsConfigPtrOutput)
}

type EnvironmentConfigPtrOutput struct{ *pulumi.OutputState }

func (EnvironmentConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**EnvironmentConfig)(nil)).Elem()
}

func (o EnvironmentConfigPtrOutput) ToEnvironmentConfigPtrOutput() EnvironmentConfigPtrOutput {
	return o
}

func (o EnvironmentConfigPtrOutput) ToEnvironmentConfigPtrOutputWithContext(ctx context.Context) EnvironmentConfigPtrOutput {
	return o
}

func (o EnvironmentConfigPtrOutput) Elem() EnvironmentConfigOutput {
	return o.ApplyT(func(v *EnvironmentConfig) EnvironmentConfig {
		if v != nil {
			return *v
		}
		var ret EnvironmentConfig
		return ret
	}).(EnvironmentConfigOutput)
}

// Optional. Execution configuration for a workload.
func (o EnvironmentConfigPtrOutput) ExecutionConfig() ExecutionConfigPtrOutput {
	return o.ApplyT(func(v *EnvironmentConfig) *ExecutionConfig {
		if v == nil {
			return nil
		}
		return v.ExecutionConfig
	}).(ExecutionConfigPtrOutput)
}

// Optional. Peripherals configuration that workload has access to.
func (o EnvironmentConfigPtrOutput) PeripheralsConfig() PeripheralsConfigPtrOutput {
	return o.ApplyT(func(v *EnvironmentConfig) *PeripheralsConfig {
		if v == nil {
			return nil
		}
		return v.PeripheralsConfig
	}).(PeripheralsConfigPtrOutput)
}

// Environment configuration for a workload.
type EnvironmentConfigResponse struct {
	// Optional. Execution configuration for a workload.
	ExecutionConfig ExecutionConfigResponse `pulumi:"executionConfig"`
	// Optional. Peripherals configuration that workload has access to.
	PeripheralsConfig PeripheralsConfigResponse `pulumi:"peripheralsConfig"`
}

// Environment configuration for a workload.
type EnvironmentConfigResponseOutput struct{ *pulumi.OutputState }

func (EnvironmentConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*EnvironmentConfigResponse)(nil)).Elem()
}

func (o EnvironmentConfigResponseOutput) ToEnvironmentConfigResponseOutput() EnvironmentConfigResponseOutput {
	return o
}

func (o EnvironmentConfigResponseOutput) ToEnvironmentConfigResponseOutputWithContext(ctx context.Context) EnvironmentConfigResponseOutput {
	return o
}

// Optional. Execution configuration for a workload.
func (o EnvironmentConfigResponseOutput) ExecutionConfig() ExecutionConfigResponseOutput {
	return o.ApplyT(func(v EnvironmentConfigResponse) ExecutionConfigResponse { return v.ExecutionConfig }).(ExecutionConfigResponseOutput)
}

// Optional. Peripherals configuration that workload has access to.
func (o EnvironmentConfigResponseOutput) PeripheralsConfig() PeripheralsConfigResponseOutput {
	return o.ApplyT(func(v EnvironmentConfigResponse) PeripheralsConfigResponse { return v.PeripheralsConfig }).(PeripheralsConfigResponseOutput)
}

// Execution configuration for a workload.
type ExecutionConfig struct {
	// Optional. The Cloud KMS key to use for encryption.
	KmsKey *string `pulumi:"kmsKey"`
	// Optional. Tags used for network traffic control.
	NetworkTags []string `pulumi:"networkTags"`
	// Optional. Network URI to connect workload to.
	NetworkUri *string `pulumi:"networkUri"`
	// Optional. Service account that used to execute workload.
	ServiceAccount *string `pulumi:"serviceAccount"`
	// Optional. Subnetwork URI to connect workload to.
	SubnetworkUri *string `pulumi:"subnetworkUri"`
}

// ExecutionConfigInput is an input type that accepts ExecutionConfigArgs and ExecutionConfigOutput values.
// You can construct a concrete instance of `ExecutionConfigInput` via:
//
//          ExecutionConfigArgs{...}
type ExecutionConfigInput interface {
	pulumi.Input

	ToExecutionConfigOutput() ExecutionConfigOutput
	ToExecutionConfigOutputWithContext(context.Context) ExecutionConfigOutput
}

// Execution configuration for a workload.
type ExecutionConfigArgs struct {
	// Optional. The Cloud KMS key to use for encryption.
	KmsKey pulumi.StringPtrInput `pulumi:"kmsKey"`
	// Optional. Tags used for network traffic control.
	NetworkTags pulumi.StringArrayInput `pulumi:"networkTags"`
	// Optional. Network URI to connect workload to.
	NetworkUri pulumi.StringPtrInput `pulumi:"networkUri"`
	// Optional. Service account that used to execute workload.
	ServiceAccount pulumi.StringPtrInput `pulumi:"serviceAccount"`
	// Optional. Subnetwork URI to connect workload to.
	SubnetworkUri pulumi.StringPtrInput `pulumi:"subnetworkUri"`
}

func (ExecutionConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ExecutionConfig)(nil)).Elem()
}

func (i ExecutionConfigArgs) ToExecutionConfigOutput() ExecutionConfigOutput {
	return i.ToExecutionConfigOutputWithContext(context.Background())
}

func (i ExecutionConfigArgs) ToExecutionConfigOutputWithContext(ctx context.Context) ExecutionConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExecutionConfigOutput)
}

func (i ExecutionConfigArgs) ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput {
	return i.ToExecutionConfigPtrOutputWithContext(context.Background())
}

func (i ExecutionConfigArgs) ToExecutionConfigPtrOutputWithContext(ctx context.Context) ExecutionConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExecutionConfigOutput).ToExecutionConfigPtrOutputWithContext(ctx)
}

// ExecutionConfigPtrInput is an input type that accepts ExecutionConfigArgs, ExecutionConfigPtr and ExecutionConfigPtrOutput values.
// You can construct a concrete instance of `ExecutionConfigPtrInput` via:
//
//          ExecutionConfigArgs{...}
//
//  or:
//
//          nil
type ExecutionConfigPtrInput interface {
	pulumi.Input

	ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput
	ToExecutionConfigPtrOutputWithContext(context.Context) ExecutionConfigPtrOutput
}

type executionConfigPtrType ExecutionConfigArgs

func ExecutionConfigPtr(v *ExecutionConfigArgs) ExecutionConfigPtrInput {
	return (*executionConfigPtrType)(v)
}

func (*executionConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ExecutionConfig)(nil)).Elem()
}

func (i *executionConfigPtrType) ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput {
	return i.ToExecutionConfigPtrOutputWithContext(context.Background())
}

func (i *executionConfigPtrType) ToExecutionConfigPtrOutputWithContext(ctx context.Context) ExecutionConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExecutionConfigPtrOutput)
}

// Execution configuration for a workload.
type ExecutionConfigOutput struct{ *pulumi.OutputState }

func (ExecutionConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ExecutionConfig)(nil)).Elem()
}

func (o ExecutionConfigOutput) ToExecutionConfigOutput() ExecutionConfigOutput {
	return o
}

func (o ExecutionConfigOutput) ToExecutionConfigOutputWithContext(ctx context.Context) ExecutionConfigOutput {
	return o
}

func (o ExecutionConfigOutput) ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput {
	return o.ToExecutionConfigPtrOutputWithContext(context.Background())
}

func (o ExecutionConfigOutput) ToExecutionConfigPtrOutputWithContext(ctx context.Context) ExecutionConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ExecutionConfig) *ExecutionConfig {
		return &v
	}).(ExecutionConfigPtrOutput)
}

// Optional. The Cloud KMS key to use for encryption.
func (o ExecutionConfigOutput) KmsKey() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.KmsKey }).(pulumi.StringPtrOutput)
}

// Optional. Tags used for network traffic control.
func (o ExecutionConfigOutput) NetworkTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ExecutionConfig) []string { return v.NetworkTags }).(pulumi.StringArrayOutput)
}

// Optional. Network URI to connect workload to.
func (o ExecutionConfigOutput) NetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.NetworkUri }).(pulumi.StringPtrOutput)
}

// Optional. Service account that used to execute workload.
func (o ExecutionConfigOutput) ServiceAccount() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.ServiceAccount }).(pulumi.StringPtrOutput)
}

// Optional. Subnetwork URI to connect workload to.
func (o ExecutionConfigOutput) SubnetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ExecutionConfig) *string { return v.SubnetworkUri }).(pulumi.StringPtrOutput)
}

type ExecutionConfigPtrOutput struct{ *pulumi.OutputState }

func (ExecutionConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ExecutionConfig)(nil)).Elem()
}

func (o ExecutionConfigPtrOutput) ToExecutionConfigPtrOutput() ExecutionConfigPtrOutput {
	return o
}

func (o ExecutionConfigPtrOutput) ToExecutionConfigPtrOutputWithContext(ctx context.Context) ExecutionConfigPtrOutput {
	return o
}

func (o ExecutionConfigPtrOutput) Elem() ExecutionConfigOutput {
	return o.ApplyT(func(v *ExecutionConfig) ExecutionConfig {
		if v != nil {
			return *v
		}
		var ret ExecutionConfig
		return ret
	}).(ExecutionConfigOutput)
}

// Optional. The Cloud KMS key to use for encryption.
func (o ExecutionConfigPtrOutput) KmsKey() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.KmsKey
	}).(pulumi.StringPtrOutput)
}

// Optional. Tags used for network traffic control.
func (o ExecutionConfigPtrOutput) NetworkTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *ExecutionConfig) []string {
		if v == nil {
			return nil
		}
		return v.NetworkTags
	}).(pulumi.StringArrayOutput)
}

// Optional. Network URI to connect workload to.
func (o ExecutionConfigPtrOutput) NetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.NetworkUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Service account that used to execute workload.
func (o ExecutionConfigPtrOutput) ServiceAccount() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.ServiceAccount
	}).(pulumi.StringPtrOutput)
}

// Optional. Subnetwork URI to connect workload to.
func (o ExecutionConfigPtrOutput) SubnetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ExecutionConfig) *string {
		if v == nil {
			return nil
		}
		return v.SubnetworkUri
	}).(pulumi.StringPtrOutput)
}

// Execution configuration for a workload.
type ExecutionConfigResponse struct {
	// Optional. The Cloud KMS key to use for encryption.
	KmsKey string `pulumi:"kmsKey"`
	// Optional. Tags used for network traffic control.
	NetworkTags []string `pulumi:"networkTags"`
	// Optional. Network URI to connect workload to.
	NetworkUri string `pulumi:"networkUri"`
	// Optional. Service account that used to execute workload.
	ServiceAccount string `pulumi:"serviceAccount"`
	// Optional. Subnetwork URI to connect workload to.
	SubnetworkUri string `pulumi:"subnetworkUri"`
}

// Execution configuration for a workload.
type ExecutionConfigResponseOutput struct{ *pulumi.OutputState }

func (ExecutionConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ExecutionConfigResponse)(nil)).Elem()
}

func (o ExecutionConfigResponseOutput) ToExecutionConfigResponseOutput() ExecutionConfigResponseOutput {
	return o
}

func (o ExecutionConfigResponseOutput) ToExecutionConfigResponseOutputWithContext(ctx context.Context) ExecutionConfigResponseOutput {
	return o
}

// Optional. The Cloud KMS key to use for encryption.
func (o ExecutionConfigResponseOutput) KmsKey() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.KmsKey }).(pulumi.StringOutput)
}

// Optional. Tags used for network traffic control.
func (o ExecutionConfigResponseOutput) NetworkTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) []string { return v.NetworkTags }).(pulumi.StringArrayOutput)
}

// Optional. Network URI to connect workload to.
func (o ExecutionConfigResponseOutput) NetworkUri() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.NetworkUri }).(pulumi.StringOutput)
}

// Optional. Service account that used to execute workload.
func (o ExecutionConfigResponseOutput) ServiceAccount() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.ServiceAccount }).(pulumi.StringOutput)
}

// Optional. Subnetwork URI to connect workload to.
func (o ExecutionConfigResponseOutput) SubnetworkUri() pulumi.StringOutput {
	return o.ApplyT(func(v ExecutionConfigResponse) string { return v.SubnetworkUri }).(pulumi.StringOutput)
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type Expr struct {
	// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
	Description *string `pulumi:"description"`
	// Textual representation of an expression in Common Expression Language syntax.
	Expression *string `pulumi:"expression"`
	// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
	Location *string `pulumi:"location"`
	// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
	Title *string `pulumi:"title"`
}

// ExprInput is an input type that accepts ExprArgs and ExprOutput values.
// You can construct a concrete instance of `ExprInput` via:
//
//          ExprArgs{...}
type ExprInput interface {
	pulumi.Input

	ToExprOutput() ExprOutput
	ToExprOutputWithContext(context.Context) ExprOutput
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type ExprArgs struct {
	// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
	Description pulumi.StringPtrInput `pulumi:"description"`
	// Textual representation of an expression in Common Expression Language syntax.
	Expression pulumi.StringPtrInput `pulumi:"expression"`
	// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
	Location pulumi.StringPtrInput `pulumi:"location"`
	// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
	Title pulumi.StringPtrInput `pulumi:"title"`
}

func (ExprArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*Expr)(nil)).Elem()
}

func (i ExprArgs) ToExprOutput() ExprOutput {
	return i.ToExprOutputWithContext(context.Background())
}

func (i ExprArgs) ToExprOutputWithContext(ctx context.Context) ExprOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExprOutput)
}

func (i ExprArgs) ToExprPtrOutput() ExprPtrOutput {
	return i.ToExprPtrOutputWithContext(context.Background())
}

func (i ExprArgs) ToExprPtrOutputWithContext(ctx context.Context) ExprPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExprOutput).ToExprPtrOutputWithContext(ctx)
}

// ExprPtrInput is an input type that accepts ExprArgs, ExprPtr and ExprPtrOutput values.
// You can construct a concrete instance of `ExprPtrInput` via:
//
//          ExprArgs{...}
//
//  or:
//
//          nil
type ExprPtrInput interface {
	pulumi.Input

	ToExprPtrOutput() ExprPtrOutput
	ToExprPtrOutputWithContext(context.Context) ExprPtrOutput
}

type exprPtrType ExprArgs

func ExprPtr(v *ExprArgs) ExprPtrInput {
	return (*exprPtrType)(v)
}

func (*exprPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**Expr)(nil)).Elem()
}

func (i *exprPtrType) ToExprPtrOutput() ExprPtrOutput {
	return i.ToExprPtrOutputWithContext(context.Background())
}

func (i *exprPtrType) ToExprPtrOutputWithContext(ctx context.Context) ExprPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ExprPtrOutput)
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type ExprOutput struct{ *pulumi.OutputState }

func (ExprOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*Expr)(nil)).Elem()
}

func (o ExprOutput) ToExprOutput() ExprOutput {
	return o
}

func (o ExprOutput) ToExprOutputWithContext(ctx context.Context) ExprOutput {
	return o
}

func (o ExprOutput) ToExprPtrOutput() ExprPtrOutput {
	return o.ToExprPtrOutputWithContext(context.Background())
}

func (o ExprOutput) ToExprPtrOutputWithContext(ctx context.Context) ExprPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v Expr) *Expr {
		return &v
	}).(ExprPtrOutput)
}

// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
func (o ExprOutput) Description() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Expr) *string { return v.Description }).(pulumi.StringPtrOutput)
}

// Textual representation of an expression in Common Expression Language syntax.
func (o ExprOutput) Expression() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Expr) *string { return v.Expression }).(pulumi.StringPtrOutput)
}

// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
func (o ExprOutput) Location() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Expr) *string { return v.Location }).(pulumi.StringPtrOutput)
}

// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
func (o ExprOutput) Title() pulumi.StringPtrOutput {
	return o.ApplyT(func(v Expr) *string { return v.Title }).(pulumi.StringPtrOutput)
}

type ExprPtrOutput struct{ *pulumi.OutputState }

func (ExprPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**Expr)(nil)).Elem()
}

func (o ExprPtrOutput) ToExprPtrOutput() ExprPtrOutput {
	return o
}

func (o ExprPtrOutput) ToExprPtrOutputWithContext(ctx context.Context) ExprPtrOutput {
	return o
}

func (o ExprPtrOutput) Elem() ExprOutput {
	return o.ApplyT(func(v *Expr) Expr {
		if v != nil {
			return *v
		}
		var ret Expr
		return ret
	}).(ExprOutput)
}

// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
func (o ExprPtrOutput) Description() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Expr) *string {
		if v == nil {
			return nil
		}
		return v.Description
	}).(pulumi.StringPtrOutput)
}

// Textual representation of an expression in Common Expression Language syntax.
func (o ExprPtrOutput) Expression() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Expr) *string {
		if v == nil {
			return nil
		}
		return v.Expression
	}).(pulumi.StringPtrOutput)
}

// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
func (o ExprPtrOutput) Location() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Expr) *string {
		if v == nil {
			return nil
		}
		return v.Location
	}).(pulumi.StringPtrOutput)
}

// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
func (o ExprPtrOutput) Title() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *Expr) *string {
		if v == nil {
			return nil
		}
		return v.Title
	}).(pulumi.StringPtrOutput)
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type ExprResponse struct {
	// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
	Description string `pulumi:"description"`
	// Textual representation of an expression in Common Expression Language syntax.
	Expression string `pulumi:"expression"`
	// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
	Location string `pulumi:"location"`
	// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
	Title string `pulumi:"title"`
}

// Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
type ExprResponseOutput struct{ *pulumi.OutputState }

func (ExprResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ExprResponse)(nil)).Elem()
}

func (o ExprResponseOutput) ToExprResponseOutput() ExprResponseOutput {
	return o
}

func (o ExprResponseOutput) ToExprResponseOutputWithContext(ctx context.Context) ExprResponseOutput {
	return o
}

// Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
func (o ExprResponseOutput) Description() pulumi.StringOutput {
	return o.ApplyT(func(v ExprResponse) string { return v.Description }).(pulumi.StringOutput)
}

// Textual representation of an expression in Common Expression Language syntax.
func (o ExprResponseOutput) Expression() pulumi.StringOutput {
	return o.ApplyT(func(v ExprResponse) string { return v.Expression }).(pulumi.StringOutput)
}

// Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
func (o ExprResponseOutput) Location() pulumi.StringOutput {
	return o.ApplyT(func(v ExprResponse) string { return v.Location }).(pulumi.StringOutput)
}

// Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
func (o ExprResponseOutput) Title() pulumi.StringOutput {
	return o.ApplyT(func(v ExprResponse) string { return v.Title }).(pulumi.StringOutput)
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfig struct {
	// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
	ConfidentialInstanceConfig *ConfidentialInstanceConfig `pulumi:"confidentialInstanceConfig"`
	// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
	InternalIpOnly *bool `pulumi:"internalIpOnly"`
	// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
	Metadata map[string]string `pulumi:"metadata"`
	// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default projects/[project_id]/regions/global/default default
	NetworkUri *string `pulumi:"networkUri"`
	// Optional. Node Group Affinity for sole-tenant clusters.
	NodeGroupAffinity *NodeGroupAffinity `pulumi:"nodeGroupAffinity"`
	// Optional. The type of IPv6 access for a cluster.
	PrivateIpv6GoogleAccess *GceClusterConfigPrivateIpv6GoogleAccess `pulumi:"privateIpv6GoogleAccess"`
	// Optional. Reservation Affinity for consuming Zonal reservation.
	ReservationAffinity *ReservationAffinity `pulumi:"reservationAffinity"`
	// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
	ServiceAccount *string `pulumi:"serviceAccount"`
	// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
	ServiceAccountScopes []string `pulumi:"serviceAccountScopes"`
	// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
	ShieldedInstanceConfig *ShieldedInstanceConfig `pulumi:"shieldedInstanceConfig"`
	// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0 projects/[project_id]/regions/us-east1/subnetworks/sub0 sub0
	SubnetworkUri *string `pulumi:"subnetworkUri"`
	// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
	Tags []string `pulumi:"tags"`
	// Optional. The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] us-central1-f
	ZoneUri *string `pulumi:"zoneUri"`
}

// GceClusterConfigInput is an input type that accepts GceClusterConfigArgs and GceClusterConfigOutput values.
// You can construct a concrete instance of `GceClusterConfigInput` via:
//
//          GceClusterConfigArgs{...}
type GceClusterConfigInput interface {
	pulumi.Input

	ToGceClusterConfigOutput() GceClusterConfigOutput
	ToGceClusterConfigOutputWithContext(context.Context) GceClusterConfigOutput
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfigArgs struct {
	// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
	ConfidentialInstanceConfig ConfidentialInstanceConfigPtrInput `pulumi:"confidentialInstanceConfig"`
	// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
	InternalIpOnly pulumi.BoolPtrInput `pulumi:"internalIpOnly"`
	// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
	Metadata pulumi.StringMapInput `pulumi:"metadata"`
	// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default projects/[project_id]/regions/global/default default
	NetworkUri pulumi.StringPtrInput `pulumi:"networkUri"`
	// Optional. Node Group Affinity for sole-tenant clusters.
	NodeGroupAffinity NodeGroupAffinityPtrInput `pulumi:"nodeGroupAffinity"`
	// Optional. The type of IPv6 access for a cluster.
	PrivateIpv6GoogleAccess GceClusterConfigPrivateIpv6GoogleAccessPtrInput `pulumi:"privateIpv6GoogleAccess"`
	// Optional. Reservation Affinity for consuming Zonal reservation.
	ReservationAffinity ReservationAffinityPtrInput `pulumi:"reservationAffinity"`
	// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
	ServiceAccount pulumi.StringPtrInput `pulumi:"serviceAccount"`
	// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
	ServiceAccountScopes pulumi.StringArrayInput `pulumi:"serviceAccountScopes"`
	// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
	ShieldedInstanceConfig ShieldedInstanceConfigPtrInput `pulumi:"shieldedInstanceConfig"`
	// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0 projects/[project_id]/regions/us-east1/subnetworks/sub0 sub0
	SubnetworkUri pulumi.StringPtrInput `pulumi:"subnetworkUri"`
	// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
	Tags pulumi.StringArrayInput `pulumi:"tags"`
	// Optional. The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] us-central1-f
	ZoneUri pulumi.StringPtrInput `pulumi:"zoneUri"`
}

func (GceClusterConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*GceClusterConfig)(nil)).Elem()
}

func (i GceClusterConfigArgs) ToGceClusterConfigOutput() GceClusterConfigOutput {
	return i.ToGceClusterConfigOutputWithContext(context.Background())
}

func (i GceClusterConfigArgs) ToGceClusterConfigOutputWithContext(ctx context.Context) GceClusterConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GceClusterConfigOutput)
}

func (i GceClusterConfigArgs) ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput {
	return i.ToGceClusterConfigPtrOutputWithContext(context.Background())
}

func (i GceClusterConfigArgs) ToGceClusterConfigPtrOutputWithContext(ctx context.Context) GceClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GceClusterConfigOutput).ToGceClusterConfigPtrOutputWithContext(ctx)
}

// GceClusterConfigPtrInput is an input type that accepts GceClusterConfigArgs, GceClusterConfigPtr and GceClusterConfigPtrOutput values.
// You can construct a concrete instance of `GceClusterConfigPtrInput` via:
//
//          GceClusterConfigArgs{...}
//
//  or:
//
//          nil
type GceClusterConfigPtrInput interface {
	pulumi.Input

	ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput
	ToGceClusterConfigPtrOutputWithContext(context.Context) GceClusterConfigPtrOutput
}

type gceClusterConfigPtrType GceClusterConfigArgs

func GceClusterConfigPtr(v *GceClusterConfigArgs) GceClusterConfigPtrInput {
	return (*gceClusterConfigPtrType)(v)
}

func (*gceClusterConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**GceClusterConfig)(nil)).Elem()
}

func (i *gceClusterConfigPtrType) ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput {
	return i.ToGceClusterConfigPtrOutputWithContext(context.Background())
}

func (i *gceClusterConfigPtrType) ToGceClusterConfigPtrOutputWithContext(ctx context.Context) GceClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GceClusterConfigPtrOutput)
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfigOutput struct{ *pulumi.OutputState }

func (GceClusterConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GceClusterConfig)(nil)).Elem()
}

func (o GceClusterConfigOutput) ToGceClusterConfigOutput() GceClusterConfigOutput {
	return o
}

func (o GceClusterConfigOutput) ToGceClusterConfigOutputWithContext(ctx context.Context) GceClusterConfigOutput {
	return o
}

func (o GceClusterConfigOutput) ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput {
	return o.ToGceClusterConfigPtrOutputWithContext(context.Background())
}

func (o GceClusterConfigOutput) ToGceClusterConfigPtrOutputWithContext(ctx context.Context) GceClusterConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v GceClusterConfig) *GceClusterConfig {
		return &v
	}).(GceClusterConfigPtrOutput)
}

// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
func (o GceClusterConfigOutput) ConfidentialInstanceConfig() ConfidentialInstanceConfigPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *ConfidentialInstanceConfig { return v.ConfidentialInstanceConfig }).(ConfidentialInstanceConfigPtrOutput)
}

// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
func (o GceClusterConfigOutput) InternalIpOnly() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *bool { return v.InternalIpOnly }).(pulumi.BoolPtrOutput)
}

// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
func (o GceClusterConfigOutput) Metadata() pulumi.StringMapOutput {
	return o.ApplyT(func(v GceClusterConfig) map[string]string { return v.Metadata }).(pulumi.StringMapOutput)
}

// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default projects/[project_id]/regions/global/default default
func (o GceClusterConfigOutput) NetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *string { return v.NetworkUri }).(pulumi.StringPtrOutput)
}

// Optional. Node Group Affinity for sole-tenant clusters.
func (o GceClusterConfigOutput) NodeGroupAffinity() NodeGroupAffinityPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *NodeGroupAffinity { return v.NodeGroupAffinity }).(NodeGroupAffinityPtrOutput)
}

// Optional. The type of IPv6 access for a cluster.
func (o GceClusterConfigOutput) PrivateIpv6GoogleAccess() GceClusterConfigPrivateIpv6GoogleAccessPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *GceClusterConfigPrivateIpv6GoogleAccess { return v.PrivateIpv6GoogleAccess }).(GceClusterConfigPrivateIpv6GoogleAccessPtrOutput)
}

// Optional. Reservation Affinity for consuming Zonal reservation.
func (o GceClusterConfigOutput) ReservationAffinity() ReservationAffinityPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *ReservationAffinity { return v.ReservationAffinity }).(ReservationAffinityPtrOutput)
}

// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
func (o GceClusterConfigOutput) ServiceAccount() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *string { return v.ServiceAccount }).(pulumi.StringPtrOutput)
}

// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
func (o GceClusterConfigOutput) ServiceAccountScopes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GceClusterConfig) []string { return v.ServiceAccountScopes }).(pulumi.StringArrayOutput)
}

// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
func (o GceClusterConfigOutput) ShieldedInstanceConfig() ShieldedInstanceConfigPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *ShieldedInstanceConfig { return v.ShieldedInstanceConfig }).(ShieldedInstanceConfigPtrOutput)
}

// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0 projects/[project_id]/regions/us-east1/subnetworks/sub0 sub0
func (o GceClusterConfigOutput) SubnetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *string { return v.SubnetworkUri }).(pulumi.StringPtrOutput)
}

// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
func (o GceClusterConfigOutput) Tags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GceClusterConfig) []string { return v.Tags }).(pulumi.StringArrayOutput)
}

// Optional. The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] us-central1-f
func (o GceClusterConfigOutput) ZoneUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v GceClusterConfig) *string { return v.ZoneUri }).(pulumi.StringPtrOutput)
}

type GceClusterConfigPtrOutput struct{ *pulumi.OutputState }

func (GceClusterConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**GceClusterConfig)(nil)).Elem()
}

func (o GceClusterConfigPtrOutput) ToGceClusterConfigPtrOutput() GceClusterConfigPtrOutput {
	return o
}

func (o GceClusterConfigPtrOutput) ToGceClusterConfigPtrOutputWithContext(ctx context.Context) GceClusterConfigPtrOutput {
	return o
}

func (o GceClusterConfigPtrOutput) Elem() GceClusterConfigOutput {
	return o.ApplyT(func(v *GceClusterConfig) GceClusterConfig {
		if v != nil {
			return *v
		}
		var ret GceClusterConfig
		return ret
	}).(GceClusterConfigOutput)
}

// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
func (o GceClusterConfigPtrOutput) ConfidentialInstanceConfig() ConfidentialInstanceConfigPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *ConfidentialInstanceConfig {
		if v == nil {
			return nil
		}
		return v.ConfidentialInstanceConfig
	}).(ConfidentialInstanceConfigPtrOutput)
}

// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
func (o GceClusterConfigPtrOutput) InternalIpOnly() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *bool {
		if v == nil {
			return nil
		}
		return v.InternalIpOnly
	}).(pulumi.BoolPtrOutput)
}

// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
func (o GceClusterConfigPtrOutput) Metadata() pulumi.StringMapOutput {
	return o.ApplyT(func(v *GceClusterConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.Metadata
	}).(pulumi.StringMapOutput)
}

// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default projects/[project_id]/regions/global/default default
func (o GceClusterConfigPtrOutput) NetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.NetworkUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Node Group Affinity for sole-tenant clusters.
func (o GceClusterConfigPtrOutput) NodeGroupAffinity() NodeGroupAffinityPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *NodeGroupAffinity {
		if v == nil {
			return nil
		}
		return v.NodeGroupAffinity
	}).(NodeGroupAffinityPtrOutput)
}

// Optional. The type of IPv6 access for a cluster.
func (o GceClusterConfigPtrOutput) PrivateIpv6GoogleAccess() GceClusterConfigPrivateIpv6GoogleAccessPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *GceClusterConfigPrivateIpv6GoogleAccess {
		if v == nil {
			return nil
		}
		return v.PrivateIpv6GoogleAccess
	}).(GceClusterConfigPrivateIpv6GoogleAccessPtrOutput)
}

// Optional. Reservation Affinity for consuming Zonal reservation.
func (o GceClusterConfigPtrOutput) ReservationAffinity() ReservationAffinityPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *ReservationAffinity {
		if v == nil {
			return nil
		}
		return v.ReservationAffinity
	}).(ReservationAffinityPtrOutput)
}

// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
func (o GceClusterConfigPtrOutput) ServiceAccount() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.ServiceAccount
	}).(pulumi.StringPtrOutput)
}

// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
func (o GceClusterConfigPtrOutput) ServiceAccountScopes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *GceClusterConfig) []string {
		if v == nil {
			return nil
		}
		return v.ServiceAccountScopes
	}).(pulumi.StringArrayOutput)
}

// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
func (o GceClusterConfigPtrOutput) ShieldedInstanceConfig() ShieldedInstanceConfigPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *ShieldedInstanceConfig {
		if v == nil {
			return nil
		}
		return v.ShieldedInstanceConfig
	}).(ShieldedInstanceConfigPtrOutput)
}

// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0 projects/[project_id]/regions/us-east1/subnetworks/sub0 sub0
func (o GceClusterConfigPtrOutput) SubnetworkUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.SubnetworkUri
	}).(pulumi.StringPtrOutput)
}

// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
func (o GceClusterConfigPtrOutput) Tags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *GceClusterConfig) []string {
		if v == nil {
			return nil
		}
		return v.Tags
	}).(pulumi.StringArrayOutput)
}

// Optional. The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] us-central1-f
func (o GceClusterConfigPtrOutput) ZoneUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *GceClusterConfig) *string {
		if v == nil {
			return nil
		}
		return v.ZoneUri
	}).(pulumi.StringPtrOutput)
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfigResponse struct {
	// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
	ConfidentialInstanceConfig ConfidentialInstanceConfigResponse `pulumi:"confidentialInstanceConfig"`
	// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
	InternalIpOnly bool `pulumi:"internalIpOnly"`
	// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
	Metadata map[string]string `pulumi:"metadata"`
	// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default projects/[project_id]/regions/global/default default
	NetworkUri string `pulumi:"networkUri"`
	// Optional. Node Group Affinity for sole-tenant clusters.
	NodeGroupAffinity NodeGroupAffinityResponse `pulumi:"nodeGroupAffinity"`
	// Optional. The type of IPv6 access for a cluster.
	PrivateIpv6GoogleAccess string `pulumi:"privateIpv6GoogleAccess"`
	// Optional. Reservation Affinity for consuming Zonal reservation.
	ReservationAffinity ReservationAffinityResponse `pulumi:"reservationAffinity"`
	// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
	ServiceAccount string `pulumi:"serviceAccount"`
	// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
	ServiceAccountScopes []string `pulumi:"serviceAccountScopes"`
	// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
	ShieldedInstanceConfig ShieldedInstanceConfigResponse `pulumi:"shieldedInstanceConfig"`
	// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0 projects/[project_id]/regions/us-east1/subnetworks/sub0 sub0
	SubnetworkUri string `pulumi:"subnetworkUri"`
	// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
	Tags []string `pulumi:"tags"`
	// Optional. The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] us-central1-f
	ZoneUri string `pulumi:"zoneUri"`
}

// Common config settings for resources of Compute Engine cluster instances, applicable to all instances in the cluster.
type GceClusterConfigResponseOutput struct{ *pulumi.OutputState }

func (GceClusterConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GceClusterConfigResponse)(nil)).Elem()
}

func (o GceClusterConfigResponseOutput) ToGceClusterConfigResponseOutput() GceClusterConfigResponseOutput {
	return o
}

func (o GceClusterConfigResponseOutput) ToGceClusterConfigResponseOutputWithContext(ctx context.Context) GceClusterConfigResponseOutput {
	return o
}

// Optional. Confidential Instance Config for clusters using Confidential VMs (https://cloud.google.com/compute/confidential-vm/docs).
func (o GceClusterConfigResponseOutput) ConfidentialInstanceConfig() ConfidentialInstanceConfigResponseOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) ConfidentialInstanceConfigResponse {
		return v.ConfidentialInstanceConfig
	}).(ConfidentialInstanceConfigResponseOutput)
}

// Optional. If true, all instances in the cluster will only have internal IP addresses. By default, clusters are not restricted to internal IP addresses, and will have ephemeral external IP addresses assigned to each instance. This internal_ip_only restriction can only be enabled for subnetwork enabled networks, and all off-cluster dependencies must be configured to be accessible without external IP addresses.
func (o GceClusterConfigResponseOutput) InternalIpOnly() pulumi.BoolOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) bool { return v.InternalIpOnly }).(pulumi.BoolOutput)
}

// The Compute Engine metadata entries to add to all instances (see Project and instance metadata (https://cloud.google.com/compute/docs/storing-retrieving-metadata#project_and_instance_metadata)).
func (o GceClusterConfigResponseOutput) Metadata() pulumi.StringMapOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) map[string]string { return v.Metadata }).(pulumi.StringMapOutput)
}

// Optional. The Compute Engine network to be used for machine communications. Cannot be specified with subnetwork_uri. If neither network_uri nor subnetwork_uri is specified, the "default" network of the project is used, if it exists. Cannot be a "Custom Subnet Network" (see Using Subnetworks (https://cloud.google.com/compute/docs/subnetworks) for more information).A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/global/default projects/[project_id]/regions/global/default default
func (o GceClusterConfigResponseOutput) NetworkUri() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.NetworkUri }).(pulumi.StringOutput)
}

// Optional. Node Group Affinity for sole-tenant clusters.
func (o GceClusterConfigResponseOutput) NodeGroupAffinity() NodeGroupAffinityResponseOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) NodeGroupAffinityResponse { return v.NodeGroupAffinity }).(NodeGroupAffinityResponseOutput)
}

// Optional. The type of IPv6 access for a cluster.
func (o GceClusterConfigResponseOutput) PrivateIpv6GoogleAccess() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.PrivateIpv6GoogleAccess }).(pulumi.StringOutput)
}

// Optional. Reservation Affinity for consuming Zonal reservation.
func (o GceClusterConfigResponseOutput) ReservationAffinity() ReservationAffinityResponseOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) ReservationAffinityResponse { return v.ReservationAffinity }).(ReservationAffinityResponseOutput)
}

// Optional. The Dataproc service account (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) (also see VM Data Plane identity (https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity)) used by Dataproc cluster VM instances to access Google Cloud Platform services.If not specified, the Compute Engine default service account (https://cloud.google.com/compute/docs/access/service-accounts#default_service_account) is used.
func (o GceClusterConfigResponseOutput) ServiceAccount() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.ServiceAccount }).(pulumi.StringOutput)
}

// Optional. The URIs of service account scopes to be included in Compute Engine instances. The following base set of scopes is always included: https://www.googleapis.com/auth/cloud.useraccounts.readonly https://www.googleapis.com/auth/devstorage.read_write https://www.googleapis.com/auth/logging.writeIf no scopes are specified, the following defaults are also provided: https://www.googleapis.com/auth/bigquery https://www.googleapis.com/auth/bigtable.admin.table https://www.googleapis.com/auth/bigtable.data https://www.googleapis.com/auth/devstorage.full_control
func (o GceClusterConfigResponseOutput) ServiceAccountScopes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) []string { return v.ServiceAccountScopes }).(pulumi.StringArrayOutput)
}

// Optional. Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
func (o GceClusterConfigResponseOutput) ShieldedInstanceConfig() ShieldedInstanceConfigResponseOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) ShieldedInstanceConfigResponse { return v.ShieldedInstanceConfig }).(ShieldedInstanceConfigResponseOutput)
}

// Optional. The Compute Engine subnetwork to be used for machine communications. Cannot be specified with network_uri.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/regions/us-east1/subnetworks/sub0 projects/[project_id]/regions/us-east1/subnetworks/sub0 sub0
func (o GceClusterConfigResponseOutput) SubnetworkUri() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.SubnetworkUri }).(pulumi.StringOutput)
}

// The Compute Engine tags to add to all instances (see Tagging instances (https://cloud.google.com/compute/docs/label-or-tag-resources#tags)).
func (o GceClusterConfigResponseOutput) Tags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) []string { return v.Tags }).(pulumi.StringArrayOutput)
}

// Optional. The zone where the Compute Engine cluster will be located. On a create request, it is required in the "global" region. If omitted in a non-global Dataproc region, the service will pick a zone in the corresponding Compute Engine region. On a get request, zone will always be present.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/[zone] projects/[project_id]/zones/[zone] us-central1-f
func (o GceClusterConfigResponseOutput) ZoneUri() pulumi.StringOutput {
	return o.ApplyT(func(v GceClusterConfigResponse) string { return v.ZoneUri }).(pulumi.StringOutput)
}

// The cluster's GKE config.
type GkeClusterConfig struct {
	// Optional. A target for the deployment.
	NamespacedGkeDeploymentTarget *NamespacedGkeDeploymentTarget `pulumi:"namespacedGkeDeploymentTarget"`
}

// GkeClusterConfigInput is an input type that accepts GkeClusterConfigArgs and GkeClusterConfigOutput values.
// You can construct a concrete instance of `GkeClusterConfigInput` via:
//
//          GkeClusterConfigArgs{...}
type GkeClusterConfigInput interface {
	pulumi.Input

	ToGkeClusterConfigOutput() GkeClusterConfigOutput
	ToGkeClusterConfigOutputWithContext(context.Context) GkeClusterConfigOutput
}

// The cluster's GKE config.
type GkeClusterConfigArgs struct {
	// Optional. A target for the deployment.
	NamespacedGkeDeploymentTarget NamespacedGkeDeploymentTargetPtrInput `pulumi:"namespacedGkeDeploymentTarget"`
}

func (GkeClusterConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeClusterConfig)(nil)).Elem()
}

func (i GkeClusterConfigArgs) ToGkeClusterConfigOutput() GkeClusterConfigOutput {
	return i.ToGkeClusterConfigOutputWithContext(context.Background())
}

func (i GkeClusterConfigArgs) ToGkeClusterConfigOutputWithContext(ctx context.Context) GkeClusterConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeClusterConfigOutput)
}

func (i GkeClusterConfigArgs) ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput {
	return i.ToGkeClusterConfigPtrOutputWithContext(context.Background())
}

func (i GkeClusterConfigArgs) ToGkeClusterConfigPtrOutputWithContext(ctx context.Context) GkeClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeClusterConfigOutput).ToGkeClusterConfigPtrOutputWithContext(ctx)
}

// GkeClusterConfigPtrInput is an input type that accepts GkeClusterConfigArgs, GkeClusterConfigPtr and GkeClusterConfigPtrOutput values.
// You can construct a concrete instance of `GkeClusterConfigPtrInput` via:
//
//          GkeClusterConfigArgs{...}
//
//  or:
//
//          nil
type GkeClusterConfigPtrInput interface {
	pulumi.Input

	ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput
	ToGkeClusterConfigPtrOutputWithContext(context.Context) GkeClusterConfigPtrOutput
}

type gkeClusterConfigPtrType GkeClusterConfigArgs

func GkeClusterConfigPtr(v *GkeClusterConfigArgs) GkeClusterConfigPtrInput {
	return (*gkeClusterConfigPtrType)(v)
}

func (*gkeClusterConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeClusterConfig)(nil)).Elem()
}

func (i *gkeClusterConfigPtrType) ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput {
	return i.ToGkeClusterConfigPtrOutputWithContext(context.Background())
}

func (i *gkeClusterConfigPtrType) ToGkeClusterConfigPtrOutputWithContext(ctx context.Context) GkeClusterConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(GkeClusterConfigPtrOutput)
}

// The cluster's GKE config.
type GkeClusterConfigOutput struct{ *pulumi.OutputState }

func (GkeClusterConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeClusterConfig)(nil)).Elem()
}

func (o GkeClusterConfigOutput) ToGkeClusterConfigOutput() GkeClusterConfigOutput {
	return o
}

func (o GkeClusterConfigOutput) ToGkeClusterConfigOutputWithContext(ctx context.Context) GkeClusterConfigOutput {
	return o
}

func (o GkeClusterConfigOutput) ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput {
	return o.ToGkeClusterConfigPtrOutputWithContext(context.Background())
}

func (o GkeClusterConfigOutput) ToGkeClusterConfigPtrOutputWithContext(ctx context.Context) GkeClusterConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v GkeClusterConfig) *GkeClusterConfig {
		return &v
	}).(GkeClusterConfigPtrOutput)
}

// Optional. A target for the deployment.
func (o GkeClusterConfigOutput) NamespacedGkeDeploymentTarget() NamespacedGkeDeploymentTargetPtrOutput {
	return o.ApplyT(func(v GkeClusterConfig) *NamespacedGkeDeploymentTarget { return v.NamespacedGkeDeploymentTarget }).(NamespacedGkeDeploymentTargetPtrOutput)
}

type GkeClusterConfigPtrOutput struct{ *pulumi.OutputState }

func (GkeClusterConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**GkeClusterConfig)(nil)).Elem()
}

func (o GkeClusterConfigPtrOutput) ToGkeClusterConfigPtrOutput() GkeClusterConfigPtrOutput {
	return o
}

func (o GkeClusterConfigPtrOutput) ToGkeClusterConfigPtrOutputWithContext(ctx context.Context) GkeClusterConfigPtrOutput {
	return o
}

func (o GkeClusterConfigPtrOutput) Elem() GkeClusterConfigOutput {
	return o.ApplyT(func(v *GkeClusterConfig) GkeClusterConfig {
		if v != nil {
			return *v
		}
		var ret GkeClusterConfig
		return ret
	}).(GkeClusterConfigOutput)
}

// Optional. A target for the deployment.
func (o GkeClusterConfigPtrOutput) NamespacedGkeDeploymentTarget() NamespacedGkeDeploymentTargetPtrOutput {
	return o.ApplyT(func(v *GkeClusterConfig) *NamespacedGkeDeploymentTarget {
		if v == nil {
			return nil
		}
		return v.NamespacedGkeDeploymentTarget
	}).(NamespacedGkeDeploymentTargetPtrOutput)
}

// The cluster's GKE config.
type GkeClusterConfigResponse struct {
	// Optional. A target for the deployment.
	NamespacedGkeDeploymentTarget NamespacedGkeDeploymentTargetResponse `pulumi:"namespacedGkeDeploymentTarget"`
}

// The cluster's GKE config.
type GkeClusterConfigResponseOutput struct{ *pulumi.OutputState }

func (GkeClusterConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*GkeClusterConfigResponse)(nil)).Elem()
}

func (o GkeClusterConfigResponseOutput) ToGkeClusterConfigResponseOutput() GkeClusterConfigResponseOutput {
	return o
}

func (o GkeClusterConfigResponseOutput) ToGkeClusterConfigResponseOutputWithContext(ctx context.Context) GkeClusterConfigResponseOutput {
	return o
}

// Optional. A target for the deployment.
func (o GkeClusterConfigResponseOutput) NamespacedGkeDeploymentTarget() NamespacedGkeDeploymentTargetResponseOutput {
	return o.ApplyT(func(v GkeClusterConfigResponse) NamespacedGkeDeploymentTargetResponse {
		return v.NamespacedGkeDeploymentTarget
	}).(NamespacedGkeDeploymentTargetResponseOutput)
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJob struct {
	// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass *string `pulumi:"mainClass"`
	// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
	MainJarFileUri *string `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// HadoopJobInput is an input type that accepts HadoopJobArgs and HadoopJobOutput values.
// You can construct a concrete instance of `HadoopJobInput` via:
//
//          HadoopJobArgs{...}
type HadoopJobInput interface {
	pulumi.Input

	ToHadoopJobOutput() HadoopJobOutput
	ToHadoopJobOutputWithContext(context.Context) HadoopJobOutput
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJobArgs struct {
	// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass pulumi.StringPtrInput `pulumi:"mainClass"`
	// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
	MainJarFileUri pulumi.StringPtrInput `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
}

func (HadoopJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*HadoopJob)(nil)).Elem()
}

func (i HadoopJobArgs) ToHadoopJobOutput() HadoopJobOutput {
	return i.ToHadoopJobOutputWithContext(context.Background())
}

func (i HadoopJobArgs) ToHadoopJobOutputWithContext(ctx context.Context) HadoopJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HadoopJobOutput)
}

func (i HadoopJobArgs) ToHadoopJobPtrOutput() HadoopJobPtrOutput {
	return i.ToHadoopJobPtrOutputWithContext(context.Background())
}

func (i HadoopJobArgs) ToHadoopJobPtrOutputWithContext(ctx context.Context) HadoopJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HadoopJobOutput).ToHadoopJobPtrOutputWithContext(ctx)
}

// HadoopJobPtrInput is an input type that accepts HadoopJobArgs, HadoopJobPtr and HadoopJobPtrOutput values.
// You can construct a concrete instance of `HadoopJobPtrInput` via:
//
//          HadoopJobArgs{...}
//
//  or:
//
//          nil
type HadoopJobPtrInput interface {
	pulumi.Input

	ToHadoopJobPtrOutput() HadoopJobPtrOutput
	ToHadoopJobPtrOutputWithContext(context.Context) HadoopJobPtrOutput
}

type hadoopJobPtrType HadoopJobArgs

func HadoopJobPtr(v *HadoopJobArgs) HadoopJobPtrInput {
	return (*hadoopJobPtrType)(v)
}

func (*hadoopJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**HadoopJob)(nil)).Elem()
}

func (i *hadoopJobPtrType) ToHadoopJobPtrOutput() HadoopJobPtrOutput {
	return i.ToHadoopJobPtrOutputWithContext(context.Background())
}

func (i *hadoopJobPtrType) ToHadoopJobPtrOutputWithContext(ctx context.Context) HadoopJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HadoopJobPtrOutput)
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJobOutput struct{ *pulumi.OutputState }

func (HadoopJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*HadoopJob)(nil)).Elem()
}

func (o HadoopJobOutput) ToHadoopJobOutput() HadoopJobOutput {
	return o
}

func (o HadoopJobOutput) ToHadoopJobOutputWithContext(ctx context.Context) HadoopJobOutput {
	return o
}

func (o HadoopJobOutput) ToHadoopJobPtrOutput() HadoopJobPtrOutput {
	return o.ToHadoopJobPtrOutputWithContext(context.Background())
}

func (o HadoopJobOutput) ToHadoopJobPtrOutputWithContext(ctx context.Context) HadoopJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v HadoopJob) *HadoopJob {
		return &v
	}).(HadoopJobPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
func (o HadoopJobOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJob) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o HadoopJobOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJob) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
func (o HadoopJobOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJob) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
func (o HadoopJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o HadoopJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v HadoopJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o HadoopJobOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v HadoopJob) *string { return v.MainClass }).(pulumi.StringPtrOutput)
}

// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
func (o HadoopJobOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v HadoopJob) *string { return v.MainJarFileUri }).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
func (o HadoopJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v HadoopJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

type HadoopJobPtrOutput struct{ *pulumi.OutputState }

func (HadoopJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**HadoopJob)(nil)).Elem()
}

func (o HadoopJobPtrOutput) ToHadoopJobPtrOutput() HadoopJobPtrOutput {
	return o
}

func (o HadoopJobPtrOutput) ToHadoopJobPtrOutputWithContext(ctx context.Context) HadoopJobPtrOutput {
	return o
}

func (o HadoopJobPtrOutput) Elem() HadoopJobOutput {
	return o.ApplyT(func(v *HadoopJob) HadoopJob {
		if v != nil {
			return *v
		}
		var ret HadoopJob
		return ret
	}).(HadoopJobOutput)
}

// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
func (o HadoopJobPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HadoopJob) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o HadoopJobPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HadoopJob) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
func (o HadoopJobPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HadoopJob) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
func (o HadoopJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HadoopJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o HadoopJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *HadoopJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o HadoopJobPtrOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *HadoopJob) *string {
		if v == nil {
			return nil
		}
		return v.MainClass
	}).(pulumi.StringPtrOutput)
}

// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
func (o HadoopJobPtrOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *HadoopJob) *string {
		if v == nil {
			return nil
		}
		return v.MainJarFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
func (o HadoopJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *HadoopJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJobResponse struct {
	// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass string `pulumi:"mainClass"`
	// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
	MainJarFileUri string `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// A Dataproc job for running Apache Hadoop MapReduce (https://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html) jobs on Apache Hadoop YARN (https://hadoop.apache.org/docs/r2.7.1/hadoop-yarn/hadoop-yarn-site/YARN.html).
type HadoopJobResponseOutput struct{ *pulumi.OutputState }

func (HadoopJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*HadoopJobResponse)(nil)).Elem()
}

func (o HadoopJobResponseOutput) ToHadoopJobResponseOutput() HadoopJobResponseOutput {
	return o
}

func (o HadoopJobResponseOutput) ToHadoopJobResponseOutputWithContext(ctx context.Context) HadoopJobResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted in the working directory of Hadoop drivers and tasks. Supported file types: .jar, .tar, .tar.gz, .tgz, or .zip.
func (o HadoopJobResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJobResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as -libjars or -Dfoo=bar, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o HadoopJobResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJobResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS (Hadoop Compatible Filesystem) URIs of files to be copied to the working directory of Hadoop drivers and distributed tasks. Useful for naively parallel tasks.
func (o HadoopJobResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJobResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. Jar file URIs to add to the CLASSPATHs of the Hadoop driver and tasks.
func (o HadoopJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HadoopJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o HadoopJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v HadoopJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// The name of the driver's main class. The jar file containing the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o HadoopJobResponseOutput) MainClass() pulumi.StringOutput {
	return o.ApplyT(func(v HadoopJobResponse) string { return v.MainClass }).(pulumi.StringOutput)
}

// The HCFS URI of the jar file containing the main class. Examples: 'gs://foo-bucket/analytics-binaries/extract-useful-metrics-mr.jar' 'hdfs:/tmp/test-samples/custom-wordcount.jar' 'file:///home/usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'
func (o HadoopJobResponseOutput) MainJarFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v HadoopJobResponse) string { return v.MainJarFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure Hadoop. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site and classes in user code.
func (o HadoopJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v HadoopJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJob struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure *bool `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains Hive queries.
	QueryFileUri *string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList *QueryList `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// HiveJobInput is an input type that accepts HiveJobArgs and HiveJobOutput values.
// You can construct a concrete instance of `HiveJobInput` via:
//
//          HiveJobArgs{...}
type HiveJobInput interface {
	pulumi.Input

	ToHiveJobOutput() HiveJobOutput
	ToHiveJobOutputWithContext(context.Context) HiveJobOutput
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJobArgs struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure pulumi.BoolPtrInput `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// The HCFS URI of the script that contains Hive queries.
	QueryFileUri pulumi.StringPtrInput `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListPtrInput `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
	ScriptVariables pulumi.StringMapInput `pulumi:"scriptVariables"`
}

func (HiveJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*HiveJob)(nil)).Elem()
}

func (i HiveJobArgs) ToHiveJobOutput() HiveJobOutput {
	return i.ToHiveJobOutputWithContext(context.Background())
}

func (i HiveJobArgs) ToHiveJobOutputWithContext(ctx context.Context) HiveJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HiveJobOutput)
}

func (i HiveJobArgs) ToHiveJobPtrOutput() HiveJobPtrOutput {
	return i.ToHiveJobPtrOutputWithContext(context.Background())
}

func (i HiveJobArgs) ToHiveJobPtrOutputWithContext(ctx context.Context) HiveJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HiveJobOutput).ToHiveJobPtrOutputWithContext(ctx)
}

// HiveJobPtrInput is an input type that accepts HiveJobArgs, HiveJobPtr and HiveJobPtrOutput values.
// You can construct a concrete instance of `HiveJobPtrInput` via:
//
//          HiveJobArgs{...}
//
//  or:
//
//          nil
type HiveJobPtrInput interface {
	pulumi.Input

	ToHiveJobPtrOutput() HiveJobPtrOutput
	ToHiveJobPtrOutputWithContext(context.Context) HiveJobPtrOutput
}

type hiveJobPtrType HiveJobArgs

func HiveJobPtr(v *HiveJobArgs) HiveJobPtrInput {
	return (*hiveJobPtrType)(v)
}

func (*hiveJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**HiveJob)(nil)).Elem()
}

func (i *hiveJobPtrType) ToHiveJobPtrOutput() HiveJobPtrOutput {
	return i.ToHiveJobPtrOutputWithContext(context.Background())
}

func (i *hiveJobPtrType) ToHiveJobPtrOutputWithContext(ctx context.Context) HiveJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(HiveJobPtrOutput)
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJobOutput struct{ *pulumi.OutputState }

func (HiveJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*HiveJob)(nil)).Elem()
}

func (o HiveJobOutput) ToHiveJobOutput() HiveJobOutput {
	return o
}

func (o HiveJobOutput) ToHiveJobOutputWithContext(ctx context.Context) HiveJobOutput {
	return o
}

func (o HiveJobOutput) ToHiveJobPtrOutput() HiveJobPtrOutput {
	return o.ToHiveJobPtrOutputWithContext(context.Background())
}

func (o HiveJobOutput) ToHiveJobPtrOutputWithContext(ctx context.Context) HiveJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v HiveJob) *HiveJob {
		return &v
	}).(HiveJobPtrOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o HiveJobOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v HiveJob) *bool { return v.ContinueOnFailure }).(pulumi.BoolPtrOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
func (o HiveJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HiveJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
func (o HiveJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v HiveJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains Hive queries.
func (o HiveJobOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v HiveJob) *string { return v.QueryFileUri }).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o HiveJobOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v HiveJob) *QueryList { return v.QueryList }).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
func (o HiveJobOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v HiveJob) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

type HiveJobPtrOutput struct{ *pulumi.OutputState }

func (HiveJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**HiveJob)(nil)).Elem()
}

func (o HiveJobPtrOutput) ToHiveJobPtrOutput() HiveJobPtrOutput {
	return o
}

func (o HiveJobPtrOutput) ToHiveJobPtrOutputWithContext(ctx context.Context) HiveJobPtrOutput {
	return o
}

func (o HiveJobPtrOutput) Elem() HiveJobOutput {
	return o.ApplyT(func(v *HiveJob) HiveJob {
		if v != nil {
			return *v
		}
		var ret HiveJob
		return ret
	}).(HiveJobOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o HiveJobPtrOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *HiveJob) *bool {
		if v == nil {
			return nil
		}
		return v.ContinueOnFailure
	}).(pulumi.BoolPtrOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
func (o HiveJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *HiveJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
func (o HiveJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *HiveJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains Hive queries.
func (o HiveJobPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *HiveJob) *string {
		if v == nil {
			return nil
		}
		return v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o HiveJobPtrOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v *HiveJob) *QueryList {
		if v == nil {
			return nil
		}
		return v.QueryList
	}).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
func (o HiveJobPtrOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v *HiveJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.ScriptVariables
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJobResponse struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure bool `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains Hive queries.
	QueryFileUri string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListResponse `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// A Dataproc job for running Apache Hive (https://hive.apache.org/) queries on YARN.
type HiveJobResponseOutput struct{ *pulumi.OutputState }

func (HiveJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*HiveJobResponse)(nil)).Elem()
}

func (o HiveJobResponseOutput) ToHiveJobResponseOutput() HiveJobResponseOutput {
	return o
}

func (o HiveJobResponseOutput) ToHiveJobResponseOutputWithContext(ctx context.Context) HiveJobResponseOutput {
	return o
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o HiveJobResponseOutput) ContinueOnFailure() pulumi.BoolOutput {
	return o.ApplyT(func(v HiveJobResponse) bool { return v.ContinueOnFailure }).(pulumi.BoolOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Hive server and Hadoop MapReduce (MR) tasks. Can contain Hive SerDes and UDFs.
func (o HiveJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v HiveJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. A mapping of property names and values, used to configure Hive. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/hive/conf/hive-site.xml, and classes in user code.
func (o HiveJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v HiveJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains Hive queries.
func (o HiveJobResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v HiveJobResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// A list of queries.
func (o HiveJobResponseOutput) QueryList() QueryListResponseOutput {
	return o.ApplyT(func(v HiveJobResponse) QueryListResponse { return v.QueryList }).(QueryListResponseOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Hive command: SET name="value";).
func (o HiveJobResponseOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v HiveJobResponse) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfig struct {
	// Map of user to service account.
	UserServiceAccountMapping map[string]string `pulumi:"userServiceAccountMapping"`
}

// IdentityConfigInput is an input type that accepts IdentityConfigArgs and IdentityConfigOutput values.
// You can construct a concrete instance of `IdentityConfigInput` via:
//
//          IdentityConfigArgs{...}
type IdentityConfigInput interface {
	pulumi.Input

	ToIdentityConfigOutput() IdentityConfigOutput
	ToIdentityConfigOutputWithContext(context.Context) IdentityConfigOutput
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfigArgs struct {
	// Map of user to service account.
	UserServiceAccountMapping pulumi.StringMapInput `pulumi:"userServiceAccountMapping"`
}

func (IdentityConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*IdentityConfig)(nil)).Elem()
}

func (i IdentityConfigArgs) ToIdentityConfigOutput() IdentityConfigOutput {
	return i.ToIdentityConfigOutputWithContext(context.Background())
}

func (i IdentityConfigArgs) ToIdentityConfigOutputWithContext(ctx context.Context) IdentityConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(IdentityConfigOutput)
}

func (i IdentityConfigArgs) ToIdentityConfigPtrOutput() IdentityConfigPtrOutput {
	return i.ToIdentityConfigPtrOutputWithContext(context.Background())
}

func (i IdentityConfigArgs) ToIdentityConfigPtrOutputWithContext(ctx context.Context) IdentityConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(IdentityConfigOutput).ToIdentityConfigPtrOutputWithContext(ctx)
}

// IdentityConfigPtrInput is an input type that accepts IdentityConfigArgs, IdentityConfigPtr and IdentityConfigPtrOutput values.
// You can construct a concrete instance of `IdentityConfigPtrInput` via:
//
//          IdentityConfigArgs{...}
//
//  or:
//
//          nil
type IdentityConfigPtrInput interface {
	pulumi.Input

	ToIdentityConfigPtrOutput() IdentityConfigPtrOutput
	ToIdentityConfigPtrOutputWithContext(context.Context) IdentityConfigPtrOutput
}

type identityConfigPtrType IdentityConfigArgs

func IdentityConfigPtr(v *IdentityConfigArgs) IdentityConfigPtrInput {
	return (*identityConfigPtrType)(v)
}

func (*identityConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**IdentityConfig)(nil)).Elem()
}

func (i *identityConfigPtrType) ToIdentityConfigPtrOutput() IdentityConfigPtrOutput {
	return i.ToIdentityConfigPtrOutputWithContext(context.Background())
}

func (i *identityConfigPtrType) ToIdentityConfigPtrOutputWithContext(ctx context.Context) IdentityConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(IdentityConfigPtrOutput)
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfigOutput struct{ *pulumi.OutputState }

func (IdentityConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*IdentityConfig)(nil)).Elem()
}

func (o IdentityConfigOutput) ToIdentityConfigOutput() IdentityConfigOutput {
	return o
}

func (o IdentityConfigOutput) ToIdentityConfigOutputWithContext(ctx context.Context) IdentityConfigOutput {
	return o
}

func (o IdentityConfigOutput) ToIdentityConfigPtrOutput() IdentityConfigPtrOutput {
	return o.ToIdentityConfigPtrOutputWithContext(context.Background())
}

func (o IdentityConfigOutput) ToIdentityConfigPtrOutputWithContext(ctx context.Context) IdentityConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v IdentityConfig) *IdentityConfig {
		return &v
	}).(IdentityConfigPtrOutput)
}

// Map of user to service account.
func (o IdentityConfigOutput) UserServiceAccountMapping() pulumi.StringMapOutput {
	return o.ApplyT(func(v IdentityConfig) map[string]string { return v.UserServiceAccountMapping }).(pulumi.StringMapOutput)
}

type IdentityConfigPtrOutput struct{ *pulumi.OutputState }

func (IdentityConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**IdentityConfig)(nil)).Elem()
}

func (o IdentityConfigPtrOutput) ToIdentityConfigPtrOutput() IdentityConfigPtrOutput {
	return o
}

func (o IdentityConfigPtrOutput) ToIdentityConfigPtrOutputWithContext(ctx context.Context) IdentityConfigPtrOutput {
	return o
}

func (o IdentityConfigPtrOutput) Elem() IdentityConfigOutput {
	return o.ApplyT(func(v *IdentityConfig) IdentityConfig {
		if v != nil {
			return *v
		}
		var ret IdentityConfig
		return ret
	}).(IdentityConfigOutput)
}

// Map of user to service account.
func (o IdentityConfigPtrOutput) UserServiceAccountMapping() pulumi.StringMapOutput {
	return o.ApplyT(func(v *IdentityConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.UserServiceAccountMapping
	}).(pulumi.StringMapOutput)
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfigResponse struct {
	// Map of user to service account.
	UserServiceAccountMapping map[string]string `pulumi:"userServiceAccountMapping"`
}

// Identity related configuration, including service account based secure multi-tenancy user mappings.
type IdentityConfigResponseOutput struct{ *pulumi.OutputState }

func (IdentityConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*IdentityConfigResponse)(nil)).Elem()
}

func (o IdentityConfigResponseOutput) ToIdentityConfigResponseOutput() IdentityConfigResponseOutput {
	return o
}

func (o IdentityConfigResponseOutput) ToIdentityConfigResponseOutputWithContext(ctx context.Context) IdentityConfigResponseOutput {
	return o
}

// Map of user to service account.
func (o IdentityConfigResponseOutput) UserServiceAccountMapping() pulumi.StringMapOutput {
	return o.ApplyT(func(v IdentityConfigResponse) map[string]string { return v.UserServiceAccountMapping }).(pulumi.StringMapOutput)
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfig struct {
	// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
	MaxInstances int `pulumi:"maxInstances"`
	// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
	MinInstances *int `pulumi:"minInstances"`
	// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
	Weight *int `pulumi:"weight"`
}

// InstanceGroupAutoscalingPolicyConfigInput is an input type that accepts InstanceGroupAutoscalingPolicyConfigArgs and InstanceGroupAutoscalingPolicyConfigOutput values.
// You can construct a concrete instance of `InstanceGroupAutoscalingPolicyConfigInput` via:
//
//          InstanceGroupAutoscalingPolicyConfigArgs{...}
type InstanceGroupAutoscalingPolicyConfigInput interface {
	pulumi.Input

	ToInstanceGroupAutoscalingPolicyConfigOutput() InstanceGroupAutoscalingPolicyConfigOutput
	ToInstanceGroupAutoscalingPolicyConfigOutputWithContext(context.Context) InstanceGroupAutoscalingPolicyConfigOutput
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfigArgs struct {
	// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
	MaxInstances pulumi.IntInput `pulumi:"maxInstances"`
	// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
	MinInstances pulumi.IntPtrInput `pulumi:"minInstances"`
	// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
	Weight pulumi.IntPtrInput `pulumi:"weight"`
}

func (InstanceGroupAutoscalingPolicyConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfig)(nil)).Elem()
}

func (i InstanceGroupAutoscalingPolicyConfigArgs) ToInstanceGroupAutoscalingPolicyConfigOutput() InstanceGroupAutoscalingPolicyConfigOutput {
	return i.ToInstanceGroupAutoscalingPolicyConfigOutputWithContext(context.Background())
}

func (i InstanceGroupAutoscalingPolicyConfigArgs) ToInstanceGroupAutoscalingPolicyConfigOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupAutoscalingPolicyConfigOutput)
}

func (i InstanceGroupAutoscalingPolicyConfigArgs) ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return i.ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(context.Background())
}

func (i InstanceGroupAutoscalingPolicyConfigArgs) ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupAutoscalingPolicyConfigOutput).ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx)
}

// InstanceGroupAutoscalingPolicyConfigPtrInput is an input type that accepts InstanceGroupAutoscalingPolicyConfigArgs, InstanceGroupAutoscalingPolicyConfigPtr and InstanceGroupAutoscalingPolicyConfigPtrOutput values.
// You can construct a concrete instance of `InstanceGroupAutoscalingPolicyConfigPtrInput` via:
//
//          InstanceGroupAutoscalingPolicyConfigArgs{...}
//
//  or:
//
//          nil
type InstanceGroupAutoscalingPolicyConfigPtrInput interface {
	pulumi.Input

	ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput
	ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput
}

type instanceGroupAutoscalingPolicyConfigPtrType InstanceGroupAutoscalingPolicyConfigArgs

func InstanceGroupAutoscalingPolicyConfigPtr(v *InstanceGroupAutoscalingPolicyConfigArgs) InstanceGroupAutoscalingPolicyConfigPtrInput {
	return (*instanceGroupAutoscalingPolicyConfigPtrType)(v)
}

func (*instanceGroupAutoscalingPolicyConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**InstanceGroupAutoscalingPolicyConfig)(nil)).Elem()
}

func (i *instanceGroupAutoscalingPolicyConfigPtrType) ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return i.ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(context.Background())
}

func (i *instanceGroupAutoscalingPolicyConfigPtrType) ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupAutoscalingPolicyConfigPtrOutput)
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfigOutput struct{ *pulumi.OutputState }

func (InstanceGroupAutoscalingPolicyConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfig)(nil)).Elem()
}

func (o InstanceGroupAutoscalingPolicyConfigOutput) ToInstanceGroupAutoscalingPolicyConfigOutput() InstanceGroupAutoscalingPolicyConfigOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigOutput) ToInstanceGroupAutoscalingPolicyConfigOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigOutput) ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return o.ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(context.Background())
}

func (o InstanceGroupAutoscalingPolicyConfigOutput) ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v InstanceGroupAutoscalingPolicyConfig) *InstanceGroupAutoscalingPolicyConfig {
		return &v
	}).(InstanceGroupAutoscalingPolicyConfigPtrOutput)
}

// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigOutput) MaxInstances() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfig) int { return v.MaxInstances }).(pulumi.IntOutput)
}

// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigOutput) MinInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfig) *int { return v.MinInstances }).(pulumi.IntPtrOutput)
}

// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
func (o InstanceGroupAutoscalingPolicyConfigOutput) Weight() pulumi.IntPtrOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfig) *int { return v.Weight }).(pulumi.IntPtrOutput)
}

type InstanceGroupAutoscalingPolicyConfigPtrOutput struct{ *pulumi.OutputState }

func (InstanceGroupAutoscalingPolicyConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**InstanceGroupAutoscalingPolicyConfig)(nil)).Elem()
}

func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) ToInstanceGroupAutoscalingPolicyConfigPtrOutput() InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) ToInstanceGroupAutoscalingPolicyConfigPtrOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigPtrOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) Elem() InstanceGroupAutoscalingPolicyConfigOutput {
	return o.ApplyT(func(v *InstanceGroupAutoscalingPolicyConfig) InstanceGroupAutoscalingPolicyConfig {
		if v != nil {
			return *v
		}
		var ret InstanceGroupAutoscalingPolicyConfig
		return ret
	}).(InstanceGroupAutoscalingPolicyConfigOutput)
}

// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) MaxInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *InstanceGroupAutoscalingPolicyConfig) *int {
		if v == nil {
			return nil
		}
		return &v.MaxInstances
	}).(pulumi.IntPtrOutput)
}

// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) MinInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *InstanceGroupAutoscalingPolicyConfig) *int {
		if v == nil {
			return nil
		}
		return v.MinInstances
	}).(pulumi.IntPtrOutput)
}

// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
func (o InstanceGroupAutoscalingPolicyConfigPtrOutput) Weight() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *InstanceGroupAutoscalingPolicyConfig) *int {
		if v == nil {
			return nil
		}
		return v.Weight
	}).(pulumi.IntPtrOutput)
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfigResponse struct {
	// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
	MaxInstances int `pulumi:"maxInstances"`
	// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
	MinInstances int `pulumi:"minInstances"`
	// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
	Weight int `pulumi:"weight"`
}

// Configuration for the size bounds of an instance group, including its proportional size to other groups.
type InstanceGroupAutoscalingPolicyConfigResponseOutput struct{ *pulumi.OutputState }

func (InstanceGroupAutoscalingPolicyConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfigResponse)(nil)).Elem()
}

func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) ToInstanceGroupAutoscalingPolicyConfigResponseOutput() InstanceGroupAutoscalingPolicyConfigResponseOutput {
	return o
}

func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) ToInstanceGroupAutoscalingPolicyConfigResponseOutputWithContext(ctx context.Context) InstanceGroupAutoscalingPolicyConfigResponseOutput {
	return o
}

// Maximum number of instances for this group. Required for primary workers. Note that by default, clusters will not use secondary workers. Required for secondary workers if the minimum secondary instances is set.Primary workers - Bounds: [min_instances, ). Secondary workers - Bounds: [min_instances, ). Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) MaxInstances() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfigResponse) int { return v.MaxInstances }).(pulumi.IntOutput)
}

// Optional. Minimum number of instances for this group.Primary workers - Bounds: 2, max_instances. Default: 2. Secondary workers - Bounds: 0, max_instances. Default: 0.
func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) MinInstances() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfigResponse) int { return v.MinInstances }).(pulumi.IntOutput)
}

// Optional. Weight for the instance group, which is used to determine the fraction of total workers in the cluster from this instance group. For example, if primary workers have weight 2, and secondary workers have weight 1, the cluster will have approximately 2 primary workers for each secondary worker.The cluster may not reach the specified balance if constrained by min/max bounds or other autoscaling settings. For example, if max_instances for secondary workers is 0, then only primary workers will be added. The cluster can also be out of balance when created.If weight is not set on any instance group, the cluster will default to equal weight for all groups: the cluster will attempt to maintain an equal number of workers in each group within the configured size bounds for each group. If weight is set for one group only, the cluster will default to zero weight on the unset group. For example if weight is set only on primary workers, the cluster will use primary workers only and no secondary workers.
func (o InstanceGroupAutoscalingPolicyConfigResponseOutput) Weight() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupAutoscalingPolicyConfigResponse) int { return v.Weight }).(pulumi.IntOutput)
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfig struct {
	// Optional. The Compute Engine accelerator configuration for these instances.
	Accelerators []AcceleratorConfig `pulumi:"accelerators"`
	// Optional. Disk option config settings.
	DiskConfig *DiskConfig `pulumi:"diskConfig"`
	// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
	ImageUri *string `pulumi:"imageUri"`
	// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
	MachineTypeUri *string `pulumi:"machineTypeUri"`
	// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
	MinCpuPlatform *string `pulumi:"minCpuPlatform"`
	// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
	NumInstances *int `pulumi:"numInstances"`
	// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
	Preemptibility *InstanceGroupConfigPreemptibility `pulumi:"preemptibility"`
}

// InstanceGroupConfigInput is an input type that accepts InstanceGroupConfigArgs and InstanceGroupConfigOutput values.
// You can construct a concrete instance of `InstanceGroupConfigInput` via:
//
//          InstanceGroupConfigArgs{...}
type InstanceGroupConfigInput interface {
	pulumi.Input

	ToInstanceGroupConfigOutput() InstanceGroupConfigOutput
	ToInstanceGroupConfigOutputWithContext(context.Context) InstanceGroupConfigOutput
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfigArgs struct {
	// Optional. The Compute Engine accelerator configuration for these instances.
	Accelerators AcceleratorConfigArrayInput `pulumi:"accelerators"`
	// Optional. Disk option config settings.
	DiskConfig DiskConfigPtrInput `pulumi:"diskConfig"`
	// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
	ImageUri pulumi.StringPtrInput `pulumi:"imageUri"`
	// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
	MachineTypeUri pulumi.StringPtrInput `pulumi:"machineTypeUri"`
	// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
	MinCpuPlatform pulumi.StringPtrInput `pulumi:"minCpuPlatform"`
	// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
	NumInstances pulumi.IntPtrInput `pulumi:"numInstances"`
	// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
	Preemptibility InstanceGroupConfigPreemptibilityPtrInput `pulumi:"preemptibility"`
}

func (InstanceGroupConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupConfig)(nil)).Elem()
}

func (i InstanceGroupConfigArgs) ToInstanceGroupConfigOutput() InstanceGroupConfigOutput {
	return i.ToInstanceGroupConfigOutputWithContext(context.Background())
}

func (i InstanceGroupConfigArgs) ToInstanceGroupConfigOutputWithContext(ctx context.Context) InstanceGroupConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupConfigOutput)
}

func (i InstanceGroupConfigArgs) ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput {
	return i.ToInstanceGroupConfigPtrOutputWithContext(context.Background())
}

func (i InstanceGroupConfigArgs) ToInstanceGroupConfigPtrOutputWithContext(ctx context.Context) InstanceGroupConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupConfigOutput).ToInstanceGroupConfigPtrOutputWithContext(ctx)
}

// InstanceGroupConfigPtrInput is an input type that accepts InstanceGroupConfigArgs, InstanceGroupConfigPtr and InstanceGroupConfigPtrOutput values.
// You can construct a concrete instance of `InstanceGroupConfigPtrInput` via:
//
//          InstanceGroupConfigArgs{...}
//
//  or:
//
//          nil
type InstanceGroupConfigPtrInput interface {
	pulumi.Input

	ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput
	ToInstanceGroupConfigPtrOutputWithContext(context.Context) InstanceGroupConfigPtrOutput
}

type instanceGroupConfigPtrType InstanceGroupConfigArgs

func InstanceGroupConfigPtr(v *InstanceGroupConfigArgs) InstanceGroupConfigPtrInput {
	return (*instanceGroupConfigPtrType)(v)
}

func (*instanceGroupConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**InstanceGroupConfig)(nil)).Elem()
}

func (i *instanceGroupConfigPtrType) ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput {
	return i.ToInstanceGroupConfigPtrOutputWithContext(context.Background())
}

func (i *instanceGroupConfigPtrType) ToInstanceGroupConfigPtrOutputWithContext(ctx context.Context) InstanceGroupConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(InstanceGroupConfigPtrOutput)
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfigOutput struct{ *pulumi.OutputState }

func (InstanceGroupConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupConfig)(nil)).Elem()
}

func (o InstanceGroupConfigOutput) ToInstanceGroupConfigOutput() InstanceGroupConfigOutput {
	return o
}

func (o InstanceGroupConfigOutput) ToInstanceGroupConfigOutputWithContext(ctx context.Context) InstanceGroupConfigOutput {
	return o
}

func (o InstanceGroupConfigOutput) ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput {
	return o.ToInstanceGroupConfigPtrOutputWithContext(context.Background())
}

func (o InstanceGroupConfigOutput) ToInstanceGroupConfigPtrOutputWithContext(ctx context.Context) InstanceGroupConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v InstanceGroupConfig) *InstanceGroupConfig {
		return &v
	}).(InstanceGroupConfigPtrOutput)
}

// Optional. The Compute Engine accelerator configuration for these instances.
func (o InstanceGroupConfigOutput) Accelerators() AcceleratorConfigArrayOutput {
	return o.ApplyT(func(v InstanceGroupConfig) []AcceleratorConfig { return v.Accelerators }).(AcceleratorConfigArrayOutput)
}

// Optional. Disk option config settings.
func (o InstanceGroupConfigOutput) DiskConfig() DiskConfigPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *DiskConfig { return v.DiskConfig }).(DiskConfigPtrOutput)
}

// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
func (o InstanceGroupConfigOutput) ImageUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *string { return v.ImageUri }).(pulumi.StringPtrOutput)
}

// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
func (o InstanceGroupConfigOutput) MachineTypeUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *string { return v.MachineTypeUri }).(pulumi.StringPtrOutput)
}

// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
func (o InstanceGroupConfigOutput) MinCpuPlatform() pulumi.StringPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *string { return v.MinCpuPlatform }).(pulumi.StringPtrOutput)
}

// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
func (o InstanceGroupConfigOutput) NumInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *int { return v.NumInstances }).(pulumi.IntPtrOutput)
}

// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
func (o InstanceGroupConfigOutput) Preemptibility() InstanceGroupConfigPreemptibilityPtrOutput {
	return o.ApplyT(func(v InstanceGroupConfig) *InstanceGroupConfigPreemptibility { return v.Preemptibility }).(InstanceGroupConfigPreemptibilityPtrOutput)
}

type InstanceGroupConfigPtrOutput struct{ *pulumi.OutputState }

func (InstanceGroupConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**InstanceGroupConfig)(nil)).Elem()
}

func (o InstanceGroupConfigPtrOutput) ToInstanceGroupConfigPtrOutput() InstanceGroupConfigPtrOutput {
	return o
}

func (o InstanceGroupConfigPtrOutput) ToInstanceGroupConfigPtrOutputWithContext(ctx context.Context) InstanceGroupConfigPtrOutput {
	return o
}

func (o InstanceGroupConfigPtrOutput) Elem() InstanceGroupConfigOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) InstanceGroupConfig {
		if v != nil {
			return *v
		}
		var ret InstanceGroupConfig
		return ret
	}).(InstanceGroupConfigOutput)
}

// Optional. The Compute Engine accelerator configuration for these instances.
func (o InstanceGroupConfigPtrOutput) Accelerators() AcceleratorConfigArrayOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) []AcceleratorConfig {
		if v == nil {
			return nil
		}
		return v.Accelerators
	}).(AcceleratorConfigArrayOutput)
}

// Optional. Disk option config settings.
func (o InstanceGroupConfigPtrOutput) DiskConfig() DiskConfigPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *DiskConfig {
		if v == nil {
			return nil
		}
		return v.DiskConfig
	}).(DiskConfigPtrOutput)
}

// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
func (o InstanceGroupConfigPtrOutput) ImageUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *string {
		if v == nil {
			return nil
		}
		return v.ImageUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
func (o InstanceGroupConfigPtrOutput) MachineTypeUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *string {
		if v == nil {
			return nil
		}
		return v.MachineTypeUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
func (o InstanceGroupConfigPtrOutput) MinCpuPlatform() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *string {
		if v == nil {
			return nil
		}
		return v.MinCpuPlatform
	}).(pulumi.StringPtrOutput)
}

// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
func (o InstanceGroupConfigPtrOutput) NumInstances() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *int {
		if v == nil {
			return nil
		}
		return v.NumInstances
	}).(pulumi.IntPtrOutput)
}

// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
func (o InstanceGroupConfigPtrOutput) Preemptibility() InstanceGroupConfigPreemptibilityPtrOutput {
	return o.ApplyT(func(v *InstanceGroupConfig) *InstanceGroupConfigPreemptibility {
		if v == nil {
			return nil
		}
		return v.Preemptibility
	}).(InstanceGroupConfigPreemptibilityPtrOutput)
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfigResponse struct {
	// Optional. The Compute Engine accelerator configuration for these instances.
	Accelerators []AcceleratorConfigResponse `pulumi:"accelerators"`
	// Optional. Disk option config settings.
	DiskConfig DiskConfigResponse `pulumi:"diskConfig"`
	// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
	ImageUri string `pulumi:"imageUri"`
	// The list of instance names. Dataproc derives the names from cluster_name, num_instances, and the instance group.
	InstanceNames []string `pulumi:"instanceNames"`
	// List of references to Compute Engine instances.
	InstanceReferences []InstanceReferenceResponse `pulumi:"instanceReferences"`
	// Specifies that this instance group contains preemptible instances.
	IsPreemptible bool `pulumi:"isPreemptible"`
	// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
	MachineTypeUri string `pulumi:"machineTypeUri"`
	// The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
	ManagedGroupConfig ManagedGroupConfigResponse `pulumi:"managedGroupConfig"`
	// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
	MinCpuPlatform string `pulumi:"minCpuPlatform"`
	// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
	NumInstances int `pulumi:"numInstances"`
	// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
	Preemptibility string `pulumi:"preemptibility"`
}

// The config settings for Compute Engine resources in an instance group, such as a master or worker group.
type InstanceGroupConfigResponseOutput struct{ *pulumi.OutputState }

func (InstanceGroupConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceGroupConfigResponse)(nil)).Elem()
}

func (o InstanceGroupConfigResponseOutput) ToInstanceGroupConfigResponseOutput() InstanceGroupConfigResponseOutput {
	return o
}

func (o InstanceGroupConfigResponseOutput) ToInstanceGroupConfigResponseOutputWithContext(ctx context.Context) InstanceGroupConfigResponseOutput {
	return o
}

// Optional. The Compute Engine accelerator configuration for these instances.
func (o InstanceGroupConfigResponseOutput) Accelerators() AcceleratorConfigResponseArrayOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) []AcceleratorConfigResponse { return v.Accelerators }).(AcceleratorConfigResponseArrayOutput)
}

// Optional. Disk option config settings.
func (o InstanceGroupConfigResponseOutput) DiskConfig() DiskConfigResponseOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) DiskConfigResponse { return v.DiskConfig }).(DiskConfigResponseOutput)
}

// Optional. The Compute Engine image resource used for cluster instances.The URI can represent an image or image family.Image examples: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/[image-id] projects/[project_id]/global/images/[image-id] image-idImage family examples. Dataproc will use the most recent image from the family: https://www.googleapis.com/compute/beta/projects/[project_id]/global/images/family/[custom-image-family-name] projects/[project_id]/global/images/family/[custom-image-family-name]If the URI is unspecified, it will be inferred from SoftwareConfig.image_version or the system default.
func (o InstanceGroupConfigResponseOutput) ImageUri() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) string { return v.ImageUri }).(pulumi.StringOutput)
}

// The list of instance names. Dataproc derives the names from cluster_name, num_instances, and the instance group.
func (o InstanceGroupConfigResponseOutput) InstanceNames() pulumi.StringArrayOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) []string { return v.InstanceNames }).(pulumi.StringArrayOutput)
}

// List of references to Compute Engine instances.
func (o InstanceGroupConfigResponseOutput) InstanceReferences() InstanceReferenceResponseArrayOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) []InstanceReferenceResponse { return v.InstanceReferences }).(InstanceReferenceResponseArrayOutput)
}

// Specifies that this instance group contains preemptible instances.
func (o InstanceGroupConfigResponseOutput) IsPreemptible() pulumi.BoolOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) bool { return v.IsPreemptible }).(pulumi.BoolOutput)
}

// Optional. The Compute Engine machine type used for cluster instances.A full URL, partial URI, or short name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 projects/[project_id]/zones/us-east1-a/machineTypes/n1-standard-2 n1-standard-2Auto Zone Exception: If you are using the Dataproc Auto Zone Placement (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone#using_auto_zone_placement) feature, you must use the short name of the machine type resource, for example, n1-standard-2.
func (o InstanceGroupConfigResponseOutput) MachineTypeUri() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) string { return v.MachineTypeUri }).(pulumi.StringOutput)
}

// The config for Compute Engine Instance Group Manager that manages this group. This is only used for preemptible instance groups.
func (o InstanceGroupConfigResponseOutput) ManagedGroupConfig() ManagedGroupConfigResponseOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) ManagedGroupConfigResponse { return v.ManagedGroupConfig }).(ManagedGroupConfigResponseOutput)
}

// Optional. Specifies the minimum cpu platform for the Instance Group. See Dataproc -> Minimum CPU Platform (https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-min-cpu).
func (o InstanceGroupConfigResponseOutput) MinCpuPlatform() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) string { return v.MinCpuPlatform }).(pulumi.StringOutput)
}

// Optional. The number of VM instances in the instance group. For HA cluster master_config groups, must be set to 3. For standard cluster master_config groups, must be set to 1.
func (o InstanceGroupConfigResponseOutput) NumInstances() pulumi.IntOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) int { return v.NumInstances }).(pulumi.IntOutput)
}

// Optional. Specifies the preemptibility of the instance group.The default value for master and worker groups is NON_PREEMPTIBLE. This default cannot be changed.The default value for secondary instances is PREEMPTIBLE.
func (o InstanceGroupConfigResponseOutput) Preemptibility() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceGroupConfigResponse) string { return v.Preemptibility }).(pulumi.StringOutput)
}

// A reference to a Compute Engine instance.
type InstanceReferenceResponse struct {
	// The unique identifier of the Compute Engine instance.
	InstanceId string `pulumi:"instanceId"`
	// The user-friendly name of the Compute Engine instance.
	InstanceName string `pulumi:"instanceName"`
	// The public ECIES key used for sharing data with this instance.
	PublicEciesKey string `pulumi:"publicEciesKey"`
	// The public RSA key used for sharing data with this instance.
	PublicKey string `pulumi:"publicKey"`
}

// A reference to a Compute Engine instance.
type InstanceReferenceResponseOutput struct{ *pulumi.OutputState }

func (InstanceReferenceResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*InstanceReferenceResponse)(nil)).Elem()
}

func (o InstanceReferenceResponseOutput) ToInstanceReferenceResponseOutput() InstanceReferenceResponseOutput {
	return o
}

func (o InstanceReferenceResponseOutput) ToInstanceReferenceResponseOutputWithContext(ctx context.Context) InstanceReferenceResponseOutput {
	return o
}

// The unique identifier of the Compute Engine instance.
func (o InstanceReferenceResponseOutput) InstanceId() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceReferenceResponse) string { return v.InstanceId }).(pulumi.StringOutput)
}

// The user-friendly name of the Compute Engine instance.
func (o InstanceReferenceResponseOutput) InstanceName() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceReferenceResponse) string { return v.InstanceName }).(pulumi.StringOutput)
}

// The public ECIES key used for sharing data with this instance.
func (o InstanceReferenceResponseOutput) PublicEciesKey() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceReferenceResponse) string { return v.PublicEciesKey }).(pulumi.StringOutput)
}

// The public RSA key used for sharing data with this instance.
func (o InstanceReferenceResponseOutput) PublicKey() pulumi.StringOutput {
	return o.ApplyT(func(v InstanceReferenceResponse) string { return v.PublicKey }).(pulumi.StringOutput)
}

type InstanceReferenceResponseArrayOutput struct{ *pulumi.OutputState }

func (InstanceReferenceResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]InstanceReferenceResponse)(nil)).Elem()
}

func (o InstanceReferenceResponseArrayOutput) ToInstanceReferenceResponseArrayOutput() InstanceReferenceResponseArrayOutput {
	return o
}

func (o InstanceReferenceResponseArrayOutput) ToInstanceReferenceResponseArrayOutputWithContext(ctx context.Context) InstanceReferenceResponseArrayOutput {
	return o
}

func (o InstanceReferenceResponseArrayOutput) Index(i pulumi.IntInput) InstanceReferenceResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) InstanceReferenceResponse {
		return vs[0].([]InstanceReferenceResponse)[vs[1].(int)]
	}).(InstanceReferenceResponseOutput)
}

// Dataproc job config.
type JobPlacement struct {
	// Optional. Cluster labels to identify a cluster where the job will be submitted.
	ClusterLabels map[string]string `pulumi:"clusterLabels"`
	// The name of the cluster where the job will be submitted.
	ClusterName string `pulumi:"clusterName"`
}

// JobPlacementInput is an input type that accepts JobPlacementArgs and JobPlacementOutput values.
// You can construct a concrete instance of `JobPlacementInput` via:
//
//          JobPlacementArgs{...}
type JobPlacementInput interface {
	pulumi.Input

	ToJobPlacementOutput() JobPlacementOutput
	ToJobPlacementOutputWithContext(context.Context) JobPlacementOutput
}

// Dataproc job config.
type JobPlacementArgs struct {
	// Optional. Cluster labels to identify a cluster where the job will be submitted.
	ClusterLabels pulumi.StringMapInput `pulumi:"clusterLabels"`
	// The name of the cluster where the job will be submitted.
	ClusterName pulumi.StringInput `pulumi:"clusterName"`
}

func (JobPlacementArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*JobPlacement)(nil)).Elem()
}

func (i JobPlacementArgs) ToJobPlacementOutput() JobPlacementOutput {
	return i.ToJobPlacementOutputWithContext(context.Background())
}

func (i JobPlacementArgs) ToJobPlacementOutputWithContext(ctx context.Context) JobPlacementOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobPlacementOutput)
}

// Dataproc job config.
type JobPlacementOutput struct{ *pulumi.OutputState }

func (JobPlacementOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobPlacement)(nil)).Elem()
}

func (o JobPlacementOutput) ToJobPlacementOutput() JobPlacementOutput {
	return o
}

func (o JobPlacementOutput) ToJobPlacementOutputWithContext(ctx context.Context) JobPlacementOutput {
	return o
}

// Optional. Cluster labels to identify a cluster where the job will be submitted.
func (o JobPlacementOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v JobPlacement) map[string]string { return v.ClusterLabels }).(pulumi.StringMapOutput)
}

// The name of the cluster where the job will be submitted.
func (o JobPlacementOutput) ClusterName() pulumi.StringOutput {
	return o.ApplyT(func(v JobPlacement) string { return v.ClusterName }).(pulumi.StringOutput)
}

// Dataproc job config.
type JobPlacementResponse struct {
	// Optional. Cluster labels to identify a cluster where the job will be submitted.
	ClusterLabels map[string]string `pulumi:"clusterLabels"`
	// The name of the cluster where the job will be submitted.
	ClusterName string `pulumi:"clusterName"`
	// A cluster UUID generated by the Dataproc service when the job is submitted.
	ClusterUuid string `pulumi:"clusterUuid"`
}

// Dataproc job config.
type JobPlacementResponseOutput struct{ *pulumi.OutputState }

func (JobPlacementResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobPlacementResponse)(nil)).Elem()
}

func (o JobPlacementResponseOutput) ToJobPlacementResponseOutput() JobPlacementResponseOutput {
	return o
}

func (o JobPlacementResponseOutput) ToJobPlacementResponseOutputWithContext(ctx context.Context) JobPlacementResponseOutput {
	return o
}

// Optional. Cluster labels to identify a cluster where the job will be submitted.
func (o JobPlacementResponseOutput) ClusterLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v JobPlacementResponse) map[string]string { return v.ClusterLabels }).(pulumi.StringMapOutput)
}

// The name of the cluster where the job will be submitted.
func (o JobPlacementResponseOutput) ClusterName() pulumi.StringOutput {
	return o.ApplyT(func(v JobPlacementResponse) string { return v.ClusterName }).(pulumi.StringOutput)
}

// A cluster UUID generated by the Dataproc service when the job is submitted.
func (o JobPlacementResponseOutput) ClusterUuid() pulumi.StringOutput {
	return o.ApplyT(func(v JobPlacementResponse) string { return v.ClusterUuid }).(pulumi.StringOutput)
}

// Encapsulates the full scoping used to reference a job.
type JobReference struct {
	// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
	JobId *string `pulumi:"jobId"`
	// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
	Project *string `pulumi:"project"`
}

// JobReferenceInput is an input type that accepts JobReferenceArgs and JobReferenceOutput values.
// You can construct a concrete instance of `JobReferenceInput` via:
//
//          JobReferenceArgs{...}
type JobReferenceInput interface {
	pulumi.Input

	ToJobReferenceOutput() JobReferenceOutput
	ToJobReferenceOutputWithContext(context.Context) JobReferenceOutput
}

// Encapsulates the full scoping used to reference a job.
type JobReferenceArgs struct {
	// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
	JobId pulumi.StringPtrInput `pulumi:"jobId"`
	// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
	Project pulumi.StringPtrInput `pulumi:"project"`
}

func (JobReferenceArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*JobReference)(nil)).Elem()
}

func (i JobReferenceArgs) ToJobReferenceOutput() JobReferenceOutput {
	return i.ToJobReferenceOutputWithContext(context.Background())
}

func (i JobReferenceArgs) ToJobReferenceOutputWithContext(ctx context.Context) JobReferenceOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobReferenceOutput)
}

func (i JobReferenceArgs) ToJobReferencePtrOutput() JobReferencePtrOutput {
	return i.ToJobReferencePtrOutputWithContext(context.Background())
}

func (i JobReferenceArgs) ToJobReferencePtrOutputWithContext(ctx context.Context) JobReferencePtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobReferenceOutput).ToJobReferencePtrOutputWithContext(ctx)
}

// JobReferencePtrInput is an input type that accepts JobReferenceArgs, JobReferencePtr and JobReferencePtrOutput values.
// You can construct a concrete instance of `JobReferencePtrInput` via:
//
//          JobReferenceArgs{...}
//
//  or:
//
//          nil
type JobReferencePtrInput interface {
	pulumi.Input

	ToJobReferencePtrOutput() JobReferencePtrOutput
	ToJobReferencePtrOutputWithContext(context.Context) JobReferencePtrOutput
}

type jobReferencePtrType JobReferenceArgs

func JobReferencePtr(v *JobReferenceArgs) JobReferencePtrInput {
	return (*jobReferencePtrType)(v)
}

func (*jobReferencePtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**JobReference)(nil)).Elem()
}

func (i *jobReferencePtrType) ToJobReferencePtrOutput() JobReferencePtrOutput {
	return i.ToJobReferencePtrOutputWithContext(context.Background())
}

func (i *jobReferencePtrType) ToJobReferencePtrOutputWithContext(ctx context.Context) JobReferencePtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobReferencePtrOutput)
}

// Encapsulates the full scoping used to reference a job.
type JobReferenceOutput struct{ *pulumi.OutputState }

func (JobReferenceOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobReference)(nil)).Elem()
}

func (o JobReferenceOutput) ToJobReferenceOutput() JobReferenceOutput {
	return o
}

func (o JobReferenceOutput) ToJobReferenceOutputWithContext(ctx context.Context) JobReferenceOutput {
	return o
}

func (o JobReferenceOutput) ToJobReferencePtrOutput() JobReferencePtrOutput {
	return o.ToJobReferencePtrOutputWithContext(context.Background())
}

func (o JobReferenceOutput) ToJobReferencePtrOutputWithContext(ctx context.Context) JobReferencePtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v JobReference) *JobReference {
		return &v
	}).(JobReferencePtrOutput)
}

// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
func (o JobReferenceOutput) JobId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v JobReference) *string { return v.JobId }).(pulumi.StringPtrOutput)
}

// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
func (o JobReferenceOutput) Project() pulumi.StringPtrOutput {
	return o.ApplyT(func(v JobReference) *string { return v.Project }).(pulumi.StringPtrOutput)
}

type JobReferencePtrOutput struct{ *pulumi.OutputState }

func (JobReferencePtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**JobReference)(nil)).Elem()
}

func (o JobReferencePtrOutput) ToJobReferencePtrOutput() JobReferencePtrOutput {
	return o
}

func (o JobReferencePtrOutput) ToJobReferencePtrOutputWithContext(ctx context.Context) JobReferencePtrOutput {
	return o
}

func (o JobReferencePtrOutput) Elem() JobReferenceOutput {
	return o.ApplyT(func(v *JobReference) JobReference {
		if v != nil {
			return *v
		}
		var ret JobReference
		return ret
	}).(JobReferenceOutput)
}

// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
func (o JobReferencePtrOutput) JobId() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *JobReference) *string {
		if v == nil {
			return nil
		}
		return v.JobId
	}).(pulumi.StringPtrOutput)
}

// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
func (o JobReferencePtrOutput) Project() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *JobReference) *string {
		if v == nil {
			return nil
		}
		return v.Project
	}).(pulumi.StringPtrOutput)
}

// Encapsulates the full scoping used to reference a job.
type JobReferenceResponse struct {
	// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
	JobId string `pulumi:"jobId"`
	// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
	Project string `pulumi:"project"`
}

// Encapsulates the full scoping used to reference a job.
type JobReferenceResponseOutput struct{ *pulumi.OutputState }

func (JobReferenceResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobReferenceResponse)(nil)).Elem()
}

func (o JobReferenceResponseOutput) ToJobReferenceResponseOutput() JobReferenceResponseOutput {
	return o
}

func (o JobReferenceResponseOutput) ToJobReferenceResponseOutputWithContext(ctx context.Context) JobReferenceResponseOutput {
	return o
}

// Optional. The job ID, which must be unique within the project.The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or hyphens (-). The maximum length is 100 characters.If not specified by the caller, the job ID will be provided by the server.
func (o JobReferenceResponseOutput) JobId() pulumi.StringOutput {
	return o.ApplyT(func(v JobReferenceResponse) string { return v.JobId }).(pulumi.StringOutput)
}

// Optional. The ID of the Google Cloud Platform project that the job belongs to. If specified, must match the request project ID.
func (o JobReferenceResponseOutput) Project() pulumi.StringOutput {
	return o.ApplyT(func(v JobReferenceResponse) string { return v.Project }).(pulumi.StringOutput)
}

// Job scheduling options.
type JobScheduling struct {
	// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window.Maximum value is 10.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
	MaxFailuresPerHour *int `pulumi:"maxFailuresPerHour"`
	// Optional. Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
	MaxFailuresTotal *int `pulumi:"maxFailuresTotal"`
}

// JobSchedulingInput is an input type that accepts JobSchedulingArgs and JobSchedulingOutput values.
// You can construct a concrete instance of `JobSchedulingInput` via:
//
//          JobSchedulingArgs{...}
type JobSchedulingInput interface {
	pulumi.Input

	ToJobSchedulingOutput() JobSchedulingOutput
	ToJobSchedulingOutputWithContext(context.Context) JobSchedulingOutput
}

// Job scheduling options.
type JobSchedulingArgs struct {
	// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window.Maximum value is 10.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
	MaxFailuresPerHour pulumi.IntPtrInput `pulumi:"maxFailuresPerHour"`
	// Optional. Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
	MaxFailuresTotal pulumi.IntPtrInput `pulumi:"maxFailuresTotal"`
}

func (JobSchedulingArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*JobScheduling)(nil)).Elem()
}

func (i JobSchedulingArgs) ToJobSchedulingOutput() JobSchedulingOutput {
	return i.ToJobSchedulingOutputWithContext(context.Background())
}

func (i JobSchedulingArgs) ToJobSchedulingOutputWithContext(ctx context.Context) JobSchedulingOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobSchedulingOutput)
}

func (i JobSchedulingArgs) ToJobSchedulingPtrOutput() JobSchedulingPtrOutput {
	return i.ToJobSchedulingPtrOutputWithContext(context.Background())
}

func (i JobSchedulingArgs) ToJobSchedulingPtrOutputWithContext(ctx context.Context) JobSchedulingPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobSchedulingOutput).ToJobSchedulingPtrOutputWithContext(ctx)
}

// JobSchedulingPtrInput is an input type that accepts JobSchedulingArgs, JobSchedulingPtr and JobSchedulingPtrOutput values.
// You can construct a concrete instance of `JobSchedulingPtrInput` via:
//
//          JobSchedulingArgs{...}
//
//  or:
//
//          nil
type JobSchedulingPtrInput interface {
	pulumi.Input

	ToJobSchedulingPtrOutput() JobSchedulingPtrOutput
	ToJobSchedulingPtrOutputWithContext(context.Context) JobSchedulingPtrOutput
}

type jobSchedulingPtrType JobSchedulingArgs

func JobSchedulingPtr(v *JobSchedulingArgs) JobSchedulingPtrInput {
	return (*jobSchedulingPtrType)(v)
}

func (*jobSchedulingPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**JobScheduling)(nil)).Elem()
}

func (i *jobSchedulingPtrType) ToJobSchedulingPtrOutput() JobSchedulingPtrOutput {
	return i.ToJobSchedulingPtrOutputWithContext(context.Background())
}

func (i *jobSchedulingPtrType) ToJobSchedulingPtrOutputWithContext(ctx context.Context) JobSchedulingPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(JobSchedulingPtrOutput)
}

// Job scheduling options.
type JobSchedulingOutput struct{ *pulumi.OutputState }

func (JobSchedulingOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobScheduling)(nil)).Elem()
}

func (o JobSchedulingOutput) ToJobSchedulingOutput() JobSchedulingOutput {
	return o
}

func (o JobSchedulingOutput) ToJobSchedulingOutputWithContext(ctx context.Context) JobSchedulingOutput {
	return o
}

func (o JobSchedulingOutput) ToJobSchedulingPtrOutput() JobSchedulingPtrOutput {
	return o.ToJobSchedulingPtrOutputWithContext(context.Background())
}

func (o JobSchedulingOutput) ToJobSchedulingPtrOutputWithContext(ctx context.Context) JobSchedulingPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v JobScheduling) *JobScheduling {
		return &v
	}).(JobSchedulingPtrOutput)
}

// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window.Maximum value is 10.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
func (o JobSchedulingOutput) MaxFailuresPerHour() pulumi.IntPtrOutput {
	return o.ApplyT(func(v JobScheduling) *int { return v.MaxFailuresPerHour }).(pulumi.IntPtrOutput)
}

// Optional. Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
func (o JobSchedulingOutput) MaxFailuresTotal() pulumi.IntPtrOutput {
	return o.ApplyT(func(v JobScheduling) *int { return v.MaxFailuresTotal }).(pulumi.IntPtrOutput)
}

type JobSchedulingPtrOutput struct{ *pulumi.OutputState }

func (JobSchedulingPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**JobScheduling)(nil)).Elem()
}

func (o JobSchedulingPtrOutput) ToJobSchedulingPtrOutput() JobSchedulingPtrOutput {
	return o
}

func (o JobSchedulingPtrOutput) ToJobSchedulingPtrOutputWithContext(ctx context.Context) JobSchedulingPtrOutput {
	return o
}

func (o JobSchedulingPtrOutput) Elem() JobSchedulingOutput {
	return o.ApplyT(func(v *JobScheduling) JobScheduling {
		if v != nil {
			return *v
		}
		var ret JobScheduling
		return ret
	}).(JobSchedulingOutput)
}

// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window.Maximum value is 10.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
func (o JobSchedulingPtrOutput) MaxFailuresPerHour() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *JobScheduling) *int {
		if v == nil {
			return nil
		}
		return v.MaxFailuresPerHour
	}).(pulumi.IntPtrOutput)
}

// Optional. Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
func (o JobSchedulingPtrOutput) MaxFailuresTotal() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *JobScheduling) *int {
		if v == nil {
			return nil
		}
		return v.MaxFailuresTotal
	}).(pulumi.IntPtrOutput)
}

// Job scheduling options.
type JobSchedulingResponse struct {
	// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window.Maximum value is 10.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
	MaxFailuresPerHour int `pulumi:"maxFailuresPerHour"`
	// Optional. Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
	MaxFailuresTotal int `pulumi:"maxFailuresTotal"`
}

// Job scheduling options.
type JobSchedulingResponseOutput struct{ *pulumi.OutputState }

func (JobSchedulingResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobSchedulingResponse)(nil)).Elem()
}

func (o JobSchedulingResponseOutput) ToJobSchedulingResponseOutput() JobSchedulingResponseOutput {
	return o
}

func (o JobSchedulingResponseOutput) ToJobSchedulingResponseOutputWithContext(ctx context.Context) JobSchedulingResponseOutput {
	return o
}

// Optional. Maximum number of times per hour a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed.A job may be reported as thrashing if driver exits with non-zero code 4 times within 10 minute window.Maximum value is 10.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
func (o JobSchedulingResponseOutput) MaxFailuresPerHour() pulumi.IntOutput {
	return o.ApplyT(func(v JobSchedulingResponse) int { return v.MaxFailuresPerHour }).(pulumi.IntOutput)
}

// Optional. Maximum number of times in total a driver may be restarted as a result of driver exiting with non-zero code before job is reported failed. Maximum value is 240.Note: Currently, this restartable job option is not supported in Dataproc workflow template (https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#adding_jobs_to_a_template) jobs.
func (o JobSchedulingResponseOutput) MaxFailuresTotal() pulumi.IntOutput {
	return o.ApplyT(func(v JobSchedulingResponse) int { return v.MaxFailuresTotal }).(pulumi.IntOutput)
}

// Dataproc job status.
type JobStatusResponse struct {
	// Optional. Output only. Job state details, such as an error description if the state is ERROR.
	Details string `pulumi:"details"`
	// A state message specifying the overall job state.
	State string `pulumi:"state"`
	// The time when this state was entered.
	StateStartTime string `pulumi:"stateStartTime"`
	// Additional state information, which includes status reported by the agent.
	Substate string `pulumi:"substate"`
}

// Dataproc job status.
type JobStatusResponseOutput struct{ *pulumi.OutputState }

func (JobStatusResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*JobStatusResponse)(nil)).Elem()
}

func (o JobStatusResponseOutput) ToJobStatusResponseOutput() JobStatusResponseOutput {
	return o
}

func (o JobStatusResponseOutput) ToJobStatusResponseOutputWithContext(ctx context.Context) JobStatusResponseOutput {
	return o
}

// Optional. Output only. Job state details, such as an error description if the state is ERROR.
func (o JobStatusResponseOutput) Details() pulumi.StringOutput {
	return o.ApplyT(func(v JobStatusResponse) string { return v.Details }).(pulumi.StringOutput)
}

// A state message specifying the overall job state.
func (o JobStatusResponseOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v JobStatusResponse) string { return v.State }).(pulumi.StringOutput)
}

// The time when this state was entered.
func (o JobStatusResponseOutput) StateStartTime() pulumi.StringOutput {
	return o.ApplyT(func(v JobStatusResponse) string { return v.StateStartTime }).(pulumi.StringOutput)
}

// Additional state information, which includes status reported by the agent.
func (o JobStatusResponseOutput) Substate() pulumi.StringOutput {
	return o.ApplyT(func(v JobStatusResponse) string { return v.Substate }).(pulumi.StringOutput)
}

type JobStatusResponseArrayOutput struct{ *pulumi.OutputState }

func (JobStatusResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]JobStatusResponse)(nil)).Elem()
}

func (o JobStatusResponseArrayOutput) ToJobStatusResponseArrayOutput() JobStatusResponseArrayOutput {
	return o
}

func (o JobStatusResponseArrayOutput) ToJobStatusResponseArrayOutputWithContext(ctx context.Context) JobStatusResponseArrayOutput {
	return o
}

func (o JobStatusResponseArrayOutput) Index(i pulumi.IntInput) JobStatusResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) JobStatusResponse {
		return vs[0].([]JobStatusResponse)[vs[1].(int)]
	}).(JobStatusResponseOutput)
}

// Specifies Kerberos related configuration.
type KerberosConfig struct {
	// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustAdminServer *string `pulumi:"crossRealmTrustAdminServer"`
	// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustKdc *string `pulumi:"crossRealmTrustKdc"`
	// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
	CrossRealmTrustRealm *string `pulumi:"crossRealmTrustRealm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
	CrossRealmTrustSharedPasswordUri *string `pulumi:"crossRealmTrustSharedPasswordUri"`
	// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
	EnableKerberos *bool `pulumi:"enableKerberos"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
	KdcDbKeyUri *string `pulumi:"kdcDbKeyUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
	KeyPasswordUri *string `pulumi:"keyPasswordUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
	KeystorePasswordUri *string `pulumi:"keystorePasswordUri"`
	// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	KeystoreUri *string `pulumi:"keystoreUri"`
	// Optional. The uri of the KMS key used to encrypt various sensitive files.
	KmsKeyUri *string `pulumi:"kmsKeyUri"`
	// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
	Realm *string `pulumi:"realm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
	RootPrincipalPasswordUri *string `pulumi:"rootPrincipalPasswordUri"`
	// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
	TgtLifetimeHours *int `pulumi:"tgtLifetimeHours"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
	TruststorePasswordUri *string `pulumi:"truststorePasswordUri"`
	// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	TruststoreUri *string `pulumi:"truststoreUri"`
}

// KerberosConfigInput is an input type that accepts KerberosConfigArgs and KerberosConfigOutput values.
// You can construct a concrete instance of `KerberosConfigInput` via:
//
//          KerberosConfigArgs{...}
type KerberosConfigInput interface {
	pulumi.Input

	ToKerberosConfigOutput() KerberosConfigOutput
	ToKerberosConfigOutputWithContext(context.Context) KerberosConfigOutput
}

// Specifies Kerberos related configuration.
type KerberosConfigArgs struct {
	// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustAdminServer pulumi.StringPtrInput `pulumi:"crossRealmTrustAdminServer"`
	// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustKdc pulumi.StringPtrInput `pulumi:"crossRealmTrustKdc"`
	// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
	CrossRealmTrustRealm pulumi.StringPtrInput `pulumi:"crossRealmTrustRealm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
	CrossRealmTrustSharedPasswordUri pulumi.StringPtrInput `pulumi:"crossRealmTrustSharedPasswordUri"`
	// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
	EnableKerberos pulumi.BoolPtrInput `pulumi:"enableKerberos"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
	KdcDbKeyUri pulumi.StringPtrInput `pulumi:"kdcDbKeyUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
	KeyPasswordUri pulumi.StringPtrInput `pulumi:"keyPasswordUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
	KeystorePasswordUri pulumi.StringPtrInput `pulumi:"keystorePasswordUri"`
	// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	KeystoreUri pulumi.StringPtrInput `pulumi:"keystoreUri"`
	// Optional. The uri of the KMS key used to encrypt various sensitive files.
	KmsKeyUri pulumi.StringPtrInput `pulumi:"kmsKeyUri"`
	// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
	Realm pulumi.StringPtrInput `pulumi:"realm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
	RootPrincipalPasswordUri pulumi.StringPtrInput `pulumi:"rootPrincipalPasswordUri"`
	// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
	TgtLifetimeHours pulumi.IntPtrInput `pulumi:"tgtLifetimeHours"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
	TruststorePasswordUri pulumi.StringPtrInput `pulumi:"truststorePasswordUri"`
	// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	TruststoreUri pulumi.StringPtrInput `pulumi:"truststoreUri"`
}

func (KerberosConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*KerberosConfig)(nil)).Elem()
}

func (i KerberosConfigArgs) ToKerberosConfigOutput() KerberosConfigOutput {
	return i.ToKerberosConfigOutputWithContext(context.Background())
}

func (i KerberosConfigArgs) ToKerberosConfigOutputWithContext(ctx context.Context) KerberosConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KerberosConfigOutput)
}

func (i KerberosConfigArgs) ToKerberosConfigPtrOutput() KerberosConfigPtrOutput {
	return i.ToKerberosConfigPtrOutputWithContext(context.Background())
}

func (i KerberosConfigArgs) ToKerberosConfigPtrOutputWithContext(ctx context.Context) KerberosConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KerberosConfigOutput).ToKerberosConfigPtrOutputWithContext(ctx)
}

// KerberosConfigPtrInput is an input type that accepts KerberosConfigArgs, KerberosConfigPtr and KerberosConfigPtrOutput values.
// You can construct a concrete instance of `KerberosConfigPtrInput` via:
//
//          KerberosConfigArgs{...}
//
//  or:
//
//          nil
type KerberosConfigPtrInput interface {
	pulumi.Input

	ToKerberosConfigPtrOutput() KerberosConfigPtrOutput
	ToKerberosConfigPtrOutputWithContext(context.Context) KerberosConfigPtrOutput
}

type kerberosConfigPtrType KerberosConfigArgs

func KerberosConfigPtr(v *KerberosConfigArgs) KerberosConfigPtrInput {
	return (*kerberosConfigPtrType)(v)
}

func (*kerberosConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**KerberosConfig)(nil)).Elem()
}

func (i *kerberosConfigPtrType) ToKerberosConfigPtrOutput() KerberosConfigPtrOutput {
	return i.ToKerberosConfigPtrOutputWithContext(context.Background())
}

func (i *kerberosConfigPtrType) ToKerberosConfigPtrOutputWithContext(ctx context.Context) KerberosConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(KerberosConfigPtrOutput)
}

// Specifies Kerberos related configuration.
type KerberosConfigOutput struct{ *pulumi.OutputState }

func (KerberosConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*KerberosConfig)(nil)).Elem()
}

func (o KerberosConfigOutput) ToKerberosConfigOutput() KerberosConfigOutput {
	return o
}

func (o KerberosConfigOutput) ToKerberosConfigOutputWithContext(ctx context.Context) KerberosConfigOutput {
	return o
}

func (o KerberosConfigOutput) ToKerberosConfigPtrOutput() KerberosConfigPtrOutput {
	return o.ToKerberosConfigPtrOutputWithContext(context.Background())
}

func (o KerberosConfigOutput) ToKerberosConfigPtrOutputWithContext(ctx context.Context) KerberosConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v KerberosConfig) *KerberosConfig {
		return &v
	}).(KerberosConfigPtrOutput)
}

// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigOutput) CrossRealmTrustAdminServer() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.CrossRealmTrustAdminServer }).(pulumi.StringPtrOutput)
}

// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigOutput) CrossRealmTrustKdc() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.CrossRealmTrustKdc }).(pulumi.StringPtrOutput)
}

// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
func (o KerberosConfigOutput) CrossRealmTrustRealm() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.CrossRealmTrustRealm }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
func (o KerberosConfigOutput) CrossRealmTrustSharedPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.CrossRealmTrustSharedPasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
func (o KerberosConfigOutput) EnableKerberos() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *bool { return v.EnableKerberos }).(pulumi.BoolPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
func (o KerberosConfigOutput) KdcDbKeyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KdcDbKeyUri }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigOutput) KeyPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KeyPasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigOutput) KeystorePasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KeystorePasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigOutput) KeystoreUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KeystoreUri }).(pulumi.StringPtrOutput)
}

// Optional. The uri of the KMS key used to encrypt various sensitive files.
func (o KerberosConfigOutput) KmsKeyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.KmsKeyUri }).(pulumi.StringPtrOutput)
}

// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
func (o KerberosConfigOutput) Realm() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.Realm }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
func (o KerberosConfigOutput) RootPrincipalPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.RootPrincipalPasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
func (o KerberosConfigOutput) TgtLifetimeHours() pulumi.IntPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *int { return v.TgtLifetimeHours }).(pulumi.IntPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigOutput) TruststorePasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.TruststorePasswordUri }).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigOutput) TruststoreUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v KerberosConfig) *string { return v.TruststoreUri }).(pulumi.StringPtrOutput)
}

type KerberosConfigPtrOutput struct{ *pulumi.OutputState }

func (KerberosConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**KerberosConfig)(nil)).Elem()
}

func (o KerberosConfigPtrOutput) ToKerberosConfigPtrOutput() KerberosConfigPtrOutput {
	return o
}

func (o KerberosConfigPtrOutput) ToKerberosConfigPtrOutputWithContext(ctx context.Context) KerberosConfigPtrOutput {
	return o
}

func (o KerberosConfigPtrOutput) Elem() KerberosConfigOutput {
	return o.ApplyT(func(v *KerberosConfig) KerberosConfig {
		if v != nil {
			return *v
		}
		var ret KerberosConfig
		return ret
	}).(KerberosConfigOutput)
}

// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigPtrOutput) CrossRealmTrustAdminServer() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.CrossRealmTrustAdminServer
	}).(pulumi.StringPtrOutput)
}

// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigPtrOutput) CrossRealmTrustKdc() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.CrossRealmTrustKdc
	}).(pulumi.StringPtrOutput)
}

// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
func (o KerberosConfigPtrOutput) CrossRealmTrustRealm() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.CrossRealmTrustRealm
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
func (o KerberosConfigPtrOutput) CrossRealmTrustSharedPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.CrossRealmTrustSharedPasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
func (o KerberosConfigPtrOutput) EnableKerberos() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableKerberos
	}).(pulumi.BoolPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
func (o KerberosConfigPtrOutput) KdcDbKeyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KdcDbKeyUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigPtrOutput) KeyPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KeyPasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigPtrOutput) KeystorePasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KeystorePasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigPtrOutput) KeystoreUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KeystoreUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The uri of the KMS key used to encrypt various sensitive files.
func (o KerberosConfigPtrOutput) KmsKeyUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.KmsKeyUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
func (o KerberosConfigPtrOutput) Realm() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.Realm
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
func (o KerberosConfigPtrOutput) RootPrincipalPasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.RootPrincipalPasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
func (o KerberosConfigPtrOutput) TgtLifetimeHours() pulumi.IntPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *int {
		if v == nil {
			return nil
		}
		return v.TgtLifetimeHours
	}).(pulumi.IntPtrOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigPtrOutput) TruststorePasswordUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.TruststorePasswordUri
	}).(pulumi.StringPtrOutput)
}

// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigPtrOutput) TruststoreUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *KerberosConfig) *string {
		if v == nil {
			return nil
		}
		return v.TruststoreUri
	}).(pulumi.StringPtrOutput)
}

// Specifies Kerberos related configuration.
type KerberosConfigResponse struct {
	// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustAdminServer string `pulumi:"crossRealmTrustAdminServer"`
	// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
	CrossRealmTrustKdc string `pulumi:"crossRealmTrustKdc"`
	// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
	CrossRealmTrustRealm string `pulumi:"crossRealmTrustRealm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
	CrossRealmTrustSharedPasswordUri string `pulumi:"crossRealmTrustSharedPasswordUri"`
	// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
	EnableKerberos bool `pulumi:"enableKerberos"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
	KdcDbKeyUri string `pulumi:"kdcDbKeyUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
	KeyPasswordUri string `pulumi:"keyPasswordUri"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
	KeystorePasswordUri string `pulumi:"keystorePasswordUri"`
	// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	KeystoreUri string `pulumi:"keystoreUri"`
	// Optional. The uri of the KMS key used to encrypt various sensitive files.
	KmsKeyUri string `pulumi:"kmsKeyUri"`
	// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
	Realm string `pulumi:"realm"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
	RootPrincipalPasswordUri string `pulumi:"rootPrincipalPasswordUri"`
	// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
	TgtLifetimeHours int `pulumi:"tgtLifetimeHours"`
	// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
	TruststorePasswordUri string `pulumi:"truststorePasswordUri"`
	// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
	TruststoreUri string `pulumi:"truststoreUri"`
}

// Specifies Kerberos related configuration.
type KerberosConfigResponseOutput struct{ *pulumi.OutputState }

func (KerberosConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*KerberosConfigResponse)(nil)).Elem()
}

func (o KerberosConfigResponseOutput) ToKerberosConfigResponseOutput() KerberosConfigResponseOutput {
	return o
}

func (o KerberosConfigResponseOutput) ToKerberosConfigResponseOutputWithContext(ctx context.Context) KerberosConfigResponseOutput {
	return o
}

// Optional. The admin server (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigResponseOutput) CrossRealmTrustAdminServer() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.CrossRealmTrustAdminServer }).(pulumi.StringOutput)
}

// Optional. The KDC (IP or hostname) for the remote trusted realm in a cross realm trust relationship.
func (o KerberosConfigResponseOutput) CrossRealmTrustKdc() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.CrossRealmTrustKdc }).(pulumi.StringOutput)
}

// Optional. The remote realm the Dataproc on-cluster KDC will trust, should the user enable cross realm trust.
func (o KerberosConfigResponseOutput) CrossRealmTrustRealm() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.CrossRealmTrustRealm }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the shared password between the on-cluster Kerberos realm and the remote trusted realm, in a cross realm trust relationship.
func (o KerberosConfigResponseOutput) CrossRealmTrustSharedPasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.CrossRealmTrustSharedPasswordUri }).(pulumi.StringOutput)
}

// Optional. Flag to indicate whether to Kerberize the cluster (default: false). Set this field to true to enable Kerberos on a cluster.
func (o KerberosConfigResponseOutput) EnableKerberos() pulumi.BoolOutput {
	return o.ApplyT(func(v KerberosConfigResponse) bool { return v.EnableKerberos }).(pulumi.BoolOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the master key of the KDC database.
func (o KerberosConfigResponseOutput) KdcDbKeyUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KdcDbKeyUri }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided key. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigResponseOutput) KeyPasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KeyPasswordUri }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided keystore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigResponseOutput) KeystorePasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KeystorePasswordUri }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of the keystore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigResponseOutput) KeystoreUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KeystoreUri }).(pulumi.StringOutput)
}

// Optional. The uri of the KMS key used to encrypt various sensitive files.
func (o KerberosConfigResponseOutput) KmsKeyUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.KmsKeyUri }).(pulumi.StringOutput)
}

// Optional. The name of the on-cluster Kerberos realm. If not specified, the uppercased domain of hostnames will be the realm.
func (o KerberosConfigResponseOutput) Realm() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.Realm }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the root principal password.
func (o KerberosConfigResponseOutput) RootPrincipalPasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.RootPrincipalPasswordUri }).(pulumi.StringOutput)
}

// Optional. The lifetime of the ticket granting ticket, in hours. If not specified, or user specifies 0, then default value 10 will be used.
func (o KerberosConfigResponseOutput) TgtLifetimeHours() pulumi.IntOutput {
	return o.ApplyT(func(v KerberosConfigResponse) int { return v.TgtLifetimeHours }).(pulumi.IntOutput)
}

// Optional. The Cloud Storage URI of a KMS encrypted file containing the password to the user provided truststore. For the self-signed certificate, this password is generated by Dataproc.
func (o KerberosConfigResponseOutput) TruststorePasswordUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.TruststorePasswordUri }).(pulumi.StringOutput)
}

// Optional. The Cloud Storage URI of the truststore file used for SSL encryption. If not provided, Dataproc will provide a self-signed certificate.
func (o KerberosConfigResponseOutput) TruststoreUri() pulumi.StringOutput {
	return o.ApplyT(func(v KerberosConfigResponse) string { return v.TruststoreUri }).(pulumi.StringOutput)
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfig struct {
	// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTime *string `pulumi:"autoDeleteTime"`
	// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTtl *string `pulumi:"autoDeleteTtl"`
	// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	IdleDeleteTtl *string `pulumi:"idleDeleteTtl"`
}

// LifecycleConfigInput is an input type that accepts LifecycleConfigArgs and LifecycleConfigOutput values.
// You can construct a concrete instance of `LifecycleConfigInput` via:
//
//          LifecycleConfigArgs{...}
type LifecycleConfigInput interface {
	pulumi.Input

	ToLifecycleConfigOutput() LifecycleConfigOutput
	ToLifecycleConfigOutputWithContext(context.Context) LifecycleConfigOutput
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfigArgs struct {
	// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTime pulumi.StringPtrInput `pulumi:"autoDeleteTime"`
	// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTtl pulumi.StringPtrInput `pulumi:"autoDeleteTtl"`
	// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	IdleDeleteTtl pulumi.StringPtrInput `pulumi:"idleDeleteTtl"`
}

func (LifecycleConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*LifecycleConfig)(nil)).Elem()
}

func (i LifecycleConfigArgs) ToLifecycleConfigOutput() LifecycleConfigOutput {
	return i.ToLifecycleConfigOutputWithContext(context.Background())
}

func (i LifecycleConfigArgs) ToLifecycleConfigOutputWithContext(ctx context.Context) LifecycleConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LifecycleConfigOutput)
}

func (i LifecycleConfigArgs) ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput {
	return i.ToLifecycleConfigPtrOutputWithContext(context.Background())
}

func (i LifecycleConfigArgs) ToLifecycleConfigPtrOutputWithContext(ctx context.Context) LifecycleConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LifecycleConfigOutput).ToLifecycleConfigPtrOutputWithContext(ctx)
}

// LifecycleConfigPtrInput is an input type that accepts LifecycleConfigArgs, LifecycleConfigPtr and LifecycleConfigPtrOutput values.
// You can construct a concrete instance of `LifecycleConfigPtrInput` via:
//
//          LifecycleConfigArgs{...}
//
//  or:
//
//          nil
type LifecycleConfigPtrInput interface {
	pulumi.Input

	ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput
	ToLifecycleConfigPtrOutputWithContext(context.Context) LifecycleConfigPtrOutput
}

type lifecycleConfigPtrType LifecycleConfigArgs

func LifecycleConfigPtr(v *LifecycleConfigArgs) LifecycleConfigPtrInput {
	return (*lifecycleConfigPtrType)(v)
}

func (*lifecycleConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**LifecycleConfig)(nil)).Elem()
}

func (i *lifecycleConfigPtrType) ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput {
	return i.ToLifecycleConfigPtrOutputWithContext(context.Background())
}

func (i *lifecycleConfigPtrType) ToLifecycleConfigPtrOutputWithContext(ctx context.Context) LifecycleConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LifecycleConfigPtrOutput)
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfigOutput struct{ *pulumi.OutputState }

func (LifecycleConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*LifecycleConfig)(nil)).Elem()
}

func (o LifecycleConfigOutput) ToLifecycleConfigOutput() LifecycleConfigOutput {
	return o
}

func (o LifecycleConfigOutput) ToLifecycleConfigOutputWithContext(ctx context.Context) LifecycleConfigOutput {
	return o
}

func (o LifecycleConfigOutput) ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput {
	return o.ToLifecycleConfigPtrOutputWithContext(context.Background())
}

func (o LifecycleConfigOutput) ToLifecycleConfigPtrOutputWithContext(ctx context.Context) LifecycleConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v LifecycleConfig) *LifecycleConfig {
		return &v
	}).(LifecycleConfigPtrOutput)
}

// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigOutput) AutoDeleteTime() pulumi.StringPtrOutput {
	return o.ApplyT(func(v LifecycleConfig) *string { return v.AutoDeleteTime }).(pulumi.StringPtrOutput)
}

// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigOutput) AutoDeleteTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v LifecycleConfig) *string { return v.AutoDeleteTtl }).(pulumi.StringPtrOutput)
}

// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigOutput) IdleDeleteTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v LifecycleConfig) *string { return v.IdleDeleteTtl }).(pulumi.StringPtrOutput)
}

type LifecycleConfigPtrOutput struct{ *pulumi.OutputState }

func (LifecycleConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**LifecycleConfig)(nil)).Elem()
}

func (o LifecycleConfigPtrOutput) ToLifecycleConfigPtrOutput() LifecycleConfigPtrOutput {
	return o
}

func (o LifecycleConfigPtrOutput) ToLifecycleConfigPtrOutputWithContext(ctx context.Context) LifecycleConfigPtrOutput {
	return o
}

func (o LifecycleConfigPtrOutput) Elem() LifecycleConfigOutput {
	return o.ApplyT(func(v *LifecycleConfig) LifecycleConfig {
		if v != nil {
			return *v
		}
		var ret LifecycleConfig
		return ret
	}).(LifecycleConfigOutput)
}

// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigPtrOutput) AutoDeleteTime() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LifecycleConfig) *string {
		if v == nil {
			return nil
		}
		return v.AutoDeleteTime
	}).(pulumi.StringPtrOutput)
}

// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigPtrOutput) AutoDeleteTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LifecycleConfig) *string {
		if v == nil {
			return nil
		}
		return v.AutoDeleteTtl
	}).(pulumi.StringPtrOutput)
}

// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigPtrOutput) IdleDeleteTtl() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *LifecycleConfig) *string {
		if v == nil {
			return nil
		}
		return v.IdleDeleteTtl
	}).(pulumi.StringPtrOutput)
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfigResponse struct {
	// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTime string `pulumi:"autoDeleteTime"`
	// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	AutoDeleteTtl string `pulumi:"autoDeleteTtl"`
	// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	IdleDeleteTtl string `pulumi:"idleDeleteTtl"`
	// The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
	IdleStartTime string `pulumi:"idleStartTime"`
}

// Specifies the cluster auto-delete schedule configuration.
type LifecycleConfigResponseOutput struct{ *pulumi.OutputState }

func (LifecycleConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*LifecycleConfigResponse)(nil)).Elem()
}

func (o LifecycleConfigResponseOutput) ToLifecycleConfigResponseOutput() LifecycleConfigResponseOutput {
	return o
}

func (o LifecycleConfigResponseOutput) ToLifecycleConfigResponseOutputWithContext(ctx context.Context) LifecycleConfigResponseOutput {
	return o
}

// Optional. The time when cluster will be auto-deleted (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigResponseOutput) AutoDeleteTime() pulumi.StringOutput {
	return o.ApplyT(func(v LifecycleConfigResponse) string { return v.AutoDeleteTime }).(pulumi.StringOutput)
}

// Optional. The lifetime duration of cluster. The cluster will be auto-deleted at the end of this period. Minimum value is 10 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigResponseOutput) AutoDeleteTtl() pulumi.StringOutput {
	return o.ApplyT(func(v LifecycleConfigResponse) string { return v.AutoDeleteTtl }).(pulumi.StringOutput)
}

// Optional. The duration to keep the cluster alive while idling (when no jobs are running). Passing this threshold will cause the cluster to be deleted. Minimum value is 5 minutes; maximum value is 14 days (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigResponseOutput) IdleDeleteTtl() pulumi.StringOutput {
	return o.ApplyT(func(v LifecycleConfigResponse) string { return v.IdleDeleteTtl }).(pulumi.StringOutput)
}

// The time when cluster became idle (most recent job finished) and became eligible for deletion due to idleness (see JSON representation of Timestamp (https://developers.google.com/protocol-buffers/docs/proto3#json)).
func (o LifecycleConfigResponseOutput) IdleStartTime() pulumi.StringOutput {
	return o.ApplyT(func(v LifecycleConfigResponse) string { return v.IdleStartTime }).(pulumi.StringOutput)
}

// The runtime logging config of the job.
type LoggingConfig struct {
	// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	DriverLogLevels map[string]string `pulumi:"driverLogLevels"`
}

// LoggingConfigInput is an input type that accepts LoggingConfigArgs and LoggingConfigOutput values.
// You can construct a concrete instance of `LoggingConfigInput` via:
//
//          LoggingConfigArgs{...}
type LoggingConfigInput interface {
	pulumi.Input

	ToLoggingConfigOutput() LoggingConfigOutput
	ToLoggingConfigOutputWithContext(context.Context) LoggingConfigOutput
}

// The runtime logging config of the job.
type LoggingConfigArgs struct {
	// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	DriverLogLevels pulumi.StringMapInput `pulumi:"driverLogLevels"`
}

func (LoggingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*LoggingConfig)(nil)).Elem()
}

func (i LoggingConfigArgs) ToLoggingConfigOutput() LoggingConfigOutput {
	return i.ToLoggingConfigOutputWithContext(context.Background())
}

func (i LoggingConfigArgs) ToLoggingConfigOutputWithContext(ctx context.Context) LoggingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LoggingConfigOutput)
}

func (i LoggingConfigArgs) ToLoggingConfigPtrOutput() LoggingConfigPtrOutput {
	return i.ToLoggingConfigPtrOutputWithContext(context.Background())
}

func (i LoggingConfigArgs) ToLoggingConfigPtrOutputWithContext(ctx context.Context) LoggingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LoggingConfigOutput).ToLoggingConfigPtrOutputWithContext(ctx)
}

// LoggingConfigPtrInput is an input type that accepts LoggingConfigArgs, LoggingConfigPtr and LoggingConfigPtrOutput values.
// You can construct a concrete instance of `LoggingConfigPtrInput` via:
//
//          LoggingConfigArgs{...}
//
//  or:
//
//          nil
type LoggingConfigPtrInput interface {
	pulumi.Input

	ToLoggingConfigPtrOutput() LoggingConfigPtrOutput
	ToLoggingConfigPtrOutputWithContext(context.Context) LoggingConfigPtrOutput
}

type loggingConfigPtrType LoggingConfigArgs

func LoggingConfigPtr(v *LoggingConfigArgs) LoggingConfigPtrInput {
	return (*loggingConfigPtrType)(v)
}

func (*loggingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**LoggingConfig)(nil)).Elem()
}

func (i *loggingConfigPtrType) ToLoggingConfigPtrOutput() LoggingConfigPtrOutput {
	return i.ToLoggingConfigPtrOutputWithContext(context.Background())
}

func (i *loggingConfigPtrType) ToLoggingConfigPtrOutputWithContext(ctx context.Context) LoggingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(LoggingConfigPtrOutput)
}

// The runtime logging config of the job.
type LoggingConfigOutput struct{ *pulumi.OutputState }

func (LoggingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*LoggingConfig)(nil)).Elem()
}

func (o LoggingConfigOutput) ToLoggingConfigOutput() LoggingConfigOutput {
	return o
}

func (o LoggingConfigOutput) ToLoggingConfigOutputWithContext(ctx context.Context) LoggingConfigOutput {
	return o
}

func (o LoggingConfigOutput) ToLoggingConfigPtrOutput() LoggingConfigPtrOutput {
	return o.ToLoggingConfigPtrOutputWithContext(context.Background())
}

func (o LoggingConfigOutput) ToLoggingConfigPtrOutputWithContext(ctx context.Context) LoggingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v LoggingConfig) *LoggingConfig {
		return &v
	}).(LoggingConfigPtrOutput)
}

// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
func (o LoggingConfigOutput) DriverLogLevels() pulumi.StringMapOutput {
	return o.ApplyT(func(v LoggingConfig) map[string]string { return v.DriverLogLevels }).(pulumi.StringMapOutput)
}

type LoggingConfigPtrOutput struct{ *pulumi.OutputState }

func (LoggingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**LoggingConfig)(nil)).Elem()
}

func (o LoggingConfigPtrOutput) ToLoggingConfigPtrOutput() LoggingConfigPtrOutput {
	return o
}

func (o LoggingConfigPtrOutput) ToLoggingConfigPtrOutputWithContext(ctx context.Context) LoggingConfigPtrOutput {
	return o
}

func (o LoggingConfigPtrOutput) Elem() LoggingConfigOutput {
	return o.ApplyT(func(v *LoggingConfig) LoggingConfig {
		if v != nil {
			return *v
		}
		var ret LoggingConfig
		return ret
	}).(LoggingConfigOutput)
}

// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
func (o LoggingConfigPtrOutput) DriverLogLevels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *LoggingConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.DriverLogLevels
	}).(pulumi.StringMapOutput)
}

// The runtime logging config of the job.
type LoggingConfigResponse struct {
	// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
	DriverLogLevels map[string]string `pulumi:"driverLogLevels"`
}

// The runtime logging config of the job.
type LoggingConfigResponseOutput struct{ *pulumi.OutputState }

func (LoggingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*LoggingConfigResponse)(nil)).Elem()
}

func (o LoggingConfigResponseOutput) ToLoggingConfigResponseOutput() LoggingConfigResponseOutput {
	return o
}

func (o LoggingConfigResponseOutput) ToLoggingConfigResponseOutputWithContext(ctx context.Context) LoggingConfigResponseOutput {
	return o
}

// The per-package log levels for the driver. This may include "root" package name to configure rootLogger. Examples: 'com.google = FATAL', 'root = INFO', 'org.apache = DEBUG'
func (o LoggingConfigResponseOutput) DriverLogLevels() pulumi.StringMapOutput {
	return o.ApplyT(func(v LoggingConfigResponse) map[string]string { return v.DriverLogLevels }).(pulumi.StringMapOutput)
}

// Cluster that is managed by the workflow.
type ManagedCluster struct {
	// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
	ClusterName string `pulumi:"clusterName"`
	// The cluster configuration.
	Config ClusterConfig `pulumi:"config"`
	// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
	Labels map[string]string `pulumi:"labels"`
}

// ManagedClusterInput is an input type that accepts ManagedClusterArgs and ManagedClusterOutput values.
// You can construct a concrete instance of `ManagedClusterInput` via:
//
//          ManagedClusterArgs{...}
type ManagedClusterInput interface {
	pulumi.Input

	ToManagedClusterOutput() ManagedClusterOutput
	ToManagedClusterOutputWithContext(context.Context) ManagedClusterOutput
}

// Cluster that is managed by the workflow.
type ManagedClusterArgs struct {
	// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
	ClusterName pulumi.StringInput `pulumi:"clusterName"`
	// The cluster configuration.
	Config ClusterConfigInput `pulumi:"config"`
	// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
	Labels pulumi.StringMapInput `pulumi:"labels"`
}

func (ManagedClusterArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ManagedCluster)(nil)).Elem()
}

func (i ManagedClusterArgs) ToManagedClusterOutput() ManagedClusterOutput {
	return i.ToManagedClusterOutputWithContext(context.Background())
}

func (i ManagedClusterArgs) ToManagedClusterOutputWithContext(ctx context.Context) ManagedClusterOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ManagedClusterOutput)
}

func (i ManagedClusterArgs) ToManagedClusterPtrOutput() ManagedClusterPtrOutput {
	return i.ToManagedClusterPtrOutputWithContext(context.Background())
}

func (i ManagedClusterArgs) ToManagedClusterPtrOutputWithContext(ctx context.Context) ManagedClusterPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ManagedClusterOutput).ToManagedClusterPtrOutputWithContext(ctx)
}

// ManagedClusterPtrInput is an input type that accepts ManagedClusterArgs, ManagedClusterPtr and ManagedClusterPtrOutput values.
// You can construct a concrete instance of `ManagedClusterPtrInput` via:
//
//          ManagedClusterArgs{...}
//
//  or:
//
//          nil
type ManagedClusterPtrInput interface {
	pulumi.Input

	ToManagedClusterPtrOutput() ManagedClusterPtrOutput
	ToManagedClusterPtrOutputWithContext(context.Context) ManagedClusterPtrOutput
}

type managedClusterPtrType ManagedClusterArgs

func ManagedClusterPtr(v *ManagedClusterArgs) ManagedClusterPtrInput {
	return (*managedClusterPtrType)(v)
}

func (*managedClusterPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ManagedCluster)(nil)).Elem()
}

func (i *managedClusterPtrType) ToManagedClusterPtrOutput() ManagedClusterPtrOutput {
	return i.ToManagedClusterPtrOutputWithContext(context.Background())
}

func (i *managedClusterPtrType) ToManagedClusterPtrOutputWithContext(ctx context.Context) ManagedClusterPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ManagedClusterPtrOutput)
}

// Cluster that is managed by the workflow.
type ManagedClusterOutput struct{ *pulumi.OutputState }

func (ManagedClusterOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ManagedCluster)(nil)).Elem()
}

func (o ManagedClusterOutput) ToManagedClusterOutput() ManagedClusterOutput {
	return o
}

func (o ManagedClusterOutput) ToManagedClusterOutputWithContext(ctx context.Context) ManagedClusterOutput {
	return o
}

func (o ManagedClusterOutput) ToManagedClusterPtrOutput() ManagedClusterPtrOutput {
	return o.ToManagedClusterPtrOutputWithContext(context.Background())
}

func (o ManagedClusterOutput) ToManagedClusterPtrOutputWithContext(ctx context.Context) ManagedClusterPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ManagedCluster) *ManagedCluster {
		return &v
	}).(ManagedClusterPtrOutput)
}

// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
func (o ManagedClusterOutput) ClusterName() pulumi.StringOutput {
	return o.ApplyT(func(v ManagedCluster) string { return v.ClusterName }).(pulumi.StringOutput)
}

// The cluster configuration.
func (o ManagedClusterOutput) Config() ClusterConfigOutput {
	return o.ApplyT(func(v ManagedCluster) ClusterConfig { return v.Config }).(ClusterConfigOutput)
}

// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
func (o ManagedClusterOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v ManagedCluster) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

type ManagedClusterPtrOutput struct{ *pulumi.OutputState }

func (ManagedClusterPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ManagedCluster)(nil)).Elem()
}

func (o ManagedClusterPtrOutput) ToManagedClusterPtrOutput() ManagedClusterPtrOutput {
	return o
}

func (o ManagedClusterPtrOutput) ToManagedClusterPtrOutputWithContext(ctx context.Context) ManagedClusterPtrOutput {
	return o
}

func (o ManagedClusterPtrOutput) Elem() ManagedClusterOutput {
	return o.ApplyT(func(v *ManagedCluster) ManagedCluster {
		if v != nil {
			return *v
		}
		var ret ManagedCluster
		return ret
	}).(ManagedClusterOutput)
}

// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
func (o ManagedClusterPtrOutput) ClusterName() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ManagedCluster) *string {
		if v == nil {
			return nil
		}
		return &v.ClusterName
	}).(pulumi.StringPtrOutput)
}

// The cluster configuration.
func (o ManagedClusterPtrOutput) Config() ClusterConfigPtrOutput {
	return o.ApplyT(func(v *ManagedCluster) *ClusterConfig {
		if v == nil {
			return nil
		}
		return &v.Config
	}).(ClusterConfigPtrOutput)
}

// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
func (o ManagedClusterPtrOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *ManagedCluster) map[string]string {
		if v == nil {
			return nil
		}
		return v.Labels
	}).(pulumi.StringMapOutput)
}

// Cluster that is managed by the workflow.
type ManagedClusterResponse struct {
	// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
	ClusterName string `pulumi:"clusterName"`
	// The cluster configuration.
	Config ClusterConfigResponse `pulumi:"config"`
	// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
	Labels map[string]string `pulumi:"labels"`
}

// Cluster that is managed by the workflow.
type ManagedClusterResponseOutput struct{ *pulumi.OutputState }

func (ManagedClusterResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ManagedClusterResponse)(nil)).Elem()
}

func (o ManagedClusterResponseOutput) ToManagedClusterResponseOutput() ManagedClusterResponseOutput {
	return o
}

func (o ManagedClusterResponseOutput) ToManagedClusterResponseOutputWithContext(ctx context.Context) ManagedClusterResponseOutput {
	return o
}

// The cluster name prefix. A unique cluster name will be formed by appending a random suffix.The name must contain only lower-case letters (a-z), numbers (0-9), and hyphens (-). Must begin with a letter. Cannot begin or end with hyphen. Must consist of between 2 and 35 characters.
func (o ManagedClusterResponseOutput) ClusterName() pulumi.StringOutput {
	return o.ApplyT(func(v ManagedClusterResponse) string { return v.ClusterName }).(pulumi.StringOutput)
}

// The cluster configuration.
func (o ManagedClusterResponseOutput) Config() ClusterConfigResponseOutput {
	return o.ApplyT(func(v ManagedClusterResponse) ClusterConfigResponse { return v.Config }).(ClusterConfigResponseOutput)
}

// Optional. The labels to associate with this cluster.Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given cluster.
func (o ManagedClusterResponseOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v ManagedClusterResponse) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

// Specifies the resources used to actively manage an instance group.
type ManagedGroupConfigResponse struct {
	// The name of the Instance Group Manager for this group.
	InstanceGroupManagerName string `pulumi:"instanceGroupManagerName"`
	// The name of the Instance Template used for the Managed Instance Group.
	InstanceTemplateName string `pulumi:"instanceTemplateName"`
}

// Specifies the resources used to actively manage an instance group.
type ManagedGroupConfigResponseOutput struct{ *pulumi.OutputState }

func (ManagedGroupConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ManagedGroupConfigResponse)(nil)).Elem()
}

func (o ManagedGroupConfigResponseOutput) ToManagedGroupConfigResponseOutput() ManagedGroupConfigResponseOutput {
	return o
}

func (o ManagedGroupConfigResponseOutput) ToManagedGroupConfigResponseOutputWithContext(ctx context.Context) ManagedGroupConfigResponseOutput {
	return o
}

// The name of the Instance Group Manager for this group.
func (o ManagedGroupConfigResponseOutput) InstanceGroupManagerName() pulumi.StringOutput {
	return o.ApplyT(func(v ManagedGroupConfigResponse) string { return v.InstanceGroupManagerName }).(pulumi.StringOutput)
}

// The name of the Instance Template used for the Managed Instance Group.
func (o ManagedGroupConfigResponseOutput) InstanceTemplateName() pulumi.StringOutput {
	return o.ApplyT(func(v ManagedGroupConfigResponse) string { return v.InstanceTemplateName }).(pulumi.StringOutput)
}

// Specifies a Metastore configuration.
type MetastoreConfig struct {
	// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
	DataprocMetastoreService string `pulumi:"dataprocMetastoreService"`
}

// MetastoreConfigInput is an input type that accepts MetastoreConfigArgs and MetastoreConfigOutput values.
// You can construct a concrete instance of `MetastoreConfigInput` via:
//
//          MetastoreConfigArgs{...}
type MetastoreConfigInput interface {
	pulumi.Input

	ToMetastoreConfigOutput() MetastoreConfigOutput
	ToMetastoreConfigOutputWithContext(context.Context) MetastoreConfigOutput
}

// Specifies a Metastore configuration.
type MetastoreConfigArgs struct {
	// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
	DataprocMetastoreService pulumi.StringInput `pulumi:"dataprocMetastoreService"`
}

func (MetastoreConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*MetastoreConfig)(nil)).Elem()
}

func (i MetastoreConfigArgs) ToMetastoreConfigOutput() MetastoreConfigOutput {
	return i.ToMetastoreConfigOutputWithContext(context.Background())
}

func (i MetastoreConfigArgs) ToMetastoreConfigOutputWithContext(ctx context.Context) MetastoreConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(MetastoreConfigOutput)
}

func (i MetastoreConfigArgs) ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput {
	return i.ToMetastoreConfigPtrOutputWithContext(context.Background())
}

func (i MetastoreConfigArgs) ToMetastoreConfigPtrOutputWithContext(ctx context.Context) MetastoreConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(MetastoreConfigOutput).ToMetastoreConfigPtrOutputWithContext(ctx)
}

// MetastoreConfigPtrInput is an input type that accepts MetastoreConfigArgs, MetastoreConfigPtr and MetastoreConfigPtrOutput values.
// You can construct a concrete instance of `MetastoreConfigPtrInput` via:
//
//          MetastoreConfigArgs{...}
//
//  or:
//
//          nil
type MetastoreConfigPtrInput interface {
	pulumi.Input

	ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput
	ToMetastoreConfigPtrOutputWithContext(context.Context) MetastoreConfigPtrOutput
}

type metastoreConfigPtrType MetastoreConfigArgs

func MetastoreConfigPtr(v *MetastoreConfigArgs) MetastoreConfigPtrInput {
	return (*metastoreConfigPtrType)(v)
}

func (*metastoreConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**MetastoreConfig)(nil)).Elem()
}

func (i *metastoreConfigPtrType) ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput {
	return i.ToMetastoreConfigPtrOutputWithContext(context.Background())
}

func (i *metastoreConfigPtrType) ToMetastoreConfigPtrOutputWithContext(ctx context.Context) MetastoreConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(MetastoreConfigPtrOutput)
}

// Specifies a Metastore configuration.
type MetastoreConfigOutput struct{ *pulumi.OutputState }

func (MetastoreConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*MetastoreConfig)(nil)).Elem()
}

func (o MetastoreConfigOutput) ToMetastoreConfigOutput() MetastoreConfigOutput {
	return o
}

func (o MetastoreConfigOutput) ToMetastoreConfigOutputWithContext(ctx context.Context) MetastoreConfigOutput {
	return o
}

func (o MetastoreConfigOutput) ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput {
	return o.ToMetastoreConfigPtrOutputWithContext(context.Background())
}

func (o MetastoreConfigOutput) ToMetastoreConfigPtrOutputWithContext(ctx context.Context) MetastoreConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v MetastoreConfig) *MetastoreConfig {
		return &v
	}).(MetastoreConfigPtrOutput)
}

// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
func (o MetastoreConfigOutput) DataprocMetastoreService() pulumi.StringOutput {
	return o.ApplyT(func(v MetastoreConfig) string { return v.DataprocMetastoreService }).(pulumi.StringOutput)
}

type MetastoreConfigPtrOutput struct{ *pulumi.OutputState }

func (MetastoreConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**MetastoreConfig)(nil)).Elem()
}

func (o MetastoreConfigPtrOutput) ToMetastoreConfigPtrOutput() MetastoreConfigPtrOutput {
	return o
}

func (o MetastoreConfigPtrOutput) ToMetastoreConfigPtrOutputWithContext(ctx context.Context) MetastoreConfigPtrOutput {
	return o
}

func (o MetastoreConfigPtrOutput) Elem() MetastoreConfigOutput {
	return o.ApplyT(func(v *MetastoreConfig) MetastoreConfig {
		if v != nil {
			return *v
		}
		var ret MetastoreConfig
		return ret
	}).(MetastoreConfigOutput)
}

// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
func (o MetastoreConfigPtrOutput) DataprocMetastoreService() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *MetastoreConfig) *string {
		if v == nil {
			return nil
		}
		return &v.DataprocMetastoreService
	}).(pulumi.StringPtrOutput)
}

// Specifies a Metastore configuration.
type MetastoreConfigResponse struct {
	// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
	DataprocMetastoreService string `pulumi:"dataprocMetastoreService"`
}

// Specifies a Metastore configuration.
type MetastoreConfigResponseOutput struct{ *pulumi.OutputState }

func (MetastoreConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*MetastoreConfigResponse)(nil)).Elem()
}

func (o MetastoreConfigResponseOutput) ToMetastoreConfigResponseOutput() MetastoreConfigResponseOutput {
	return o
}

func (o MetastoreConfigResponseOutput) ToMetastoreConfigResponseOutputWithContext(ctx context.Context) MetastoreConfigResponseOutput {
	return o
}

// Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[dataproc_region]/services/[service-name]
func (o MetastoreConfigResponseOutput) DataprocMetastoreService() pulumi.StringOutput {
	return o.ApplyT(func(v MetastoreConfigResponse) string { return v.DataprocMetastoreService }).(pulumi.StringOutput)
}

// A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTarget struct {
	// Optional. A namespace within the GKE cluster to deploy into.
	ClusterNamespace *string `pulumi:"clusterNamespace"`
	// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
	TargetGkeCluster *string `pulumi:"targetGkeCluster"`
}

// NamespacedGkeDeploymentTargetInput is an input type that accepts NamespacedGkeDeploymentTargetArgs and NamespacedGkeDeploymentTargetOutput values.
// You can construct a concrete instance of `NamespacedGkeDeploymentTargetInput` via:
//
//          NamespacedGkeDeploymentTargetArgs{...}
type NamespacedGkeDeploymentTargetInput interface {
	pulumi.Input

	ToNamespacedGkeDeploymentTargetOutput() NamespacedGkeDeploymentTargetOutput
	ToNamespacedGkeDeploymentTargetOutputWithContext(context.Context) NamespacedGkeDeploymentTargetOutput
}

// A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTargetArgs struct {
	// Optional. A namespace within the GKE cluster to deploy into.
	ClusterNamespace pulumi.StringPtrInput `pulumi:"clusterNamespace"`
	// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
	TargetGkeCluster pulumi.StringPtrInput `pulumi:"targetGkeCluster"`
}

func (NamespacedGkeDeploymentTargetArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*NamespacedGkeDeploymentTarget)(nil)).Elem()
}

func (i NamespacedGkeDeploymentTargetArgs) ToNamespacedGkeDeploymentTargetOutput() NamespacedGkeDeploymentTargetOutput {
	return i.ToNamespacedGkeDeploymentTargetOutputWithContext(context.Background())
}

func (i NamespacedGkeDeploymentTargetArgs) ToNamespacedGkeDeploymentTargetOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NamespacedGkeDeploymentTargetOutput)
}

func (i NamespacedGkeDeploymentTargetArgs) ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput {
	return i.ToNamespacedGkeDeploymentTargetPtrOutputWithContext(context.Background())
}

func (i NamespacedGkeDeploymentTargetArgs) ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NamespacedGkeDeploymentTargetOutput).ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx)
}

// NamespacedGkeDeploymentTargetPtrInput is an input type that accepts NamespacedGkeDeploymentTargetArgs, NamespacedGkeDeploymentTargetPtr and NamespacedGkeDeploymentTargetPtrOutput values.
// You can construct a concrete instance of `NamespacedGkeDeploymentTargetPtrInput` via:
//
//          NamespacedGkeDeploymentTargetArgs{...}
//
//  or:
//
//          nil
type NamespacedGkeDeploymentTargetPtrInput interface {
	pulumi.Input

	ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput
	ToNamespacedGkeDeploymentTargetPtrOutputWithContext(context.Context) NamespacedGkeDeploymentTargetPtrOutput
}

type namespacedGkeDeploymentTargetPtrType NamespacedGkeDeploymentTargetArgs

func NamespacedGkeDeploymentTargetPtr(v *NamespacedGkeDeploymentTargetArgs) NamespacedGkeDeploymentTargetPtrInput {
	return (*namespacedGkeDeploymentTargetPtrType)(v)
}

func (*namespacedGkeDeploymentTargetPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**NamespacedGkeDeploymentTarget)(nil)).Elem()
}

func (i *namespacedGkeDeploymentTargetPtrType) ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput {
	return i.ToNamespacedGkeDeploymentTargetPtrOutputWithContext(context.Background())
}

func (i *namespacedGkeDeploymentTargetPtrType) ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NamespacedGkeDeploymentTargetPtrOutput)
}

// A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTargetOutput struct{ *pulumi.OutputState }

func (NamespacedGkeDeploymentTargetOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NamespacedGkeDeploymentTarget)(nil)).Elem()
}

func (o NamespacedGkeDeploymentTargetOutput) ToNamespacedGkeDeploymentTargetOutput() NamespacedGkeDeploymentTargetOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetOutput) ToNamespacedGkeDeploymentTargetOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetOutput) ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput {
	return o.ToNamespacedGkeDeploymentTargetPtrOutputWithContext(context.Background())
}

func (o NamespacedGkeDeploymentTargetOutput) ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v NamespacedGkeDeploymentTarget) *NamespacedGkeDeploymentTarget {
		return &v
	}).(NamespacedGkeDeploymentTargetPtrOutput)
}

// Optional. A namespace within the GKE cluster to deploy into.
func (o NamespacedGkeDeploymentTargetOutput) ClusterNamespace() pulumi.StringPtrOutput {
	return o.ApplyT(func(v NamespacedGkeDeploymentTarget) *string { return v.ClusterNamespace }).(pulumi.StringPtrOutput)
}

// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
func (o NamespacedGkeDeploymentTargetOutput) TargetGkeCluster() pulumi.StringPtrOutput {
	return o.ApplyT(func(v NamespacedGkeDeploymentTarget) *string { return v.TargetGkeCluster }).(pulumi.StringPtrOutput)
}

type NamespacedGkeDeploymentTargetPtrOutput struct{ *pulumi.OutputState }

func (NamespacedGkeDeploymentTargetPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**NamespacedGkeDeploymentTarget)(nil)).Elem()
}

func (o NamespacedGkeDeploymentTargetPtrOutput) ToNamespacedGkeDeploymentTargetPtrOutput() NamespacedGkeDeploymentTargetPtrOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetPtrOutput) ToNamespacedGkeDeploymentTargetPtrOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetPtrOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetPtrOutput) Elem() NamespacedGkeDeploymentTargetOutput {
	return o.ApplyT(func(v *NamespacedGkeDeploymentTarget) NamespacedGkeDeploymentTarget {
		if v != nil {
			return *v
		}
		var ret NamespacedGkeDeploymentTarget
		return ret
	}).(NamespacedGkeDeploymentTargetOutput)
}

// Optional. A namespace within the GKE cluster to deploy into.
func (o NamespacedGkeDeploymentTargetPtrOutput) ClusterNamespace() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *NamespacedGkeDeploymentTarget) *string {
		if v == nil {
			return nil
		}
		return v.ClusterNamespace
	}).(pulumi.StringPtrOutput)
}

// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
func (o NamespacedGkeDeploymentTargetPtrOutput) TargetGkeCluster() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *NamespacedGkeDeploymentTarget) *string {
		if v == nil {
			return nil
		}
		return v.TargetGkeCluster
	}).(pulumi.StringPtrOutput)
}

// A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTargetResponse struct {
	// Optional. A namespace within the GKE cluster to deploy into.
	ClusterNamespace string `pulumi:"clusterNamespace"`
	// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
	TargetGkeCluster string `pulumi:"targetGkeCluster"`
}

// A full, namespace-isolated deployment target for an existing GKE cluster.
type NamespacedGkeDeploymentTargetResponseOutput struct{ *pulumi.OutputState }

func (NamespacedGkeDeploymentTargetResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NamespacedGkeDeploymentTargetResponse)(nil)).Elem()
}

func (o NamespacedGkeDeploymentTargetResponseOutput) ToNamespacedGkeDeploymentTargetResponseOutput() NamespacedGkeDeploymentTargetResponseOutput {
	return o
}

func (o NamespacedGkeDeploymentTargetResponseOutput) ToNamespacedGkeDeploymentTargetResponseOutputWithContext(ctx context.Context) NamespacedGkeDeploymentTargetResponseOutput {
	return o
}

// Optional. A namespace within the GKE cluster to deploy into.
func (o NamespacedGkeDeploymentTargetResponseOutput) ClusterNamespace() pulumi.StringOutput {
	return o.ApplyT(func(v NamespacedGkeDeploymentTargetResponse) string { return v.ClusterNamespace }).(pulumi.StringOutput)
}

// Optional. The target GKE cluster to deploy to. Format: 'projects/{project}/locations/{location}/clusters/{cluster_id}'
func (o NamespacedGkeDeploymentTargetResponseOutput) TargetGkeCluster() pulumi.StringOutput {
	return o.ApplyT(func(v NamespacedGkeDeploymentTargetResponse) string { return v.TargetGkeCluster }).(pulumi.StringOutput)
}

// Node Group Affinity for clusters using sole-tenant node groups.
type NodeGroupAffinity struct {
	// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 node-group-1
	NodeGroupUri string `pulumi:"nodeGroupUri"`
}

// NodeGroupAffinityInput is an input type that accepts NodeGroupAffinityArgs and NodeGroupAffinityOutput values.
// You can construct a concrete instance of `NodeGroupAffinityInput` via:
//
//          NodeGroupAffinityArgs{...}
type NodeGroupAffinityInput interface {
	pulumi.Input

	ToNodeGroupAffinityOutput() NodeGroupAffinityOutput
	ToNodeGroupAffinityOutputWithContext(context.Context) NodeGroupAffinityOutput
}

// Node Group Affinity for clusters using sole-tenant node groups.
type NodeGroupAffinityArgs struct {
	// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 node-group-1
	NodeGroupUri pulumi.StringInput `pulumi:"nodeGroupUri"`
}

func (NodeGroupAffinityArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeGroupAffinity)(nil)).Elem()
}

func (i NodeGroupAffinityArgs) ToNodeGroupAffinityOutput() NodeGroupAffinityOutput {
	return i.ToNodeGroupAffinityOutputWithContext(context.Background())
}

func (i NodeGroupAffinityArgs) ToNodeGroupAffinityOutputWithContext(ctx context.Context) NodeGroupAffinityOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeGroupAffinityOutput)
}

func (i NodeGroupAffinityArgs) ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput {
	return i.ToNodeGroupAffinityPtrOutputWithContext(context.Background())
}

func (i NodeGroupAffinityArgs) ToNodeGroupAffinityPtrOutputWithContext(ctx context.Context) NodeGroupAffinityPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeGroupAffinityOutput).ToNodeGroupAffinityPtrOutputWithContext(ctx)
}

// NodeGroupAffinityPtrInput is an input type that accepts NodeGroupAffinityArgs, NodeGroupAffinityPtr and NodeGroupAffinityPtrOutput values.
// You can construct a concrete instance of `NodeGroupAffinityPtrInput` via:
//
//          NodeGroupAffinityArgs{...}
//
//  or:
//
//          nil
type NodeGroupAffinityPtrInput interface {
	pulumi.Input

	ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput
	ToNodeGroupAffinityPtrOutputWithContext(context.Context) NodeGroupAffinityPtrOutput
}

type nodeGroupAffinityPtrType NodeGroupAffinityArgs

func NodeGroupAffinityPtr(v *NodeGroupAffinityArgs) NodeGroupAffinityPtrInput {
	return (*nodeGroupAffinityPtrType)(v)
}

func (*nodeGroupAffinityPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**NodeGroupAffinity)(nil)).Elem()
}

func (i *nodeGroupAffinityPtrType) ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput {
	return i.ToNodeGroupAffinityPtrOutputWithContext(context.Background())
}

func (i *nodeGroupAffinityPtrType) ToNodeGroupAffinityPtrOutputWithContext(ctx context.Context) NodeGroupAffinityPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeGroupAffinityPtrOutput)
}

// Node Group Affinity for clusters using sole-tenant node groups.
type NodeGroupAffinityOutput struct{ *pulumi.OutputState }

func (NodeGroupAffinityOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeGroupAffinity)(nil)).Elem()
}

func (o NodeGroupAffinityOutput) ToNodeGroupAffinityOutput() NodeGroupAffinityOutput {
	return o
}

func (o NodeGroupAffinityOutput) ToNodeGroupAffinityOutputWithContext(ctx context.Context) NodeGroupAffinityOutput {
	return o
}

func (o NodeGroupAffinityOutput) ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput {
	return o.ToNodeGroupAffinityPtrOutputWithContext(context.Background())
}

func (o NodeGroupAffinityOutput) ToNodeGroupAffinityPtrOutputWithContext(ctx context.Context) NodeGroupAffinityPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v NodeGroupAffinity) *NodeGroupAffinity {
		return &v
	}).(NodeGroupAffinityPtrOutput)
}

// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 node-group-1
func (o NodeGroupAffinityOutput) NodeGroupUri() pulumi.StringOutput {
	return o.ApplyT(func(v NodeGroupAffinity) string { return v.NodeGroupUri }).(pulumi.StringOutput)
}

type NodeGroupAffinityPtrOutput struct{ *pulumi.OutputState }

func (NodeGroupAffinityPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**NodeGroupAffinity)(nil)).Elem()
}

func (o NodeGroupAffinityPtrOutput) ToNodeGroupAffinityPtrOutput() NodeGroupAffinityPtrOutput {
	return o
}

func (o NodeGroupAffinityPtrOutput) ToNodeGroupAffinityPtrOutputWithContext(ctx context.Context) NodeGroupAffinityPtrOutput {
	return o
}

func (o NodeGroupAffinityPtrOutput) Elem() NodeGroupAffinityOutput {
	return o.ApplyT(func(v *NodeGroupAffinity) NodeGroupAffinity {
		if v != nil {
			return *v
		}
		var ret NodeGroupAffinity
		return ret
	}).(NodeGroupAffinityOutput)
}

// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 node-group-1
func (o NodeGroupAffinityPtrOutput) NodeGroupUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *NodeGroupAffinity) *string {
		if v == nil {
			return nil
		}
		return &v.NodeGroupUri
	}).(pulumi.StringPtrOutput)
}

// Node Group Affinity for clusters using sole-tenant node groups.
type NodeGroupAffinityResponse struct {
	// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 node-group-1
	NodeGroupUri string `pulumi:"nodeGroupUri"`
}

// Node Group Affinity for clusters using sole-tenant node groups.
type NodeGroupAffinityResponseOutput struct{ *pulumi.OutputState }

func (NodeGroupAffinityResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeGroupAffinityResponse)(nil)).Elem()
}

func (o NodeGroupAffinityResponseOutput) ToNodeGroupAffinityResponseOutput() NodeGroupAffinityResponseOutput {
	return o
}

func (o NodeGroupAffinityResponseOutput) ToNodeGroupAffinityResponseOutputWithContext(ctx context.Context) NodeGroupAffinityResponseOutput {
	return o
}

// The URI of a sole-tenant node group resource (https://cloud.google.com/compute/docs/reference/rest/v1/nodeGroups) that the cluster will be created on.A full URL, partial URI, or node group name are valid. Examples: https://www.googleapis.com/compute/v1/projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 projects/[project_id]/zones/us-central1-a/nodeGroups/node-group-1 node-group-1
func (o NodeGroupAffinityResponseOutput) NodeGroupUri() pulumi.StringOutput {
	return o.ApplyT(func(v NodeGroupAffinityResponse) string { return v.NodeGroupUri }).(pulumi.StringOutput)
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationAction struct {
	// Cloud Storage URI of executable file.
	ExecutableFile string `pulumi:"executableFile"`
	// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
	ExecutionTimeout *string `pulumi:"executionTimeout"`
}

// NodeInitializationActionInput is an input type that accepts NodeInitializationActionArgs and NodeInitializationActionOutput values.
// You can construct a concrete instance of `NodeInitializationActionInput` via:
//
//          NodeInitializationActionArgs{...}
type NodeInitializationActionInput interface {
	pulumi.Input

	ToNodeInitializationActionOutput() NodeInitializationActionOutput
	ToNodeInitializationActionOutputWithContext(context.Context) NodeInitializationActionOutput
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationActionArgs struct {
	// Cloud Storage URI of executable file.
	ExecutableFile pulumi.StringInput `pulumi:"executableFile"`
	// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
	ExecutionTimeout pulumi.StringPtrInput `pulumi:"executionTimeout"`
}

func (NodeInitializationActionArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeInitializationAction)(nil)).Elem()
}

func (i NodeInitializationActionArgs) ToNodeInitializationActionOutput() NodeInitializationActionOutput {
	return i.ToNodeInitializationActionOutputWithContext(context.Background())
}

func (i NodeInitializationActionArgs) ToNodeInitializationActionOutputWithContext(ctx context.Context) NodeInitializationActionOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeInitializationActionOutput)
}

// NodeInitializationActionArrayInput is an input type that accepts NodeInitializationActionArray and NodeInitializationActionArrayOutput values.
// You can construct a concrete instance of `NodeInitializationActionArrayInput` via:
//
//          NodeInitializationActionArray{ NodeInitializationActionArgs{...} }
type NodeInitializationActionArrayInput interface {
	pulumi.Input

	ToNodeInitializationActionArrayOutput() NodeInitializationActionArrayOutput
	ToNodeInitializationActionArrayOutputWithContext(context.Context) NodeInitializationActionArrayOutput
}

type NodeInitializationActionArray []NodeInitializationActionInput

func (NodeInitializationActionArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]NodeInitializationAction)(nil)).Elem()
}

func (i NodeInitializationActionArray) ToNodeInitializationActionArrayOutput() NodeInitializationActionArrayOutput {
	return i.ToNodeInitializationActionArrayOutputWithContext(context.Background())
}

func (i NodeInitializationActionArray) ToNodeInitializationActionArrayOutputWithContext(ctx context.Context) NodeInitializationActionArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(NodeInitializationActionArrayOutput)
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationActionOutput struct{ *pulumi.OutputState }

func (NodeInitializationActionOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeInitializationAction)(nil)).Elem()
}

func (o NodeInitializationActionOutput) ToNodeInitializationActionOutput() NodeInitializationActionOutput {
	return o
}

func (o NodeInitializationActionOutput) ToNodeInitializationActionOutputWithContext(ctx context.Context) NodeInitializationActionOutput {
	return o
}

// Cloud Storage URI of executable file.
func (o NodeInitializationActionOutput) ExecutableFile() pulumi.StringOutput {
	return o.ApplyT(func(v NodeInitializationAction) string { return v.ExecutableFile }).(pulumi.StringOutput)
}

// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
func (o NodeInitializationActionOutput) ExecutionTimeout() pulumi.StringPtrOutput {
	return o.ApplyT(func(v NodeInitializationAction) *string { return v.ExecutionTimeout }).(pulumi.StringPtrOutput)
}

type NodeInitializationActionArrayOutput struct{ *pulumi.OutputState }

func (NodeInitializationActionArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]NodeInitializationAction)(nil)).Elem()
}

func (o NodeInitializationActionArrayOutput) ToNodeInitializationActionArrayOutput() NodeInitializationActionArrayOutput {
	return o
}

func (o NodeInitializationActionArrayOutput) ToNodeInitializationActionArrayOutputWithContext(ctx context.Context) NodeInitializationActionArrayOutput {
	return o
}

func (o NodeInitializationActionArrayOutput) Index(i pulumi.IntInput) NodeInitializationActionOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) NodeInitializationAction {
		return vs[0].([]NodeInitializationAction)[vs[1].(int)]
	}).(NodeInitializationActionOutput)
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationActionResponse struct {
	// Cloud Storage URI of executable file.
	ExecutableFile string `pulumi:"executableFile"`
	// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
	ExecutionTimeout string `pulumi:"executionTimeout"`
}

// Specifies an executable to run on a fully configured node and a timeout period for executable completion.
type NodeInitializationActionResponseOutput struct{ *pulumi.OutputState }

func (NodeInitializationActionResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*NodeInitializationActionResponse)(nil)).Elem()
}

func (o NodeInitializationActionResponseOutput) ToNodeInitializationActionResponseOutput() NodeInitializationActionResponseOutput {
	return o
}

func (o NodeInitializationActionResponseOutput) ToNodeInitializationActionResponseOutputWithContext(ctx context.Context) NodeInitializationActionResponseOutput {
	return o
}

// Cloud Storage URI of executable file.
func (o NodeInitializationActionResponseOutput) ExecutableFile() pulumi.StringOutput {
	return o.ApplyT(func(v NodeInitializationActionResponse) string { return v.ExecutableFile }).(pulumi.StringOutput)
}

// Optional. Amount of time executable has to complete. Default is 10 minutes (see JSON representation of Duration (https://developers.google.com/protocol-buffers/docs/proto3#json)).Cluster creation fails with an explanatory error message (the name of the executable that caused the error and the exceeded timeout period) if the executable is not completed at end of the timeout period.
func (o NodeInitializationActionResponseOutput) ExecutionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v NodeInitializationActionResponse) string { return v.ExecutionTimeout }).(pulumi.StringOutput)
}

type NodeInitializationActionResponseArrayOutput struct{ *pulumi.OutputState }

func (NodeInitializationActionResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]NodeInitializationActionResponse)(nil)).Elem()
}

func (o NodeInitializationActionResponseArrayOutput) ToNodeInitializationActionResponseArrayOutput() NodeInitializationActionResponseArrayOutput {
	return o
}

func (o NodeInitializationActionResponseArrayOutput) ToNodeInitializationActionResponseArrayOutputWithContext(ctx context.Context) NodeInitializationActionResponseArrayOutput {
	return o
}

func (o NodeInitializationActionResponseArrayOutput) Index(i pulumi.IntInput) NodeInitializationActionResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) NodeInitializationActionResponse {
		return vs[0].([]NodeInitializationActionResponse)[vs[1].(int)]
	}).(NodeInitializationActionResponseOutput)
}

// A job executed by the workflow.
type OrderedJob struct {
	// Optional. Job is a Hadoop job.
	HadoopJob *HadoopJob `pulumi:"hadoopJob"`
	// Optional. Job is a Hive job.
	HiveJob *HiveJob `pulumi:"hiveJob"`
	// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
	Labels map[string]string `pulumi:"labels"`
	// Optional. Job is a Pig job.
	PigJob *PigJob `pulumi:"pigJob"`
	// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
	PrerequisiteStepIds []string `pulumi:"prerequisiteStepIds"`
	// Optional. Job is a Presto job.
	PrestoJob *PrestoJob `pulumi:"prestoJob"`
	// Optional. Job is a PySpark job.
	PysparkJob *PySparkJob `pulumi:"pysparkJob"`
	// Optional. Job scheduling configuration.
	Scheduling *JobScheduling `pulumi:"scheduling"`
	// Optional. Job is a Spark job.
	SparkJob *SparkJob `pulumi:"sparkJob"`
	// Optional. Job is a SparkR job.
	SparkRJob *SparkRJob `pulumi:"sparkRJob"`
	// Optional. Job is a SparkSql job.
	SparkSqlJob *SparkSqlJob `pulumi:"sparkSqlJob"`
	// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
	StepId string `pulumi:"stepId"`
}

// OrderedJobInput is an input type that accepts OrderedJobArgs and OrderedJobOutput values.
// You can construct a concrete instance of `OrderedJobInput` via:
//
//          OrderedJobArgs{...}
type OrderedJobInput interface {
	pulumi.Input

	ToOrderedJobOutput() OrderedJobOutput
	ToOrderedJobOutputWithContext(context.Context) OrderedJobOutput
}

// A job executed by the workflow.
type OrderedJobArgs struct {
	// Optional. Job is a Hadoop job.
	HadoopJob HadoopJobPtrInput `pulumi:"hadoopJob"`
	// Optional. Job is a Hive job.
	HiveJob HiveJobPtrInput `pulumi:"hiveJob"`
	// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
	Labels pulumi.StringMapInput `pulumi:"labels"`
	// Optional. Job is a Pig job.
	PigJob PigJobPtrInput `pulumi:"pigJob"`
	// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
	PrerequisiteStepIds pulumi.StringArrayInput `pulumi:"prerequisiteStepIds"`
	// Optional. Job is a Presto job.
	PrestoJob PrestoJobPtrInput `pulumi:"prestoJob"`
	// Optional. Job is a PySpark job.
	PysparkJob PySparkJobPtrInput `pulumi:"pysparkJob"`
	// Optional. Job scheduling configuration.
	Scheduling JobSchedulingPtrInput `pulumi:"scheduling"`
	// Optional. Job is a Spark job.
	SparkJob SparkJobPtrInput `pulumi:"sparkJob"`
	// Optional. Job is a SparkR job.
	SparkRJob SparkRJobPtrInput `pulumi:"sparkRJob"`
	// Optional. Job is a SparkSql job.
	SparkSqlJob SparkSqlJobPtrInput `pulumi:"sparkSqlJob"`
	// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
	StepId pulumi.StringInput `pulumi:"stepId"`
}

func (OrderedJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*OrderedJob)(nil)).Elem()
}

func (i OrderedJobArgs) ToOrderedJobOutput() OrderedJobOutput {
	return i.ToOrderedJobOutputWithContext(context.Background())
}

func (i OrderedJobArgs) ToOrderedJobOutputWithContext(ctx context.Context) OrderedJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(OrderedJobOutput)
}

// OrderedJobArrayInput is an input type that accepts OrderedJobArray and OrderedJobArrayOutput values.
// You can construct a concrete instance of `OrderedJobArrayInput` via:
//
//          OrderedJobArray{ OrderedJobArgs{...} }
type OrderedJobArrayInput interface {
	pulumi.Input

	ToOrderedJobArrayOutput() OrderedJobArrayOutput
	ToOrderedJobArrayOutputWithContext(context.Context) OrderedJobArrayOutput
}

type OrderedJobArray []OrderedJobInput

func (OrderedJobArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]OrderedJob)(nil)).Elem()
}

func (i OrderedJobArray) ToOrderedJobArrayOutput() OrderedJobArrayOutput {
	return i.ToOrderedJobArrayOutputWithContext(context.Background())
}

func (i OrderedJobArray) ToOrderedJobArrayOutputWithContext(ctx context.Context) OrderedJobArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(OrderedJobArrayOutput)
}

// A job executed by the workflow.
type OrderedJobOutput struct{ *pulumi.OutputState }

func (OrderedJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*OrderedJob)(nil)).Elem()
}

func (o OrderedJobOutput) ToOrderedJobOutput() OrderedJobOutput {
	return o
}

func (o OrderedJobOutput) ToOrderedJobOutputWithContext(ctx context.Context) OrderedJobOutput {
	return o
}

// Optional. Job is a Hadoop job.
func (o OrderedJobOutput) HadoopJob() HadoopJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *HadoopJob { return v.HadoopJob }).(HadoopJobPtrOutput)
}

// Optional. Job is a Hive job.
func (o OrderedJobOutput) HiveJob() HiveJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *HiveJob { return v.HiveJob }).(HiveJobPtrOutput)
}

// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
func (o OrderedJobOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v OrderedJob) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

// Optional. Job is a Pig job.
func (o OrderedJobOutput) PigJob() PigJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *PigJob { return v.PigJob }).(PigJobPtrOutput)
}

// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
func (o OrderedJobOutput) PrerequisiteStepIds() pulumi.StringArrayOutput {
	return o.ApplyT(func(v OrderedJob) []string { return v.PrerequisiteStepIds }).(pulumi.StringArrayOutput)
}

// Optional. Job is a Presto job.
func (o OrderedJobOutput) PrestoJob() PrestoJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *PrestoJob { return v.PrestoJob }).(PrestoJobPtrOutput)
}

// Optional. Job is a PySpark job.
func (o OrderedJobOutput) PysparkJob() PySparkJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *PySparkJob { return v.PysparkJob }).(PySparkJobPtrOutput)
}

// Optional. Job scheduling configuration.
func (o OrderedJobOutput) Scheduling() JobSchedulingPtrOutput {
	return o.ApplyT(func(v OrderedJob) *JobScheduling { return v.Scheduling }).(JobSchedulingPtrOutput)
}

// Optional. Job is a Spark job.
func (o OrderedJobOutput) SparkJob() SparkJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *SparkJob { return v.SparkJob }).(SparkJobPtrOutput)
}

// Optional. Job is a SparkR job.
func (o OrderedJobOutput) SparkRJob() SparkRJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *SparkRJob { return v.SparkRJob }).(SparkRJobPtrOutput)
}

// Optional. Job is a SparkSql job.
func (o OrderedJobOutput) SparkSqlJob() SparkSqlJobPtrOutput {
	return o.ApplyT(func(v OrderedJob) *SparkSqlJob { return v.SparkSqlJob }).(SparkSqlJobPtrOutput)
}

// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
func (o OrderedJobOutput) StepId() pulumi.StringOutput {
	return o.ApplyT(func(v OrderedJob) string { return v.StepId }).(pulumi.StringOutput)
}

type OrderedJobArrayOutput struct{ *pulumi.OutputState }

func (OrderedJobArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]OrderedJob)(nil)).Elem()
}

func (o OrderedJobArrayOutput) ToOrderedJobArrayOutput() OrderedJobArrayOutput {
	return o
}

func (o OrderedJobArrayOutput) ToOrderedJobArrayOutputWithContext(ctx context.Context) OrderedJobArrayOutput {
	return o
}

func (o OrderedJobArrayOutput) Index(i pulumi.IntInput) OrderedJobOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) OrderedJob {
		return vs[0].([]OrderedJob)[vs[1].(int)]
	}).(OrderedJobOutput)
}

// A job executed by the workflow.
type OrderedJobResponse struct {
	// Optional. Job is a Hadoop job.
	HadoopJob HadoopJobResponse `pulumi:"hadoopJob"`
	// Optional. Job is a Hive job.
	HiveJob HiveJobResponse `pulumi:"hiveJob"`
	// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
	Labels map[string]string `pulumi:"labels"`
	// Optional. Job is a Pig job.
	PigJob PigJobResponse `pulumi:"pigJob"`
	// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
	PrerequisiteStepIds []string `pulumi:"prerequisiteStepIds"`
	// Optional. Job is a Presto job.
	PrestoJob PrestoJobResponse `pulumi:"prestoJob"`
	// Optional. Job is a PySpark job.
	PysparkJob PySparkJobResponse `pulumi:"pysparkJob"`
	// Optional. Job scheduling configuration.
	Scheduling JobSchedulingResponse `pulumi:"scheduling"`
	// Optional. Job is a Spark job.
	SparkJob SparkJobResponse `pulumi:"sparkJob"`
	// Optional. Job is a SparkR job.
	SparkRJob SparkRJobResponse `pulumi:"sparkRJob"`
	// Optional. Job is a SparkSql job.
	SparkSqlJob SparkSqlJobResponse `pulumi:"sparkSqlJob"`
	// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
	StepId string `pulumi:"stepId"`
}

// A job executed by the workflow.
type OrderedJobResponseOutput struct{ *pulumi.OutputState }

func (OrderedJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*OrderedJobResponse)(nil)).Elem()
}

func (o OrderedJobResponseOutput) ToOrderedJobResponseOutput() OrderedJobResponseOutput {
	return o
}

func (o OrderedJobResponseOutput) ToOrderedJobResponseOutputWithContext(ctx context.Context) OrderedJobResponseOutput {
	return o
}

// Optional. Job is a Hadoop job.
func (o OrderedJobResponseOutput) HadoopJob() HadoopJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) HadoopJobResponse { return v.HadoopJob }).(HadoopJobResponseOutput)
}

// Optional. Job is a Hive job.
func (o OrderedJobResponseOutput) HiveJob() HiveJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) HiveJobResponse { return v.HiveJob }).(HiveJobResponseOutput)
}

// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
func (o OrderedJobResponseOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v OrderedJobResponse) map[string]string { return v.Labels }).(pulumi.StringMapOutput)
}

// Optional. Job is a Pig job.
func (o OrderedJobResponseOutput) PigJob() PigJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) PigJobResponse { return v.PigJob }).(PigJobResponseOutput)
}

// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
func (o OrderedJobResponseOutput) PrerequisiteStepIds() pulumi.StringArrayOutput {
	return o.ApplyT(func(v OrderedJobResponse) []string { return v.PrerequisiteStepIds }).(pulumi.StringArrayOutput)
}

// Optional. Job is a Presto job.
func (o OrderedJobResponseOutput) PrestoJob() PrestoJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) PrestoJobResponse { return v.PrestoJob }).(PrestoJobResponseOutput)
}

// Optional. Job is a PySpark job.
func (o OrderedJobResponseOutput) PysparkJob() PySparkJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) PySparkJobResponse { return v.PysparkJob }).(PySparkJobResponseOutput)
}

// Optional. Job scheduling configuration.
func (o OrderedJobResponseOutput) Scheduling() JobSchedulingResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) JobSchedulingResponse { return v.Scheduling }).(JobSchedulingResponseOutput)
}

// Optional. Job is a Spark job.
func (o OrderedJobResponseOutput) SparkJob() SparkJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) SparkJobResponse { return v.SparkJob }).(SparkJobResponseOutput)
}

// Optional. Job is a SparkR job.
func (o OrderedJobResponseOutput) SparkRJob() SparkRJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) SparkRJobResponse { return v.SparkRJob }).(SparkRJobResponseOutput)
}

// Optional. Job is a SparkSql job.
func (o OrderedJobResponseOutput) SparkSqlJob() SparkSqlJobResponseOutput {
	return o.ApplyT(func(v OrderedJobResponse) SparkSqlJobResponse { return v.SparkSqlJob }).(SparkSqlJobResponseOutput)
}

// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
func (o OrderedJobResponseOutput) StepId() pulumi.StringOutput {
	return o.ApplyT(func(v OrderedJobResponse) string { return v.StepId }).(pulumi.StringOutput)
}

type OrderedJobResponseArrayOutput struct{ *pulumi.OutputState }

func (OrderedJobResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]OrderedJobResponse)(nil)).Elem()
}

func (o OrderedJobResponseArrayOutput) ToOrderedJobResponseArrayOutput() OrderedJobResponseArrayOutput {
	return o
}

func (o OrderedJobResponseArrayOutput) ToOrderedJobResponseArrayOutputWithContext(ctx context.Context) OrderedJobResponseArrayOutput {
	return o
}

func (o OrderedJobResponseArrayOutput) Index(i pulumi.IntInput) OrderedJobResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) OrderedJobResponse {
		return vs[0].([]OrderedJobResponse)[vs[1].(int)]
	}).(OrderedJobResponseOutput)
}

// Configuration for parameter validation.
type ParameterValidation struct {
	// Validation based on regular expressions.
	Regex *RegexValidation `pulumi:"regex"`
	// Validation based on a list of allowed values.
	Values *ValueValidation `pulumi:"values"`
}

// ParameterValidationInput is an input type that accepts ParameterValidationArgs and ParameterValidationOutput values.
// You can construct a concrete instance of `ParameterValidationInput` via:
//
//          ParameterValidationArgs{...}
type ParameterValidationInput interface {
	pulumi.Input

	ToParameterValidationOutput() ParameterValidationOutput
	ToParameterValidationOutputWithContext(context.Context) ParameterValidationOutput
}

// Configuration for parameter validation.
type ParameterValidationArgs struct {
	// Validation based on regular expressions.
	Regex RegexValidationPtrInput `pulumi:"regex"`
	// Validation based on a list of allowed values.
	Values ValueValidationPtrInput `pulumi:"values"`
}

func (ParameterValidationArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ParameterValidation)(nil)).Elem()
}

func (i ParameterValidationArgs) ToParameterValidationOutput() ParameterValidationOutput {
	return i.ToParameterValidationOutputWithContext(context.Background())
}

func (i ParameterValidationArgs) ToParameterValidationOutputWithContext(ctx context.Context) ParameterValidationOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ParameterValidationOutput)
}

func (i ParameterValidationArgs) ToParameterValidationPtrOutput() ParameterValidationPtrOutput {
	return i.ToParameterValidationPtrOutputWithContext(context.Background())
}

func (i ParameterValidationArgs) ToParameterValidationPtrOutputWithContext(ctx context.Context) ParameterValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ParameterValidationOutput).ToParameterValidationPtrOutputWithContext(ctx)
}

// ParameterValidationPtrInput is an input type that accepts ParameterValidationArgs, ParameterValidationPtr and ParameterValidationPtrOutput values.
// You can construct a concrete instance of `ParameterValidationPtrInput` via:
//
//          ParameterValidationArgs{...}
//
//  or:
//
//          nil
type ParameterValidationPtrInput interface {
	pulumi.Input

	ToParameterValidationPtrOutput() ParameterValidationPtrOutput
	ToParameterValidationPtrOutputWithContext(context.Context) ParameterValidationPtrOutput
}

type parameterValidationPtrType ParameterValidationArgs

func ParameterValidationPtr(v *ParameterValidationArgs) ParameterValidationPtrInput {
	return (*parameterValidationPtrType)(v)
}

func (*parameterValidationPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ParameterValidation)(nil)).Elem()
}

func (i *parameterValidationPtrType) ToParameterValidationPtrOutput() ParameterValidationPtrOutput {
	return i.ToParameterValidationPtrOutputWithContext(context.Background())
}

func (i *parameterValidationPtrType) ToParameterValidationPtrOutputWithContext(ctx context.Context) ParameterValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ParameterValidationPtrOutput)
}

// Configuration for parameter validation.
type ParameterValidationOutput struct{ *pulumi.OutputState }

func (ParameterValidationOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ParameterValidation)(nil)).Elem()
}

func (o ParameterValidationOutput) ToParameterValidationOutput() ParameterValidationOutput {
	return o
}

func (o ParameterValidationOutput) ToParameterValidationOutputWithContext(ctx context.Context) ParameterValidationOutput {
	return o
}

func (o ParameterValidationOutput) ToParameterValidationPtrOutput() ParameterValidationPtrOutput {
	return o.ToParameterValidationPtrOutputWithContext(context.Background())
}

func (o ParameterValidationOutput) ToParameterValidationPtrOutputWithContext(ctx context.Context) ParameterValidationPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ParameterValidation) *ParameterValidation {
		return &v
	}).(ParameterValidationPtrOutput)
}

// Validation based on regular expressions.
func (o ParameterValidationOutput) Regex() RegexValidationPtrOutput {
	return o.ApplyT(func(v ParameterValidation) *RegexValidation { return v.Regex }).(RegexValidationPtrOutput)
}

// Validation based on a list of allowed values.
func (o ParameterValidationOutput) Values() ValueValidationPtrOutput {
	return o.ApplyT(func(v ParameterValidation) *ValueValidation { return v.Values }).(ValueValidationPtrOutput)
}

type ParameterValidationPtrOutput struct{ *pulumi.OutputState }

func (ParameterValidationPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ParameterValidation)(nil)).Elem()
}

func (o ParameterValidationPtrOutput) ToParameterValidationPtrOutput() ParameterValidationPtrOutput {
	return o
}

func (o ParameterValidationPtrOutput) ToParameterValidationPtrOutputWithContext(ctx context.Context) ParameterValidationPtrOutput {
	return o
}

func (o ParameterValidationPtrOutput) Elem() ParameterValidationOutput {
	return o.ApplyT(func(v *ParameterValidation) ParameterValidation {
		if v != nil {
			return *v
		}
		var ret ParameterValidation
		return ret
	}).(ParameterValidationOutput)
}

// Validation based on regular expressions.
func (o ParameterValidationPtrOutput) Regex() RegexValidationPtrOutput {
	return o.ApplyT(func(v *ParameterValidation) *RegexValidation {
		if v == nil {
			return nil
		}
		return v.Regex
	}).(RegexValidationPtrOutput)
}

// Validation based on a list of allowed values.
func (o ParameterValidationPtrOutput) Values() ValueValidationPtrOutput {
	return o.ApplyT(func(v *ParameterValidation) *ValueValidation {
		if v == nil {
			return nil
		}
		return v.Values
	}).(ValueValidationPtrOutput)
}

// Configuration for parameter validation.
type ParameterValidationResponse struct {
	// Validation based on regular expressions.
	Regex RegexValidationResponse `pulumi:"regex"`
	// Validation based on a list of allowed values.
	Values ValueValidationResponse `pulumi:"values"`
}

// Configuration for parameter validation.
type ParameterValidationResponseOutput struct{ *pulumi.OutputState }

func (ParameterValidationResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ParameterValidationResponse)(nil)).Elem()
}

func (o ParameterValidationResponseOutput) ToParameterValidationResponseOutput() ParameterValidationResponseOutput {
	return o
}

func (o ParameterValidationResponseOutput) ToParameterValidationResponseOutputWithContext(ctx context.Context) ParameterValidationResponseOutput {
	return o
}

// Validation based on regular expressions.
func (o ParameterValidationResponseOutput) Regex() RegexValidationResponseOutput {
	return o.ApplyT(func(v ParameterValidationResponse) RegexValidationResponse { return v.Regex }).(RegexValidationResponseOutput)
}

// Validation based on a list of allowed values.
func (o ParameterValidationResponseOutput) Values() ValueValidationResponseOutput {
	return o.ApplyT(func(v ParameterValidationResponse) ValueValidationResponse { return v.Values }).(ValueValidationResponseOutput)
}

// Auxiliary services configuration for a workload.
type PeripheralsConfig struct {
	// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
	MetastoreService *string `pulumi:"metastoreService"`
	// Optional. The Spark History Server configuration for the workload.
	SparkHistoryServerConfig *SparkHistoryServerConfig `pulumi:"sparkHistoryServerConfig"`
}

// PeripheralsConfigInput is an input type that accepts PeripheralsConfigArgs and PeripheralsConfigOutput values.
// You can construct a concrete instance of `PeripheralsConfigInput` via:
//
//          PeripheralsConfigArgs{...}
type PeripheralsConfigInput interface {
	pulumi.Input

	ToPeripheralsConfigOutput() PeripheralsConfigOutput
	ToPeripheralsConfigOutputWithContext(context.Context) PeripheralsConfigOutput
}

// Auxiliary services configuration for a workload.
type PeripheralsConfigArgs struct {
	// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
	MetastoreService pulumi.StringPtrInput `pulumi:"metastoreService"`
	// Optional. The Spark History Server configuration for the workload.
	SparkHistoryServerConfig SparkHistoryServerConfigPtrInput `pulumi:"sparkHistoryServerConfig"`
}

func (PeripheralsConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PeripheralsConfig)(nil)).Elem()
}

func (i PeripheralsConfigArgs) ToPeripheralsConfigOutput() PeripheralsConfigOutput {
	return i.ToPeripheralsConfigOutputWithContext(context.Background())
}

func (i PeripheralsConfigArgs) ToPeripheralsConfigOutputWithContext(ctx context.Context) PeripheralsConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PeripheralsConfigOutput)
}

func (i PeripheralsConfigArgs) ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput {
	return i.ToPeripheralsConfigPtrOutputWithContext(context.Background())
}

func (i PeripheralsConfigArgs) ToPeripheralsConfigPtrOutputWithContext(ctx context.Context) PeripheralsConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PeripheralsConfigOutput).ToPeripheralsConfigPtrOutputWithContext(ctx)
}

// PeripheralsConfigPtrInput is an input type that accepts PeripheralsConfigArgs, PeripheralsConfigPtr and PeripheralsConfigPtrOutput values.
// You can construct a concrete instance of `PeripheralsConfigPtrInput` via:
//
//          PeripheralsConfigArgs{...}
//
//  or:
//
//          nil
type PeripheralsConfigPtrInput interface {
	pulumi.Input

	ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput
	ToPeripheralsConfigPtrOutputWithContext(context.Context) PeripheralsConfigPtrOutput
}

type peripheralsConfigPtrType PeripheralsConfigArgs

func PeripheralsConfigPtr(v *PeripheralsConfigArgs) PeripheralsConfigPtrInput {
	return (*peripheralsConfigPtrType)(v)
}

func (*peripheralsConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PeripheralsConfig)(nil)).Elem()
}

func (i *peripheralsConfigPtrType) ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput {
	return i.ToPeripheralsConfigPtrOutputWithContext(context.Background())
}

func (i *peripheralsConfigPtrType) ToPeripheralsConfigPtrOutputWithContext(ctx context.Context) PeripheralsConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PeripheralsConfigPtrOutput)
}

// Auxiliary services configuration for a workload.
type PeripheralsConfigOutput struct{ *pulumi.OutputState }

func (PeripheralsConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PeripheralsConfig)(nil)).Elem()
}

func (o PeripheralsConfigOutput) ToPeripheralsConfigOutput() PeripheralsConfigOutput {
	return o
}

func (o PeripheralsConfigOutput) ToPeripheralsConfigOutputWithContext(ctx context.Context) PeripheralsConfigOutput {
	return o
}

func (o PeripheralsConfigOutput) ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput {
	return o.ToPeripheralsConfigPtrOutputWithContext(context.Background())
}

func (o PeripheralsConfigOutput) ToPeripheralsConfigPtrOutputWithContext(ctx context.Context) PeripheralsConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PeripheralsConfig) *PeripheralsConfig {
		return &v
	}).(PeripheralsConfigPtrOutput)
}

// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
func (o PeripheralsConfigOutput) MetastoreService() pulumi.StringPtrOutput {
	return o.ApplyT(func(v PeripheralsConfig) *string { return v.MetastoreService }).(pulumi.StringPtrOutput)
}

// Optional. The Spark History Server configuration for the workload.
func (o PeripheralsConfigOutput) SparkHistoryServerConfig() SparkHistoryServerConfigPtrOutput {
	return o.ApplyT(func(v PeripheralsConfig) *SparkHistoryServerConfig { return v.SparkHistoryServerConfig }).(SparkHistoryServerConfigPtrOutput)
}

type PeripheralsConfigPtrOutput struct{ *pulumi.OutputState }

func (PeripheralsConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PeripheralsConfig)(nil)).Elem()
}

func (o PeripheralsConfigPtrOutput) ToPeripheralsConfigPtrOutput() PeripheralsConfigPtrOutput {
	return o
}

func (o PeripheralsConfigPtrOutput) ToPeripheralsConfigPtrOutputWithContext(ctx context.Context) PeripheralsConfigPtrOutput {
	return o
}

func (o PeripheralsConfigPtrOutput) Elem() PeripheralsConfigOutput {
	return o.ApplyT(func(v *PeripheralsConfig) PeripheralsConfig {
		if v != nil {
			return *v
		}
		var ret PeripheralsConfig
		return ret
	}).(PeripheralsConfigOutput)
}

// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
func (o PeripheralsConfigPtrOutput) MetastoreService() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PeripheralsConfig) *string {
		if v == nil {
			return nil
		}
		return v.MetastoreService
	}).(pulumi.StringPtrOutput)
}

// Optional. The Spark History Server configuration for the workload.
func (o PeripheralsConfigPtrOutput) SparkHistoryServerConfig() SparkHistoryServerConfigPtrOutput {
	return o.ApplyT(func(v *PeripheralsConfig) *SparkHistoryServerConfig {
		if v == nil {
			return nil
		}
		return v.SparkHistoryServerConfig
	}).(SparkHistoryServerConfigPtrOutput)
}

// Auxiliary services configuration for a workload.
type PeripheralsConfigResponse struct {
	// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
	MetastoreService string `pulumi:"metastoreService"`
	// Optional. The Spark History Server configuration for the workload.
	SparkHistoryServerConfig SparkHistoryServerConfigResponse `pulumi:"sparkHistoryServerConfig"`
}

// Auxiliary services configuration for a workload.
type PeripheralsConfigResponseOutput struct{ *pulumi.OutputState }

func (PeripheralsConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PeripheralsConfigResponse)(nil)).Elem()
}

func (o PeripheralsConfigResponseOutput) ToPeripheralsConfigResponseOutput() PeripheralsConfigResponseOutput {
	return o
}

func (o PeripheralsConfigResponseOutput) ToPeripheralsConfigResponseOutputWithContext(ctx context.Context) PeripheralsConfigResponseOutput {
	return o
}

// Optional. Resource name of an existing Dataproc Metastore service.Example: projects/[project_id]/locations/[region]/services/[service_id]
func (o PeripheralsConfigResponseOutput) MetastoreService() pulumi.StringOutput {
	return o.ApplyT(func(v PeripheralsConfigResponse) string { return v.MetastoreService }).(pulumi.StringOutput)
}

// Optional. The Spark History Server configuration for the workload.
func (o PeripheralsConfigResponseOutput) SparkHistoryServerConfig() SparkHistoryServerConfigResponseOutput {
	return o.ApplyT(func(v PeripheralsConfigResponse) SparkHistoryServerConfigResponse { return v.SparkHistoryServerConfig }).(SparkHistoryServerConfigResponseOutput)
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJob struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure *bool `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains the Pig queries.
	QueryFileUri *string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList *QueryList `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// PigJobInput is an input type that accepts PigJobArgs and PigJobOutput values.
// You can construct a concrete instance of `PigJobInput` via:
//
//          PigJobArgs{...}
type PigJobInput interface {
	pulumi.Input

	ToPigJobOutput() PigJobOutput
	ToPigJobOutputWithContext(context.Context) PigJobOutput
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJobArgs struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure pulumi.BoolPtrInput `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// The HCFS URI of the script that contains the Pig queries.
	QueryFileUri pulumi.StringPtrInput `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListPtrInput `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
	ScriptVariables pulumi.StringMapInput `pulumi:"scriptVariables"`
}

func (PigJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PigJob)(nil)).Elem()
}

func (i PigJobArgs) ToPigJobOutput() PigJobOutput {
	return i.ToPigJobOutputWithContext(context.Background())
}

func (i PigJobArgs) ToPigJobOutputWithContext(ctx context.Context) PigJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PigJobOutput)
}

func (i PigJobArgs) ToPigJobPtrOutput() PigJobPtrOutput {
	return i.ToPigJobPtrOutputWithContext(context.Background())
}

func (i PigJobArgs) ToPigJobPtrOutputWithContext(ctx context.Context) PigJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PigJobOutput).ToPigJobPtrOutputWithContext(ctx)
}

// PigJobPtrInput is an input type that accepts PigJobArgs, PigJobPtr and PigJobPtrOutput values.
// You can construct a concrete instance of `PigJobPtrInput` via:
//
//          PigJobArgs{...}
//
//  or:
//
//          nil
type PigJobPtrInput interface {
	pulumi.Input

	ToPigJobPtrOutput() PigJobPtrOutput
	ToPigJobPtrOutputWithContext(context.Context) PigJobPtrOutput
}

type pigJobPtrType PigJobArgs

func PigJobPtr(v *PigJobArgs) PigJobPtrInput {
	return (*pigJobPtrType)(v)
}

func (*pigJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PigJob)(nil)).Elem()
}

func (i *pigJobPtrType) ToPigJobPtrOutput() PigJobPtrOutput {
	return i.ToPigJobPtrOutputWithContext(context.Background())
}

func (i *pigJobPtrType) ToPigJobPtrOutputWithContext(ctx context.Context) PigJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PigJobPtrOutput)
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJobOutput struct{ *pulumi.OutputState }

func (PigJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PigJob)(nil)).Elem()
}

func (o PigJobOutput) ToPigJobOutput() PigJobOutput {
	return o
}

func (o PigJobOutput) ToPigJobOutputWithContext(ctx context.Context) PigJobOutput {
	return o
}

func (o PigJobOutput) ToPigJobPtrOutput() PigJobPtrOutput {
	return o.ToPigJobPtrOutputWithContext(context.Background())
}

func (o PigJobOutput) ToPigJobPtrOutputWithContext(ctx context.Context) PigJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PigJob) *PigJob {
		return &v
	}).(PigJobPtrOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PigJobOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v PigJob) *bool { return v.ContinueOnFailure }).(pulumi.BoolPtrOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
func (o PigJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PigJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PigJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v PigJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
func (o PigJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PigJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains the Pig queries.
func (o PigJobOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v PigJob) *string { return v.QueryFileUri }).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o PigJobOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v PigJob) *QueryList { return v.QueryList }).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
func (o PigJobOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v PigJob) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

type PigJobPtrOutput struct{ *pulumi.OutputState }

func (PigJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PigJob)(nil)).Elem()
}

func (o PigJobPtrOutput) ToPigJobPtrOutput() PigJobPtrOutput {
	return o
}

func (o PigJobPtrOutput) ToPigJobPtrOutputWithContext(ctx context.Context) PigJobPtrOutput {
	return o
}

func (o PigJobPtrOutput) Elem() PigJobOutput {
	return o.ApplyT(func(v *PigJob) PigJob {
		if v != nil {
			return *v
		}
		var ret PigJob
		return ret
	}).(PigJobOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PigJobPtrOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *PigJob) *bool {
		if v == nil {
			return nil
		}
		return v.ContinueOnFailure
	}).(pulumi.BoolPtrOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
func (o PigJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PigJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PigJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *PigJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
func (o PigJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *PigJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains the Pig queries.
func (o PigJobPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PigJob) *string {
		if v == nil {
			return nil
		}
		return v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o PigJobPtrOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v *PigJob) *QueryList {
		if v == nil {
			return nil
		}
		return v.QueryList
	}).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
func (o PigJobPtrOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v *PigJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.ScriptVariables
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJobResponse struct {
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure bool `pulumi:"continueOnFailure"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains the Pig queries.
	QueryFileUri string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListResponse `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// A Dataproc job for running Apache Pig (https://pig.apache.org/) queries on YARN.
type PigJobResponseOutput struct{ *pulumi.OutputState }

func (PigJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PigJobResponse)(nil)).Elem()
}

func (o PigJobResponseOutput) ToPigJobResponseOutput() PigJobResponseOutput {
	return o
}

func (o PigJobResponseOutput) ToPigJobResponseOutputWithContext(ctx context.Context) PigJobResponseOutput {
	return o
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PigJobResponseOutput) ContinueOnFailure() pulumi.BoolOutput {
	return o.ApplyT(func(v PigJobResponse) bool { return v.ContinueOnFailure }).(pulumi.BoolOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATH of the Pig Client and Hadoop MapReduce (MR) tasks. Can contain Pig UDFs.
func (o PigJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PigJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PigJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v PigJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// Optional. A mapping of property names to values, used to configure Pig. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/hadoop/conf/*-site.xml, /etc/pig/conf/pig.properties, and classes in user code.
func (o PigJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PigJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains the Pig queries.
func (o PigJobResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PigJobResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// A list of queries.
func (o PigJobResponseOutput) QueryList() QueryListResponseOutput {
	return o.ApplyT(func(v PigJobResponse) QueryListResponse { return v.QueryList }).(QueryListResponseOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Pig command: name=[value]).
func (o PigJobResponseOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v PigJobResponse) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJob struct {
	// Optional. Presto client tags to attach to this query
	ClientTags []string `pulumi:"clientTags"`
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure *bool `pulumi:"continueOnFailure"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
	OutputFormat *string `pulumi:"outputFormat"`
	// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri *string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList *QueryList `pulumi:"queryList"`
}

// PrestoJobInput is an input type that accepts PrestoJobArgs and PrestoJobOutput values.
// You can construct a concrete instance of `PrestoJobInput` via:
//
//          PrestoJobArgs{...}
type PrestoJobInput interface {
	pulumi.Input

	ToPrestoJobOutput() PrestoJobOutput
	ToPrestoJobOutputWithContext(context.Context) PrestoJobOutput
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJobArgs struct {
	// Optional. Presto client tags to attach to this query
	ClientTags pulumi.StringArrayInput `pulumi:"clientTags"`
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure pulumi.BoolPtrInput `pulumi:"continueOnFailure"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
	OutputFormat pulumi.StringPtrInput `pulumi:"outputFormat"`
	// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri pulumi.StringPtrInput `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListPtrInput `pulumi:"queryList"`
}

func (PrestoJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PrestoJob)(nil)).Elem()
}

func (i PrestoJobArgs) ToPrestoJobOutput() PrestoJobOutput {
	return i.ToPrestoJobOutputWithContext(context.Background())
}

func (i PrestoJobArgs) ToPrestoJobOutputWithContext(ctx context.Context) PrestoJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PrestoJobOutput)
}

func (i PrestoJobArgs) ToPrestoJobPtrOutput() PrestoJobPtrOutput {
	return i.ToPrestoJobPtrOutputWithContext(context.Background())
}

func (i PrestoJobArgs) ToPrestoJobPtrOutputWithContext(ctx context.Context) PrestoJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PrestoJobOutput).ToPrestoJobPtrOutputWithContext(ctx)
}

// PrestoJobPtrInput is an input type that accepts PrestoJobArgs, PrestoJobPtr and PrestoJobPtrOutput values.
// You can construct a concrete instance of `PrestoJobPtrInput` via:
//
//          PrestoJobArgs{...}
//
//  or:
//
//          nil
type PrestoJobPtrInput interface {
	pulumi.Input

	ToPrestoJobPtrOutput() PrestoJobPtrOutput
	ToPrestoJobPtrOutputWithContext(context.Context) PrestoJobPtrOutput
}

type prestoJobPtrType PrestoJobArgs

func PrestoJobPtr(v *PrestoJobArgs) PrestoJobPtrInput {
	return (*prestoJobPtrType)(v)
}

func (*prestoJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PrestoJob)(nil)).Elem()
}

func (i *prestoJobPtrType) ToPrestoJobPtrOutput() PrestoJobPtrOutput {
	return i.ToPrestoJobPtrOutputWithContext(context.Background())
}

func (i *prestoJobPtrType) ToPrestoJobPtrOutputWithContext(ctx context.Context) PrestoJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PrestoJobPtrOutput)
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJobOutput struct{ *pulumi.OutputState }

func (PrestoJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PrestoJob)(nil)).Elem()
}

func (o PrestoJobOutput) ToPrestoJobOutput() PrestoJobOutput {
	return o
}

func (o PrestoJobOutput) ToPrestoJobOutputWithContext(ctx context.Context) PrestoJobOutput {
	return o
}

func (o PrestoJobOutput) ToPrestoJobPtrOutput() PrestoJobPtrOutput {
	return o.ToPrestoJobPtrOutputWithContext(context.Background())
}

func (o PrestoJobOutput) ToPrestoJobPtrOutputWithContext(ctx context.Context) PrestoJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PrestoJob) *PrestoJob {
		return &v
	}).(PrestoJobPtrOutput)
}

// Optional. Presto client tags to attach to this query
func (o PrestoJobOutput) ClientTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PrestoJob) []string { return v.ClientTags }).(pulumi.StringArrayOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PrestoJobOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v PrestoJob) *bool { return v.ContinueOnFailure }).(pulumi.BoolPtrOutput)
}

// Optional. The runtime log config for job execution.
func (o PrestoJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v PrestoJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
func (o PrestoJobOutput) OutputFormat() pulumi.StringPtrOutput {
	return o.ApplyT(func(v PrestoJob) *string { return v.OutputFormat }).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
func (o PrestoJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PrestoJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o PrestoJobOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v PrestoJob) *string { return v.QueryFileUri }).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o PrestoJobOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v PrestoJob) *QueryList { return v.QueryList }).(QueryListPtrOutput)
}

type PrestoJobPtrOutput struct{ *pulumi.OutputState }

func (PrestoJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PrestoJob)(nil)).Elem()
}

func (o PrestoJobPtrOutput) ToPrestoJobPtrOutput() PrestoJobPtrOutput {
	return o
}

func (o PrestoJobPtrOutput) ToPrestoJobPtrOutputWithContext(ctx context.Context) PrestoJobPtrOutput {
	return o
}

func (o PrestoJobPtrOutput) Elem() PrestoJobOutput {
	return o.ApplyT(func(v *PrestoJob) PrestoJob {
		if v != nil {
			return *v
		}
		var ret PrestoJob
		return ret
	}).(PrestoJobOutput)
}

// Optional. Presto client tags to attach to this query
func (o PrestoJobPtrOutput) ClientTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PrestoJob) []string {
		if v == nil {
			return nil
		}
		return v.ClientTags
	}).(pulumi.StringArrayOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PrestoJobPtrOutput) ContinueOnFailure() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *bool {
		if v == nil {
			return nil
		}
		return v.ContinueOnFailure
	}).(pulumi.BoolPtrOutput)
}

// Optional. The runtime log config for job execution.
func (o PrestoJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
func (o PrestoJobPtrOutput) OutputFormat() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *string {
		if v == nil {
			return nil
		}
		return v.OutputFormat
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
func (o PrestoJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *PrestoJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o PrestoJobPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *string {
		if v == nil {
			return nil
		}
		return v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o PrestoJobPtrOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v *PrestoJob) *QueryList {
		if v == nil {
			return nil
		}
		return v.QueryList
	}).(QueryListPtrOutput)
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJobResponse struct {
	// Optional. Presto client tags to attach to this query
	ClientTags []string `pulumi:"clientTags"`
	// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
	ContinueOnFailure bool `pulumi:"continueOnFailure"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
	OutputFormat string `pulumi:"outputFormat"`
	// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListResponse `pulumi:"queryList"`
}

// A Dataproc job for running Presto (https://prestosql.io/) queries. IMPORTANT: The Dataproc Presto Optional Component (https://cloud.google.com/dataproc/docs/concepts/components/presto) must be enabled when the cluster is created to submit a Presto job to the cluster.
type PrestoJobResponseOutput struct{ *pulumi.OutputState }

func (PrestoJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PrestoJobResponse)(nil)).Elem()
}

func (o PrestoJobResponseOutput) ToPrestoJobResponseOutput() PrestoJobResponseOutput {
	return o
}

func (o PrestoJobResponseOutput) ToPrestoJobResponseOutputWithContext(ctx context.Context) PrestoJobResponseOutput {
	return o
}

// Optional. Presto client tags to attach to this query
func (o PrestoJobResponseOutput) ClientTags() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PrestoJobResponse) []string { return v.ClientTags }).(pulumi.StringArrayOutput)
}

// Optional. Whether to continue executing queries if a query fails. The default value is false. Setting to true can be useful when executing independent parallel queries.
func (o PrestoJobResponseOutput) ContinueOnFailure() pulumi.BoolOutput {
	return o.ApplyT(func(v PrestoJobResponse) bool { return v.ContinueOnFailure }).(pulumi.BoolOutput)
}

// Optional. The runtime log config for job execution.
func (o PrestoJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v PrestoJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// Optional. The format in which query output will be displayed. See the Presto documentation for supported output formats
func (o PrestoJobResponseOutput) OutputFormat() pulumi.StringOutput {
	return o.ApplyT(func(v PrestoJobResponse) string { return v.OutputFormat }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values. Used to set Presto session properties (https://prestodb.io/docs/current/sql/set-session.html) Equivalent to using the --session flag in the Presto CLI
func (o PrestoJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PrestoJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o PrestoJobResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PrestoJobResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// A list of queries.
func (o PrestoJobResponseOutput) QueryList() QueryListResponseOutput {
	return o.ApplyT(func(v PrestoJobResponse) QueryListResponse { return v.QueryList }).(QueryListResponseOutput)
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatch struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
	MainPythonFileUri string `pulumi:"mainPythonFileUri"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []string `pulumi:"pythonFileUris"`
}

// PySparkBatchInput is an input type that accepts PySparkBatchArgs and PySparkBatchOutput values.
// You can construct a concrete instance of `PySparkBatchInput` via:
//
//          PySparkBatchArgs{...}
type PySparkBatchInput interface {
	pulumi.Input

	ToPySparkBatchOutput() PySparkBatchOutput
	ToPySparkBatchOutputWithContext(context.Context) PySparkBatchOutput
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatchArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
	MainPythonFileUri pulumi.StringInput `pulumi:"mainPythonFileUri"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris pulumi.StringArrayInput `pulumi:"pythonFileUris"`
}

func (PySparkBatchArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkBatch)(nil)).Elem()
}

func (i PySparkBatchArgs) ToPySparkBatchOutput() PySparkBatchOutput {
	return i.ToPySparkBatchOutputWithContext(context.Background())
}

func (i PySparkBatchArgs) ToPySparkBatchOutputWithContext(ctx context.Context) PySparkBatchOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkBatchOutput)
}

func (i PySparkBatchArgs) ToPySparkBatchPtrOutput() PySparkBatchPtrOutput {
	return i.ToPySparkBatchPtrOutputWithContext(context.Background())
}

func (i PySparkBatchArgs) ToPySparkBatchPtrOutputWithContext(ctx context.Context) PySparkBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkBatchOutput).ToPySparkBatchPtrOutputWithContext(ctx)
}

// PySparkBatchPtrInput is an input type that accepts PySparkBatchArgs, PySparkBatchPtr and PySparkBatchPtrOutput values.
// You can construct a concrete instance of `PySparkBatchPtrInput` via:
//
//          PySparkBatchArgs{...}
//
//  or:
//
//          nil
type PySparkBatchPtrInput interface {
	pulumi.Input

	ToPySparkBatchPtrOutput() PySparkBatchPtrOutput
	ToPySparkBatchPtrOutputWithContext(context.Context) PySparkBatchPtrOutput
}

type pySparkBatchPtrType PySparkBatchArgs

func PySparkBatchPtr(v *PySparkBatchArgs) PySparkBatchPtrInput {
	return (*pySparkBatchPtrType)(v)
}

func (*pySparkBatchPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PySparkBatch)(nil)).Elem()
}

func (i *pySparkBatchPtrType) ToPySparkBatchPtrOutput() PySparkBatchPtrOutput {
	return i.ToPySparkBatchPtrOutputWithContext(context.Background())
}

func (i *pySparkBatchPtrType) ToPySparkBatchPtrOutputWithContext(ctx context.Context) PySparkBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkBatchPtrOutput)
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatchOutput struct{ *pulumi.OutputState }

func (PySparkBatchOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkBatch)(nil)).Elem()
}

func (o PySparkBatchOutput) ToPySparkBatchOutput() PySparkBatchOutput {
	return o
}

func (o PySparkBatchOutput) ToPySparkBatchOutputWithContext(ctx context.Context) PySparkBatchOutput {
	return o
}

func (o PySparkBatchOutput) ToPySparkBatchPtrOutput() PySparkBatchPtrOutput {
	return o.ToPySparkBatchPtrOutputWithContext(context.Background())
}

func (o PySparkBatchOutput) ToPySparkBatchPtrOutputWithContext(ctx context.Context) PySparkBatchPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PySparkBatch) *PySparkBatch {
		return &v
	}).(PySparkBatchPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkBatchOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o PySparkBatchOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o PySparkBatchOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o PySparkBatchOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
func (o PySparkBatchOutput) MainPythonFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PySparkBatch) string { return v.MainPythonFileUri }).(pulumi.StringOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkBatchOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatch) []string { return v.PythonFileUris }).(pulumi.StringArrayOutput)
}

type PySparkBatchPtrOutput struct{ *pulumi.OutputState }

func (PySparkBatchPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PySparkBatch)(nil)).Elem()
}

func (o PySparkBatchPtrOutput) ToPySparkBatchPtrOutput() PySparkBatchPtrOutput {
	return o
}

func (o PySparkBatchPtrOutput) ToPySparkBatchPtrOutputWithContext(ctx context.Context) PySparkBatchPtrOutput {
	return o
}

func (o PySparkBatchPtrOutput) Elem() PySparkBatchOutput {
	return o.ApplyT(func(v *PySparkBatch) PySparkBatch {
		if v != nil {
			return *v
		}
		var ret PySparkBatch
		return ret
	}).(PySparkBatchOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkBatchPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o PySparkBatchPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o PySparkBatchPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o PySparkBatchPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
func (o PySparkBatchPtrOutput) MainPythonFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PySparkBatch) *string {
		if v == nil {
			return nil
		}
		return &v.MainPythonFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkBatchPtrOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.PythonFileUris
	}).(pulumi.StringArrayOutput)
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatchResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
	MainPythonFileUri string `pulumi:"mainPythonFileUri"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []string `pulumi:"pythonFileUris"`
}

// A configuration for running an Apache PySpark (https://spark.apache.org/docs/latest/api/python/getting_started/quickstart.html) batch workload.
type PySparkBatchResponseOutput struct{ *pulumi.OutputState }

func (PySparkBatchResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkBatchResponse)(nil)).Elem()
}

func (o PySparkBatchResponseOutput) ToPySparkBatchResponseOutput() PySparkBatchResponseOutput {
	return o
}

func (o PySparkBatchResponseOutput) ToPySparkBatchResponseOutputWithContext(ctx context.Context) PySparkBatchResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkBatchResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o PySparkBatchResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o PySparkBatchResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o PySparkBatchResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main Python file to use as the Spark driver. Must be a .py file.
func (o PySparkBatchResponseOutput) MainPythonFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PySparkBatchResponse) string { return v.MainPythonFileUri }).(pulumi.StringOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkBatchResponseOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkBatchResponse) []string { return v.PythonFileUris }).(pulumi.StringArrayOutput)
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJob struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
	MainPythonFileUri string `pulumi:"mainPythonFileUri"`
	// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []string `pulumi:"pythonFileUris"`
}

// PySparkJobInput is an input type that accepts PySparkJobArgs and PySparkJobOutput values.
// You can construct a concrete instance of `PySparkJobInput` via:
//
//          PySparkJobArgs{...}
type PySparkJobInput interface {
	pulumi.Input

	ToPySparkJobOutput() PySparkJobOutput
	ToPySparkJobOutputWithContext(context.Context) PySparkJobOutput
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJobArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
	MainPythonFileUri pulumi.StringInput `pulumi:"mainPythonFileUri"`
	// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris pulumi.StringArrayInput `pulumi:"pythonFileUris"`
}

func (PySparkJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkJob)(nil)).Elem()
}

func (i PySparkJobArgs) ToPySparkJobOutput() PySparkJobOutput {
	return i.ToPySparkJobOutputWithContext(context.Background())
}

func (i PySparkJobArgs) ToPySparkJobOutputWithContext(ctx context.Context) PySparkJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkJobOutput)
}

func (i PySparkJobArgs) ToPySparkJobPtrOutput() PySparkJobPtrOutput {
	return i.ToPySparkJobPtrOutputWithContext(context.Background())
}

func (i PySparkJobArgs) ToPySparkJobPtrOutputWithContext(ctx context.Context) PySparkJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkJobOutput).ToPySparkJobPtrOutputWithContext(ctx)
}

// PySparkJobPtrInput is an input type that accepts PySparkJobArgs, PySparkJobPtr and PySparkJobPtrOutput values.
// You can construct a concrete instance of `PySparkJobPtrInput` via:
//
//          PySparkJobArgs{...}
//
//  or:
//
//          nil
type PySparkJobPtrInput interface {
	pulumi.Input

	ToPySparkJobPtrOutput() PySparkJobPtrOutput
	ToPySparkJobPtrOutputWithContext(context.Context) PySparkJobPtrOutput
}

type pySparkJobPtrType PySparkJobArgs

func PySparkJobPtr(v *PySparkJobArgs) PySparkJobPtrInput {
	return (*pySparkJobPtrType)(v)
}

func (*pySparkJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**PySparkJob)(nil)).Elem()
}

func (i *pySparkJobPtrType) ToPySparkJobPtrOutput() PySparkJobPtrOutput {
	return i.ToPySparkJobPtrOutputWithContext(context.Background())
}

func (i *pySparkJobPtrType) ToPySparkJobPtrOutputWithContext(ctx context.Context) PySparkJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(PySparkJobPtrOutput)
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJobOutput struct{ *pulumi.OutputState }

func (PySparkJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkJob)(nil)).Elem()
}

func (o PySparkJobOutput) ToPySparkJobOutput() PySparkJobOutput {
	return o
}

func (o PySparkJobOutput) ToPySparkJobOutputWithContext(ctx context.Context) PySparkJobOutput {
	return o
}

func (o PySparkJobOutput) ToPySparkJobPtrOutput() PySparkJobPtrOutput {
	return o.ToPySparkJobPtrOutputWithContext(context.Background())
}

func (o PySparkJobOutput) ToPySparkJobPtrOutputWithContext(ctx context.Context) PySparkJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v PySparkJob) *PySparkJob {
		return &v
	}).(PySparkJobPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkJobOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o PySparkJobOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o PySparkJobOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
func (o PySparkJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PySparkJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v PySparkJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
func (o PySparkJobOutput) MainPythonFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PySparkJob) string { return v.MainPythonFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o PySparkJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PySparkJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkJobOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJob) []string { return v.PythonFileUris }).(pulumi.StringArrayOutput)
}

type PySparkJobPtrOutput struct{ *pulumi.OutputState }

func (PySparkJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**PySparkJob)(nil)).Elem()
}

func (o PySparkJobPtrOutput) ToPySparkJobPtrOutput() PySparkJobPtrOutput {
	return o
}

func (o PySparkJobPtrOutput) ToPySparkJobPtrOutputWithContext(ctx context.Context) PySparkJobPtrOutput {
	return o
}

func (o PySparkJobPtrOutput) Elem() PySparkJobOutput {
	return o.ApplyT(func(v *PySparkJob) PySparkJob {
		if v != nil {
			return *v
		}
		var ret PySparkJob
		return ret
	}).(PySparkJobOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkJobPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o PySparkJobPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o PySparkJobPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
func (o PySparkJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PySparkJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *PySparkJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
func (o PySparkJobPtrOutput) MainPythonFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *PySparkJob) *string {
		if v == nil {
			return nil
		}
		return &v.MainPythonFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o PySparkJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *PySparkJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkJobPtrOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *PySparkJob) []string {
		if v == nil {
			return nil
		}
		return v.PythonFileUris
	}).(pulumi.StringArrayOutput)
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJobResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
	MainPythonFileUri string `pulumi:"mainPythonFileUri"`
	// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
	// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
	PythonFileUris []string `pulumi:"pythonFileUris"`
}

// A Dataproc job for running Apache PySpark (https://spark.apache.org/docs/0.9.0/python-programming-guide.html) applications on YARN.
type PySparkJobResponseOutput struct{ *pulumi.OutputState }

func (PySparkJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*PySparkJobResponse)(nil)).Elem()
}

func (o PySparkJobResponseOutput) ToPySparkJobResponseOutput() PySparkJobResponseOutput {
	return o
}

func (o PySparkJobResponseOutput) ToPySparkJobResponseOutputWithContext(ctx context.Context) PySparkJobResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o PySparkJobResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o PySparkJobResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o PySparkJobResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Python driver and tasks.
func (o PySparkJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o PySparkJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v PySparkJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// The HCFS URI of the main Python file to use as the driver. Must be a .py file.
func (o PySparkJobResponseOutput) MainPythonFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v PySparkJobResponse) string { return v.MainPythonFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure PySpark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o PySparkJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v PySparkJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// Optional. HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: .py, .egg, and .zip.
func (o PySparkJobResponseOutput) PythonFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v PySparkJobResponse) []string { return v.PythonFileUris }).(pulumi.StringArrayOutput)
}

// A list of queries to run on a cluster.
type QueryList struct {
	// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
	Queries []string `pulumi:"queries"`
}

// QueryListInput is an input type that accepts QueryListArgs and QueryListOutput values.
// You can construct a concrete instance of `QueryListInput` via:
//
//          QueryListArgs{...}
type QueryListInput interface {
	pulumi.Input

	ToQueryListOutput() QueryListOutput
	ToQueryListOutputWithContext(context.Context) QueryListOutput
}

// A list of queries to run on a cluster.
type QueryListArgs struct {
	// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
	Queries pulumi.StringArrayInput `pulumi:"queries"`
}

func (QueryListArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*QueryList)(nil)).Elem()
}

func (i QueryListArgs) ToQueryListOutput() QueryListOutput {
	return i.ToQueryListOutputWithContext(context.Background())
}

func (i QueryListArgs) ToQueryListOutputWithContext(ctx context.Context) QueryListOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueryListOutput)
}

func (i QueryListArgs) ToQueryListPtrOutput() QueryListPtrOutput {
	return i.ToQueryListPtrOutputWithContext(context.Background())
}

func (i QueryListArgs) ToQueryListPtrOutputWithContext(ctx context.Context) QueryListPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueryListOutput).ToQueryListPtrOutputWithContext(ctx)
}

// QueryListPtrInput is an input type that accepts QueryListArgs, QueryListPtr and QueryListPtrOutput values.
// You can construct a concrete instance of `QueryListPtrInput` via:
//
//          QueryListArgs{...}
//
//  or:
//
//          nil
type QueryListPtrInput interface {
	pulumi.Input

	ToQueryListPtrOutput() QueryListPtrOutput
	ToQueryListPtrOutputWithContext(context.Context) QueryListPtrOutput
}

type queryListPtrType QueryListArgs

func QueryListPtr(v *QueryListArgs) QueryListPtrInput {
	return (*queryListPtrType)(v)
}

func (*queryListPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**QueryList)(nil)).Elem()
}

func (i *queryListPtrType) ToQueryListPtrOutput() QueryListPtrOutput {
	return i.ToQueryListPtrOutputWithContext(context.Background())
}

func (i *queryListPtrType) ToQueryListPtrOutputWithContext(ctx context.Context) QueryListPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(QueryListPtrOutput)
}

// A list of queries to run on a cluster.
type QueryListOutput struct{ *pulumi.OutputState }

func (QueryListOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*QueryList)(nil)).Elem()
}

func (o QueryListOutput) ToQueryListOutput() QueryListOutput {
	return o
}

func (o QueryListOutput) ToQueryListOutputWithContext(ctx context.Context) QueryListOutput {
	return o
}

func (o QueryListOutput) ToQueryListPtrOutput() QueryListPtrOutput {
	return o.ToQueryListPtrOutputWithContext(context.Background())
}

func (o QueryListOutput) ToQueryListPtrOutputWithContext(ctx context.Context) QueryListPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v QueryList) *QueryList {
		return &v
	}).(QueryListPtrOutput)
}

// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
func (o QueryListOutput) Queries() pulumi.StringArrayOutput {
	return o.ApplyT(func(v QueryList) []string { return v.Queries }).(pulumi.StringArrayOutput)
}

type QueryListPtrOutput struct{ *pulumi.OutputState }

func (QueryListPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**QueryList)(nil)).Elem()
}

func (o QueryListPtrOutput) ToQueryListPtrOutput() QueryListPtrOutput {
	return o
}

func (o QueryListPtrOutput) ToQueryListPtrOutputWithContext(ctx context.Context) QueryListPtrOutput {
	return o
}

func (o QueryListPtrOutput) Elem() QueryListOutput {
	return o.ApplyT(func(v *QueryList) QueryList {
		if v != nil {
			return *v
		}
		var ret QueryList
		return ret
	}).(QueryListOutput)
}

// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
func (o QueryListPtrOutput) Queries() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *QueryList) []string {
		if v == nil {
			return nil
		}
		return v.Queries
	}).(pulumi.StringArrayOutput)
}

// A list of queries to run on a cluster.
type QueryListResponse struct {
	// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
	Queries []string `pulumi:"queries"`
}

// A list of queries to run on a cluster.
type QueryListResponseOutput struct{ *pulumi.OutputState }

func (QueryListResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*QueryListResponse)(nil)).Elem()
}

func (o QueryListResponseOutput) ToQueryListResponseOutput() QueryListResponseOutput {
	return o
}

func (o QueryListResponseOutput) ToQueryListResponseOutputWithContext(ctx context.Context) QueryListResponseOutput {
	return o
}

// The queries to execute. You do not need to end a query expression with a semicolon. Multiple queries can be specified in one string by separating each with a semicolon. Here is an example of a Dataproc API snippet that uses a QueryList to specify a HiveJob: "hiveJob": { "queryList": { "queries": [ "query1", "query2", "query3;query4", ] } }
func (o QueryListResponseOutput) Queries() pulumi.StringArrayOutput {
	return o.ApplyT(func(v QueryListResponse) []string { return v.Queries }).(pulumi.StringArrayOutput)
}

// Validation based on regular expressions.
type RegexValidation struct {
	// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
	Regexes []string `pulumi:"regexes"`
}

// RegexValidationInput is an input type that accepts RegexValidationArgs and RegexValidationOutput values.
// You can construct a concrete instance of `RegexValidationInput` via:
//
//          RegexValidationArgs{...}
type RegexValidationInput interface {
	pulumi.Input

	ToRegexValidationOutput() RegexValidationOutput
	ToRegexValidationOutputWithContext(context.Context) RegexValidationOutput
}

// Validation based on regular expressions.
type RegexValidationArgs struct {
	// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
	Regexes pulumi.StringArrayInput `pulumi:"regexes"`
}

func (RegexValidationArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*RegexValidation)(nil)).Elem()
}

func (i RegexValidationArgs) ToRegexValidationOutput() RegexValidationOutput {
	return i.ToRegexValidationOutputWithContext(context.Background())
}

func (i RegexValidationArgs) ToRegexValidationOutputWithContext(ctx context.Context) RegexValidationOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RegexValidationOutput)
}

func (i RegexValidationArgs) ToRegexValidationPtrOutput() RegexValidationPtrOutput {
	return i.ToRegexValidationPtrOutputWithContext(context.Background())
}

func (i RegexValidationArgs) ToRegexValidationPtrOutputWithContext(ctx context.Context) RegexValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RegexValidationOutput).ToRegexValidationPtrOutputWithContext(ctx)
}

// RegexValidationPtrInput is an input type that accepts RegexValidationArgs, RegexValidationPtr and RegexValidationPtrOutput values.
// You can construct a concrete instance of `RegexValidationPtrInput` via:
//
//          RegexValidationArgs{...}
//
//  or:
//
//          nil
type RegexValidationPtrInput interface {
	pulumi.Input

	ToRegexValidationPtrOutput() RegexValidationPtrOutput
	ToRegexValidationPtrOutputWithContext(context.Context) RegexValidationPtrOutput
}

type regexValidationPtrType RegexValidationArgs

func RegexValidationPtr(v *RegexValidationArgs) RegexValidationPtrInput {
	return (*regexValidationPtrType)(v)
}

func (*regexValidationPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**RegexValidation)(nil)).Elem()
}

func (i *regexValidationPtrType) ToRegexValidationPtrOutput() RegexValidationPtrOutput {
	return i.ToRegexValidationPtrOutputWithContext(context.Background())
}

func (i *regexValidationPtrType) ToRegexValidationPtrOutputWithContext(ctx context.Context) RegexValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RegexValidationPtrOutput)
}

// Validation based on regular expressions.
type RegexValidationOutput struct{ *pulumi.OutputState }

func (RegexValidationOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RegexValidation)(nil)).Elem()
}

func (o RegexValidationOutput) ToRegexValidationOutput() RegexValidationOutput {
	return o
}

func (o RegexValidationOutput) ToRegexValidationOutputWithContext(ctx context.Context) RegexValidationOutput {
	return o
}

func (o RegexValidationOutput) ToRegexValidationPtrOutput() RegexValidationPtrOutput {
	return o.ToRegexValidationPtrOutputWithContext(context.Background())
}

func (o RegexValidationOutput) ToRegexValidationPtrOutputWithContext(ctx context.Context) RegexValidationPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v RegexValidation) *RegexValidation {
		return &v
	}).(RegexValidationPtrOutput)
}

// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
func (o RegexValidationOutput) Regexes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v RegexValidation) []string { return v.Regexes }).(pulumi.StringArrayOutput)
}

type RegexValidationPtrOutput struct{ *pulumi.OutputState }

func (RegexValidationPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**RegexValidation)(nil)).Elem()
}

func (o RegexValidationPtrOutput) ToRegexValidationPtrOutput() RegexValidationPtrOutput {
	return o
}

func (o RegexValidationPtrOutput) ToRegexValidationPtrOutputWithContext(ctx context.Context) RegexValidationPtrOutput {
	return o
}

func (o RegexValidationPtrOutput) Elem() RegexValidationOutput {
	return o.ApplyT(func(v *RegexValidation) RegexValidation {
		if v != nil {
			return *v
		}
		var ret RegexValidation
		return ret
	}).(RegexValidationOutput)
}

// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
func (o RegexValidationPtrOutput) Regexes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *RegexValidation) []string {
		if v == nil {
			return nil
		}
		return v.Regexes
	}).(pulumi.StringArrayOutput)
}

// Validation based on regular expressions.
type RegexValidationResponse struct {
	// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
	Regexes []string `pulumi:"regexes"`
}

// Validation based on regular expressions.
type RegexValidationResponseOutput struct{ *pulumi.OutputState }

func (RegexValidationResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RegexValidationResponse)(nil)).Elem()
}

func (o RegexValidationResponseOutput) ToRegexValidationResponseOutput() RegexValidationResponseOutput {
	return o
}

func (o RegexValidationResponseOutput) ToRegexValidationResponseOutputWithContext(ctx context.Context) RegexValidationResponseOutput {
	return o
}

// RE2 regular expressions used to validate the parameter's value. The value must match the regex in its entirety (substring matches are not sufficient).
func (o RegexValidationResponseOutput) Regexes() pulumi.StringArrayOutput {
	return o.ApplyT(func(v RegexValidationResponse) []string { return v.Regexes }).(pulumi.StringArrayOutput)
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinity struct {
	// Optional. Type of reservation to consume
	ConsumeReservationType *ReservationAffinityConsumeReservationType `pulumi:"consumeReservationType"`
	// Optional. Corresponds to the label key of reservation resource.
	Key *string `pulumi:"key"`
	// Optional. Corresponds to the label values of reservation resource.
	Values []string `pulumi:"values"`
}

// ReservationAffinityInput is an input type that accepts ReservationAffinityArgs and ReservationAffinityOutput values.
// You can construct a concrete instance of `ReservationAffinityInput` via:
//
//          ReservationAffinityArgs{...}
type ReservationAffinityInput interface {
	pulumi.Input

	ToReservationAffinityOutput() ReservationAffinityOutput
	ToReservationAffinityOutputWithContext(context.Context) ReservationAffinityOutput
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinityArgs struct {
	// Optional. Type of reservation to consume
	ConsumeReservationType ReservationAffinityConsumeReservationTypePtrInput `pulumi:"consumeReservationType"`
	// Optional. Corresponds to the label key of reservation resource.
	Key pulumi.StringPtrInput `pulumi:"key"`
	// Optional. Corresponds to the label values of reservation resource.
	Values pulumi.StringArrayInput `pulumi:"values"`
}

func (ReservationAffinityArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ReservationAffinity)(nil)).Elem()
}

func (i ReservationAffinityArgs) ToReservationAffinityOutput() ReservationAffinityOutput {
	return i.ToReservationAffinityOutputWithContext(context.Background())
}

func (i ReservationAffinityArgs) ToReservationAffinityOutputWithContext(ctx context.Context) ReservationAffinityOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ReservationAffinityOutput)
}

func (i ReservationAffinityArgs) ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput {
	return i.ToReservationAffinityPtrOutputWithContext(context.Background())
}

func (i ReservationAffinityArgs) ToReservationAffinityPtrOutputWithContext(ctx context.Context) ReservationAffinityPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ReservationAffinityOutput).ToReservationAffinityPtrOutputWithContext(ctx)
}

// ReservationAffinityPtrInput is an input type that accepts ReservationAffinityArgs, ReservationAffinityPtr and ReservationAffinityPtrOutput values.
// You can construct a concrete instance of `ReservationAffinityPtrInput` via:
//
//          ReservationAffinityArgs{...}
//
//  or:
//
//          nil
type ReservationAffinityPtrInput interface {
	pulumi.Input

	ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput
	ToReservationAffinityPtrOutputWithContext(context.Context) ReservationAffinityPtrOutput
}

type reservationAffinityPtrType ReservationAffinityArgs

func ReservationAffinityPtr(v *ReservationAffinityArgs) ReservationAffinityPtrInput {
	return (*reservationAffinityPtrType)(v)
}

func (*reservationAffinityPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ReservationAffinity)(nil)).Elem()
}

func (i *reservationAffinityPtrType) ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput {
	return i.ToReservationAffinityPtrOutputWithContext(context.Background())
}

func (i *reservationAffinityPtrType) ToReservationAffinityPtrOutputWithContext(ctx context.Context) ReservationAffinityPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ReservationAffinityPtrOutput)
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinityOutput struct{ *pulumi.OutputState }

func (ReservationAffinityOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ReservationAffinity)(nil)).Elem()
}

func (o ReservationAffinityOutput) ToReservationAffinityOutput() ReservationAffinityOutput {
	return o
}

func (o ReservationAffinityOutput) ToReservationAffinityOutputWithContext(ctx context.Context) ReservationAffinityOutput {
	return o
}

func (o ReservationAffinityOutput) ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput {
	return o.ToReservationAffinityPtrOutputWithContext(context.Background())
}

func (o ReservationAffinityOutput) ToReservationAffinityPtrOutputWithContext(ctx context.Context) ReservationAffinityPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ReservationAffinity) *ReservationAffinity {
		return &v
	}).(ReservationAffinityPtrOutput)
}

// Optional. Type of reservation to consume
func (o ReservationAffinityOutput) ConsumeReservationType() ReservationAffinityConsumeReservationTypePtrOutput {
	return o.ApplyT(func(v ReservationAffinity) *ReservationAffinityConsumeReservationType {
		return v.ConsumeReservationType
	}).(ReservationAffinityConsumeReservationTypePtrOutput)
}

// Optional. Corresponds to the label key of reservation resource.
func (o ReservationAffinityOutput) Key() pulumi.StringPtrOutput {
	return o.ApplyT(func(v ReservationAffinity) *string { return v.Key }).(pulumi.StringPtrOutput)
}

// Optional. Corresponds to the label values of reservation resource.
func (o ReservationAffinityOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ReservationAffinity) []string { return v.Values }).(pulumi.StringArrayOutput)
}

type ReservationAffinityPtrOutput struct{ *pulumi.OutputState }

func (ReservationAffinityPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ReservationAffinity)(nil)).Elem()
}

func (o ReservationAffinityPtrOutput) ToReservationAffinityPtrOutput() ReservationAffinityPtrOutput {
	return o
}

func (o ReservationAffinityPtrOutput) ToReservationAffinityPtrOutputWithContext(ctx context.Context) ReservationAffinityPtrOutput {
	return o
}

func (o ReservationAffinityPtrOutput) Elem() ReservationAffinityOutput {
	return o.ApplyT(func(v *ReservationAffinity) ReservationAffinity {
		if v != nil {
			return *v
		}
		var ret ReservationAffinity
		return ret
	}).(ReservationAffinityOutput)
}

// Optional. Type of reservation to consume
func (o ReservationAffinityPtrOutput) ConsumeReservationType() ReservationAffinityConsumeReservationTypePtrOutput {
	return o.ApplyT(func(v *ReservationAffinity) *ReservationAffinityConsumeReservationType {
		if v == nil {
			return nil
		}
		return v.ConsumeReservationType
	}).(ReservationAffinityConsumeReservationTypePtrOutput)
}

// Optional. Corresponds to the label key of reservation resource.
func (o ReservationAffinityPtrOutput) Key() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *ReservationAffinity) *string {
		if v == nil {
			return nil
		}
		return v.Key
	}).(pulumi.StringPtrOutput)
}

// Optional. Corresponds to the label values of reservation resource.
func (o ReservationAffinityPtrOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *ReservationAffinity) []string {
		if v == nil {
			return nil
		}
		return v.Values
	}).(pulumi.StringArrayOutput)
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinityResponse struct {
	// Optional. Type of reservation to consume
	ConsumeReservationType string `pulumi:"consumeReservationType"`
	// Optional. Corresponds to the label key of reservation resource.
	Key string `pulumi:"key"`
	// Optional. Corresponds to the label values of reservation resource.
	Values []string `pulumi:"values"`
}

// Reservation Affinity for consuming Zonal reservation.
type ReservationAffinityResponseOutput struct{ *pulumi.OutputState }

func (ReservationAffinityResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ReservationAffinityResponse)(nil)).Elem()
}

func (o ReservationAffinityResponseOutput) ToReservationAffinityResponseOutput() ReservationAffinityResponseOutput {
	return o
}

func (o ReservationAffinityResponseOutput) ToReservationAffinityResponseOutputWithContext(ctx context.Context) ReservationAffinityResponseOutput {
	return o
}

// Optional. Type of reservation to consume
func (o ReservationAffinityResponseOutput) ConsumeReservationType() pulumi.StringOutput {
	return o.ApplyT(func(v ReservationAffinityResponse) string { return v.ConsumeReservationType }).(pulumi.StringOutput)
}

// Optional. Corresponds to the label key of reservation resource.
func (o ReservationAffinityResponseOutput) Key() pulumi.StringOutput {
	return o.ApplyT(func(v ReservationAffinityResponse) string { return v.Key }).(pulumi.StringOutput)
}

// Optional. Corresponds to the label values of reservation resource.
func (o ReservationAffinityResponseOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ReservationAffinityResponse) []string { return v.Values }).(pulumi.StringArrayOutput)
}

// Runtime configuration for a workload.
type RuntimeConfig struct {
	// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
	ContainerImage *string `pulumi:"containerImage"`
	// Optional. A mapping of property names to values, which are used to configure workload execution.
	Properties map[string]string `pulumi:"properties"`
	// Optional. Version of the batch runtime.
	Version *string `pulumi:"version"`
}

// RuntimeConfigInput is an input type that accepts RuntimeConfigArgs and RuntimeConfigOutput values.
// You can construct a concrete instance of `RuntimeConfigInput` via:
//
//          RuntimeConfigArgs{...}
type RuntimeConfigInput interface {
	pulumi.Input

	ToRuntimeConfigOutput() RuntimeConfigOutput
	ToRuntimeConfigOutputWithContext(context.Context) RuntimeConfigOutput
}

// Runtime configuration for a workload.
type RuntimeConfigArgs struct {
	// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
	ContainerImage pulumi.StringPtrInput `pulumi:"containerImage"`
	// Optional. A mapping of property names to values, which are used to configure workload execution.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// Optional. Version of the batch runtime.
	Version pulumi.StringPtrInput `pulumi:"version"`
}

func (RuntimeConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*RuntimeConfig)(nil)).Elem()
}

func (i RuntimeConfigArgs) ToRuntimeConfigOutput() RuntimeConfigOutput {
	return i.ToRuntimeConfigOutputWithContext(context.Background())
}

func (i RuntimeConfigArgs) ToRuntimeConfigOutputWithContext(ctx context.Context) RuntimeConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RuntimeConfigOutput)
}

func (i RuntimeConfigArgs) ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput {
	return i.ToRuntimeConfigPtrOutputWithContext(context.Background())
}

func (i RuntimeConfigArgs) ToRuntimeConfigPtrOutputWithContext(ctx context.Context) RuntimeConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RuntimeConfigOutput).ToRuntimeConfigPtrOutputWithContext(ctx)
}

// RuntimeConfigPtrInput is an input type that accepts RuntimeConfigArgs, RuntimeConfigPtr and RuntimeConfigPtrOutput values.
// You can construct a concrete instance of `RuntimeConfigPtrInput` via:
//
//          RuntimeConfigArgs{...}
//
//  or:
//
//          nil
type RuntimeConfigPtrInput interface {
	pulumi.Input

	ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput
	ToRuntimeConfigPtrOutputWithContext(context.Context) RuntimeConfigPtrOutput
}

type runtimeConfigPtrType RuntimeConfigArgs

func RuntimeConfigPtr(v *RuntimeConfigArgs) RuntimeConfigPtrInput {
	return (*runtimeConfigPtrType)(v)
}

func (*runtimeConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**RuntimeConfig)(nil)).Elem()
}

func (i *runtimeConfigPtrType) ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput {
	return i.ToRuntimeConfigPtrOutputWithContext(context.Background())
}

func (i *runtimeConfigPtrType) ToRuntimeConfigPtrOutputWithContext(ctx context.Context) RuntimeConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(RuntimeConfigPtrOutput)
}

// Runtime configuration for a workload.
type RuntimeConfigOutput struct{ *pulumi.OutputState }

func (RuntimeConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RuntimeConfig)(nil)).Elem()
}

func (o RuntimeConfigOutput) ToRuntimeConfigOutput() RuntimeConfigOutput {
	return o
}

func (o RuntimeConfigOutput) ToRuntimeConfigOutputWithContext(ctx context.Context) RuntimeConfigOutput {
	return o
}

func (o RuntimeConfigOutput) ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput {
	return o.ToRuntimeConfigPtrOutputWithContext(context.Background())
}

func (o RuntimeConfigOutput) ToRuntimeConfigPtrOutputWithContext(ctx context.Context) RuntimeConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v RuntimeConfig) *RuntimeConfig {
		return &v
	}).(RuntimeConfigPtrOutput)
}

// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
func (o RuntimeConfigOutput) ContainerImage() pulumi.StringPtrOutput {
	return o.ApplyT(func(v RuntimeConfig) *string { return v.ContainerImage }).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, which are used to configure workload execution.
func (o RuntimeConfigOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v RuntimeConfig) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// Optional. Version of the batch runtime.
func (o RuntimeConfigOutput) Version() pulumi.StringPtrOutput {
	return o.ApplyT(func(v RuntimeConfig) *string { return v.Version }).(pulumi.StringPtrOutput)
}

type RuntimeConfigPtrOutput struct{ *pulumi.OutputState }

func (RuntimeConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**RuntimeConfig)(nil)).Elem()
}

func (o RuntimeConfigPtrOutput) ToRuntimeConfigPtrOutput() RuntimeConfigPtrOutput {
	return o
}

func (o RuntimeConfigPtrOutput) ToRuntimeConfigPtrOutputWithContext(ctx context.Context) RuntimeConfigPtrOutput {
	return o
}

func (o RuntimeConfigPtrOutput) Elem() RuntimeConfigOutput {
	return o.ApplyT(func(v *RuntimeConfig) RuntimeConfig {
		if v != nil {
			return *v
		}
		var ret RuntimeConfig
		return ret
	}).(RuntimeConfigOutput)
}

// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
func (o RuntimeConfigPtrOutput) ContainerImage() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *RuntimeConfig) *string {
		if v == nil {
			return nil
		}
		return v.ContainerImage
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, which are used to configure workload execution.
func (o RuntimeConfigPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *RuntimeConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// Optional. Version of the batch runtime.
func (o RuntimeConfigPtrOutput) Version() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *RuntimeConfig) *string {
		if v == nil {
			return nil
		}
		return v.Version
	}).(pulumi.StringPtrOutput)
}

// Runtime configuration for a workload.
type RuntimeConfigResponse struct {
	// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
	ContainerImage string `pulumi:"containerImage"`
	// Optional. A mapping of property names to values, which are used to configure workload execution.
	Properties map[string]string `pulumi:"properties"`
	// Optional. Version of the batch runtime.
	Version string `pulumi:"version"`
}

// Runtime configuration for a workload.
type RuntimeConfigResponseOutput struct{ *pulumi.OutputState }

func (RuntimeConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RuntimeConfigResponse)(nil)).Elem()
}

func (o RuntimeConfigResponseOutput) ToRuntimeConfigResponseOutput() RuntimeConfigResponseOutput {
	return o
}

func (o RuntimeConfigResponseOutput) ToRuntimeConfigResponseOutputWithContext(ctx context.Context) RuntimeConfigResponseOutput {
	return o
}

// Optional. Optional custom container image for the job runtime environment. If not specified, a default container image will be used.
func (o RuntimeConfigResponseOutput) ContainerImage() pulumi.StringOutput {
	return o.ApplyT(func(v RuntimeConfigResponse) string { return v.ContainerImage }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, which are used to configure workload execution.
func (o RuntimeConfigResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v RuntimeConfigResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// Optional. Version of the batch runtime.
func (o RuntimeConfigResponseOutput) Version() pulumi.StringOutput {
	return o.ApplyT(func(v RuntimeConfigResponse) string { return v.Version }).(pulumi.StringOutput)
}

// Runtime information about workload execution.
type RuntimeInfoResponse struct {
	// A URI pointing to the location of the diagnostics tarball.
	DiagnosticOutputUri string `pulumi:"diagnosticOutputUri"`
	// Map of remote access endpoints (such as web interfaces and APIs) to their URIs.
	Endpoints map[string]string `pulumi:"endpoints"`
	// A URI pointing to the location of the stdout and stderr of the workload.
	OutputUri string `pulumi:"outputUri"`
}

// Runtime information about workload execution.
type RuntimeInfoResponseOutput struct{ *pulumi.OutputState }

func (RuntimeInfoResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*RuntimeInfoResponse)(nil)).Elem()
}

func (o RuntimeInfoResponseOutput) ToRuntimeInfoResponseOutput() RuntimeInfoResponseOutput {
	return o
}

func (o RuntimeInfoResponseOutput) ToRuntimeInfoResponseOutputWithContext(ctx context.Context) RuntimeInfoResponseOutput {
	return o
}

// A URI pointing to the location of the diagnostics tarball.
func (o RuntimeInfoResponseOutput) DiagnosticOutputUri() pulumi.StringOutput {
	return o.ApplyT(func(v RuntimeInfoResponse) string { return v.DiagnosticOutputUri }).(pulumi.StringOutput)
}

// Map of remote access endpoints (such as web interfaces and APIs) to their URIs.
func (o RuntimeInfoResponseOutput) Endpoints() pulumi.StringMapOutput {
	return o.ApplyT(func(v RuntimeInfoResponse) map[string]string { return v.Endpoints }).(pulumi.StringMapOutput)
}

// A URI pointing to the location of the stdout and stderr of the workload.
func (o RuntimeInfoResponseOutput) OutputUri() pulumi.StringOutput {
	return o.ApplyT(func(v RuntimeInfoResponse) string { return v.OutputUri }).(pulumi.StringOutput)
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfig struct {
	// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
	IdentityConfig *IdentityConfig `pulumi:"identityConfig"`
	// Optional. Kerberos related configuration.
	KerberosConfig *KerberosConfig `pulumi:"kerberosConfig"`
}

// SecurityConfigInput is an input type that accepts SecurityConfigArgs and SecurityConfigOutput values.
// You can construct a concrete instance of `SecurityConfigInput` via:
//
//          SecurityConfigArgs{...}
type SecurityConfigInput interface {
	pulumi.Input

	ToSecurityConfigOutput() SecurityConfigOutput
	ToSecurityConfigOutputWithContext(context.Context) SecurityConfigOutput
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfigArgs struct {
	// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
	IdentityConfig IdentityConfigPtrInput `pulumi:"identityConfig"`
	// Optional. Kerberos related configuration.
	KerberosConfig KerberosConfigPtrInput `pulumi:"kerberosConfig"`
}

func (SecurityConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SecurityConfig)(nil)).Elem()
}

func (i SecurityConfigArgs) ToSecurityConfigOutput() SecurityConfigOutput {
	return i.ToSecurityConfigOutputWithContext(context.Background())
}

func (i SecurityConfigArgs) ToSecurityConfigOutputWithContext(ctx context.Context) SecurityConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SecurityConfigOutput)
}

func (i SecurityConfigArgs) ToSecurityConfigPtrOutput() SecurityConfigPtrOutput {
	return i.ToSecurityConfigPtrOutputWithContext(context.Background())
}

func (i SecurityConfigArgs) ToSecurityConfigPtrOutputWithContext(ctx context.Context) SecurityConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SecurityConfigOutput).ToSecurityConfigPtrOutputWithContext(ctx)
}

// SecurityConfigPtrInput is an input type that accepts SecurityConfigArgs, SecurityConfigPtr and SecurityConfigPtrOutput values.
// You can construct a concrete instance of `SecurityConfigPtrInput` via:
//
//          SecurityConfigArgs{...}
//
//  or:
//
//          nil
type SecurityConfigPtrInput interface {
	pulumi.Input

	ToSecurityConfigPtrOutput() SecurityConfigPtrOutput
	ToSecurityConfigPtrOutputWithContext(context.Context) SecurityConfigPtrOutput
}

type securityConfigPtrType SecurityConfigArgs

func SecurityConfigPtr(v *SecurityConfigArgs) SecurityConfigPtrInput {
	return (*securityConfigPtrType)(v)
}

func (*securityConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SecurityConfig)(nil)).Elem()
}

func (i *securityConfigPtrType) ToSecurityConfigPtrOutput() SecurityConfigPtrOutput {
	return i.ToSecurityConfigPtrOutputWithContext(context.Background())
}

func (i *securityConfigPtrType) ToSecurityConfigPtrOutputWithContext(ctx context.Context) SecurityConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SecurityConfigPtrOutput)
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfigOutput struct{ *pulumi.OutputState }

func (SecurityConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SecurityConfig)(nil)).Elem()
}

func (o SecurityConfigOutput) ToSecurityConfigOutput() SecurityConfigOutput {
	return o
}

func (o SecurityConfigOutput) ToSecurityConfigOutputWithContext(ctx context.Context) SecurityConfigOutput {
	return o
}

func (o SecurityConfigOutput) ToSecurityConfigPtrOutput() SecurityConfigPtrOutput {
	return o.ToSecurityConfigPtrOutputWithContext(context.Background())
}

func (o SecurityConfigOutput) ToSecurityConfigPtrOutputWithContext(ctx context.Context) SecurityConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SecurityConfig) *SecurityConfig {
		return &v
	}).(SecurityConfigPtrOutput)
}

// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
func (o SecurityConfigOutput) IdentityConfig() IdentityConfigPtrOutput {
	return o.ApplyT(func(v SecurityConfig) *IdentityConfig { return v.IdentityConfig }).(IdentityConfigPtrOutput)
}

// Optional. Kerberos related configuration.
func (o SecurityConfigOutput) KerberosConfig() KerberosConfigPtrOutput {
	return o.ApplyT(func(v SecurityConfig) *KerberosConfig { return v.KerberosConfig }).(KerberosConfigPtrOutput)
}

type SecurityConfigPtrOutput struct{ *pulumi.OutputState }

func (SecurityConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SecurityConfig)(nil)).Elem()
}

func (o SecurityConfigPtrOutput) ToSecurityConfigPtrOutput() SecurityConfigPtrOutput {
	return o
}

func (o SecurityConfigPtrOutput) ToSecurityConfigPtrOutputWithContext(ctx context.Context) SecurityConfigPtrOutput {
	return o
}

func (o SecurityConfigPtrOutput) Elem() SecurityConfigOutput {
	return o.ApplyT(func(v *SecurityConfig) SecurityConfig {
		if v != nil {
			return *v
		}
		var ret SecurityConfig
		return ret
	}).(SecurityConfigOutput)
}

// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
func (o SecurityConfigPtrOutput) IdentityConfig() IdentityConfigPtrOutput {
	return o.ApplyT(func(v *SecurityConfig) *IdentityConfig {
		if v == nil {
			return nil
		}
		return v.IdentityConfig
	}).(IdentityConfigPtrOutput)
}

// Optional. Kerberos related configuration.
func (o SecurityConfigPtrOutput) KerberosConfig() KerberosConfigPtrOutput {
	return o.ApplyT(func(v *SecurityConfig) *KerberosConfig {
		if v == nil {
			return nil
		}
		return v.KerberosConfig
	}).(KerberosConfigPtrOutput)
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfigResponse struct {
	// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
	IdentityConfig IdentityConfigResponse `pulumi:"identityConfig"`
	// Optional. Kerberos related configuration.
	KerberosConfig KerberosConfigResponse `pulumi:"kerberosConfig"`
}

// Security related configuration, including encryption, Kerberos, etc.
type SecurityConfigResponseOutput struct{ *pulumi.OutputState }

func (SecurityConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SecurityConfigResponse)(nil)).Elem()
}

func (o SecurityConfigResponseOutput) ToSecurityConfigResponseOutput() SecurityConfigResponseOutput {
	return o
}

func (o SecurityConfigResponseOutput) ToSecurityConfigResponseOutputWithContext(ctx context.Context) SecurityConfigResponseOutput {
	return o
}

// Optional. Identity related configuration, including service account based secure multi-tenancy user mappings.
func (o SecurityConfigResponseOutput) IdentityConfig() IdentityConfigResponseOutput {
	return o.ApplyT(func(v SecurityConfigResponse) IdentityConfigResponse { return v.IdentityConfig }).(IdentityConfigResponseOutput)
}

// Optional. Kerberos related configuration.
func (o SecurityConfigResponseOutput) KerberosConfig() KerberosConfigResponseOutput {
	return o.ApplyT(func(v SecurityConfigResponse) KerberosConfigResponse { return v.KerberosConfig }).(KerberosConfigResponseOutput)
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfig struct {
	// Optional. Defines whether instances have integrity monitoring enabled.
	EnableIntegrityMonitoring *bool `pulumi:"enableIntegrityMonitoring"`
	// Optional. Defines whether instances have Secure Boot enabled.
	EnableSecureBoot *bool `pulumi:"enableSecureBoot"`
	// Optional. Defines whether instances have the vTPM enabled.
	EnableVtpm *bool `pulumi:"enableVtpm"`
}

// ShieldedInstanceConfigInput is an input type that accepts ShieldedInstanceConfigArgs and ShieldedInstanceConfigOutput values.
// You can construct a concrete instance of `ShieldedInstanceConfigInput` via:
//
//          ShieldedInstanceConfigArgs{...}
type ShieldedInstanceConfigInput interface {
	pulumi.Input

	ToShieldedInstanceConfigOutput() ShieldedInstanceConfigOutput
	ToShieldedInstanceConfigOutputWithContext(context.Context) ShieldedInstanceConfigOutput
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfigArgs struct {
	// Optional. Defines whether instances have integrity monitoring enabled.
	EnableIntegrityMonitoring pulumi.BoolPtrInput `pulumi:"enableIntegrityMonitoring"`
	// Optional. Defines whether instances have Secure Boot enabled.
	EnableSecureBoot pulumi.BoolPtrInput `pulumi:"enableSecureBoot"`
	// Optional. Defines whether instances have the vTPM enabled.
	EnableVtpm pulumi.BoolPtrInput `pulumi:"enableVtpm"`
}

func (ShieldedInstanceConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ShieldedInstanceConfig)(nil)).Elem()
}

func (i ShieldedInstanceConfigArgs) ToShieldedInstanceConfigOutput() ShieldedInstanceConfigOutput {
	return i.ToShieldedInstanceConfigOutputWithContext(context.Background())
}

func (i ShieldedInstanceConfigArgs) ToShieldedInstanceConfigOutputWithContext(ctx context.Context) ShieldedInstanceConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ShieldedInstanceConfigOutput)
}

func (i ShieldedInstanceConfigArgs) ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput {
	return i.ToShieldedInstanceConfigPtrOutputWithContext(context.Background())
}

func (i ShieldedInstanceConfigArgs) ToShieldedInstanceConfigPtrOutputWithContext(ctx context.Context) ShieldedInstanceConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ShieldedInstanceConfigOutput).ToShieldedInstanceConfigPtrOutputWithContext(ctx)
}

// ShieldedInstanceConfigPtrInput is an input type that accepts ShieldedInstanceConfigArgs, ShieldedInstanceConfigPtr and ShieldedInstanceConfigPtrOutput values.
// You can construct a concrete instance of `ShieldedInstanceConfigPtrInput` via:
//
//          ShieldedInstanceConfigArgs{...}
//
//  or:
//
//          nil
type ShieldedInstanceConfigPtrInput interface {
	pulumi.Input

	ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput
	ToShieldedInstanceConfigPtrOutputWithContext(context.Context) ShieldedInstanceConfigPtrOutput
}

type shieldedInstanceConfigPtrType ShieldedInstanceConfigArgs

func ShieldedInstanceConfigPtr(v *ShieldedInstanceConfigArgs) ShieldedInstanceConfigPtrInput {
	return (*shieldedInstanceConfigPtrType)(v)
}

func (*shieldedInstanceConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ShieldedInstanceConfig)(nil)).Elem()
}

func (i *shieldedInstanceConfigPtrType) ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput {
	return i.ToShieldedInstanceConfigPtrOutputWithContext(context.Background())
}

func (i *shieldedInstanceConfigPtrType) ToShieldedInstanceConfigPtrOutputWithContext(ctx context.Context) ShieldedInstanceConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ShieldedInstanceConfigPtrOutput)
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfigOutput struct{ *pulumi.OutputState }

func (ShieldedInstanceConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ShieldedInstanceConfig)(nil)).Elem()
}

func (o ShieldedInstanceConfigOutput) ToShieldedInstanceConfigOutput() ShieldedInstanceConfigOutput {
	return o
}

func (o ShieldedInstanceConfigOutput) ToShieldedInstanceConfigOutputWithContext(ctx context.Context) ShieldedInstanceConfigOutput {
	return o
}

func (o ShieldedInstanceConfigOutput) ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput {
	return o.ToShieldedInstanceConfigPtrOutputWithContext(context.Background())
}

func (o ShieldedInstanceConfigOutput) ToShieldedInstanceConfigPtrOutputWithContext(ctx context.Context) ShieldedInstanceConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ShieldedInstanceConfig) *ShieldedInstanceConfig {
		return &v
	}).(ShieldedInstanceConfigPtrOutput)
}

// Optional. Defines whether instances have integrity monitoring enabled.
func (o ShieldedInstanceConfigOutput) EnableIntegrityMonitoring() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v ShieldedInstanceConfig) *bool { return v.EnableIntegrityMonitoring }).(pulumi.BoolPtrOutput)
}

// Optional. Defines whether instances have Secure Boot enabled.
func (o ShieldedInstanceConfigOutput) EnableSecureBoot() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v ShieldedInstanceConfig) *bool { return v.EnableSecureBoot }).(pulumi.BoolPtrOutput)
}

// Optional. Defines whether instances have the vTPM enabled.
func (o ShieldedInstanceConfigOutput) EnableVtpm() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v ShieldedInstanceConfig) *bool { return v.EnableVtpm }).(pulumi.BoolPtrOutput)
}

type ShieldedInstanceConfigPtrOutput struct{ *pulumi.OutputState }

func (ShieldedInstanceConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ShieldedInstanceConfig)(nil)).Elem()
}

func (o ShieldedInstanceConfigPtrOutput) ToShieldedInstanceConfigPtrOutput() ShieldedInstanceConfigPtrOutput {
	return o
}

func (o ShieldedInstanceConfigPtrOutput) ToShieldedInstanceConfigPtrOutputWithContext(ctx context.Context) ShieldedInstanceConfigPtrOutput {
	return o
}

func (o ShieldedInstanceConfigPtrOutput) Elem() ShieldedInstanceConfigOutput {
	return o.ApplyT(func(v *ShieldedInstanceConfig) ShieldedInstanceConfig {
		if v != nil {
			return *v
		}
		var ret ShieldedInstanceConfig
		return ret
	}).(ShieldedInstanceConfigOutput)
}

// Optional. Defines whether instances have integrity monitoring enabled.
func (o ShieldedInstanceConfigPtrOutput) EnableIntegrityMonitoring() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *ShieldedInstanceConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableIntegrityMonitoring
	}).(pulumi.BoolPtrOutput)
}

// Optional. Defines whether instances have Secure Boot enabled.
func (o ShieldedInstanceConfigPtrOutput) EnableSecureBoot() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *ShieldedInstanceConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableSecureBoot
	}).(pulumi.BoolPtrOutput)
}

// Optional. Defines whether instances have the vTPM enabled.
func (o ShieldedInstanceConfigPtrOutput) EnableVtpm() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *ShieldedInstanceConfig) *bool {
		if v == nil {
			return nil
		}
		return v.EnableVtpm
	}).(pulumi.BoolPtrOutput)
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfigResponse struct {
	// Optional. Defines whether instances have integrity monitoring enabled.
	EnableIntegrityMonitoring bool `pulumi:"enableIntegrityMonitoring"`
	// Optional. Defines whether instances have Secure Boot enabled.
	EnableSecureBoot bool `pulumi:"enableSecureBoot"`
	// Optional. Defines whether instances have the vTPM enabled.
	EnableVtpm bool `pulumi:"enableVtpm"`
}

// Shielded Instance Config for clusters using Compute Engine Shielded VMs (https://cloud.google.com/security/shielded-cloud/shielded-vm).
type ShieldedInstanceConfigResponseOutput struct{ *pulumi.OutputState }

func (ShieldedInstanceConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ShieldedInstanceConfigResponse)(nil)).Elem()
}

func (o ShieldedInstanceConfigResponseOutput) ToShieldedInstanceConfigResponseOutput() ShieldedInstanceConfigResponseOutput {
	return o
}

func (o ShieldedInstanceConfigResponseOutput) ToShieldedInstanceConfigResponseOutputWithContext(ctx context.Context) ShieldedInstanceConfigResponseOutput {
	return o
}

// Optional. Defines whether instances have integrity monitoring enabled.
func (o ShieldedInstanceConfigResponseOutput) EnableIntegrityMonitoring() pulumi.BoolOutput {
	return o.ApplyT(func(v ShieldedInstanceConfigResponse) bool { return v.EnableIntegrityMonitoring }).(pulumi.BoolOutput)
}

// Optional. Defines whether instances have Secure Boot enabled.
func (o ShieldedInstanceConfigResponseOutput) EnableSecureBoot() pulumi.BoolOutput {
	return o.ApplyT(func(v ShieldedInstanceConfigResponse) bool { return v.EnableSecureBoot }).(pulumi.BoolOutput)
}

// Optional. Defines whether instances have the vTPM enabled.
func (o ShieldedInstanceConfigResponseOutput) EnableVtpm() pulumi.BoolOutput {
	return o.ApplyT(func(v ShieldedInstanceConfigResponse) bool { return v.EnableVtpm }).(pulumi.BoolOutput)
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfig struct {
	// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
	ImageVersion *string `pulumi:"imageVersion"`
	// Optional. The set of components to activate on the cluster.
	OptionalComponents []SoftwareConfigOptionalComponentsItem `pulumi:"optionalComponents"`
	// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
	Properties map[string]string `pulumi:"properties"`
}

// SoftwareConfigInput is an input type that accepts SoftwareConfigArgs and SoftwareConfigOutput values.
// You can construct a concrete instance of `SoftwareConfigInput` via:
//
//          SoftwareConfigArgs{...}
type SoftwareConfigInput interface {
	pulumi.Input

	ToSoftwareConfigOutput() SoftwareConfigOutput
	ToSoftwareConfigOutputWithContext(context.Context) SoftwareConfigOutput
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfigArgs struct {
	// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
	ImageVersion pulumi.StringPtrInput `pulumi:"imageVersion"`
	// Optional. The set of components to activate on the cluster.
	OptionalComponents SoftwareConfigOptionalComponentsItemArrayInput `pulumi:"optionalComponents"`
	// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
	Properties pulumi.StringMapInput `pulumi:"properties"`
}

func (SoftwareConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SoftwareConfig)(nil)).Elem()
}

func (i SoftwareConfigArgs) ToSoftwareConfigOutput() SoftwareConfigOutput {
	return i.ToSoftwareConfigOutputWithContext(context.Background())
}

func (i SoftwareConfigArgs) ToSoftwareConfigOutputWithContext(ctx context.Context) SoftwareConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SoftwareConfigOutput)
}

func (i SoftwareConfigArgs) ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput {
	return i.ToSoftwareConfigPtrOutputWithContext(context.Background())
}

func (i SoftwareConfigArgs) ToSoftwareConfigPtrOutputWithContext(ctx context.Context) SoftwareConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SoftwareConfigOutput).ToSoftwareConfigPtrOutputWithContext(ctx)
}

// SoftwareConfigPtrInput is an input type that accepts SoftwareConfigArgs, SoftwareConfigPtr and SoftwareConfigPtrOutput values.
// You can construct a concrete instance of `SoftwareConfigPtrInput` via:
//
//          SoftwareConfigArgs{...}
//
//  or:
//
//          nil
type SoftwareConfigPtrInput interface {
	pulumi.Input

	ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput
	ToSoftwareConfigPtrOutputWithContext(context.Context) SoftwareConfigPtrOutput
}

type softwareConfigPtrType SoftwareConfigArgs

func SoftwareConfigPtr(v *SoftwareConfigArgs) SoftwareConfigPtrInput {
	return (*softwareConfigPtrType)(v)
}

func (*softwareConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SoftwareConfig)(nil)).Elem()
}

func (i *softwareConfigPtrType) ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput {
	return i.ToSoftwareConfigPtrOutputWithContext(context.Background())
}

func (i *softwareConfigPtrType) ToSoftwareConfigPtrOutputWithContext(ctx context.Context) SoftwareConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SoftwareConfigPtrOutput)
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfigOutput struct{ *pulumi.OutputState }

func (SoftwareConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SoftwareConfig)(nil)).Elem()
}

func (o SoftwareConfigOutput) ToSoftwareConfigOutput() SoftwareConfigOutput {
	return o
}

func (o SoftwareConfigOutput) ToSoftwareConfigOutputWithContext(ctx context.Context) SoftwareConfigOutput {
	return o
}

func (o SoftwareConfigOutput) ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput {
	return o.ToSoftwareConfigPtrOutputWithContext(context.Background())
}

func (o SoftwareConfigOutput) ToSoftwareConfigPtrOutputWithContext(ctx context.Context) SoftwareConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SoftwareConfig) *SoftwareConfig {
		return &v
	}).(SoftwareConfigPtrOutput)
}

// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
func (o SoftwareConfigOutput) ImageVersion() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SoftwareConfig) *string { return v.ImageVersion }).(pulumi.StringPtrOutput)
}

// Optional. The set of components to activate on the cluster.
func (o SoftwareConfigOutput) OptionalComponents() SoftwareConfigOptionalComponentsItemArrayOutput {
	return o.ApplyT(func(v SoftwareConfig) []SoftwareConfigOptionalComponentsItem { return v.OptionalComponents }).(SoftwareConfigOptionalComponentsItemArrayOutput)
}

// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
func (o SoftwareConfigOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SoftwareConfig) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

type SoftwareConfigPtrOutput struct{ *pulumi.OutputState }

func (SoftwareConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SoftwareConfig)(nil)).Elem()
}

func (o SoftwareConfigPtrOutput) ToSoftwareConfigPtrOutput() SoftwareConfigPtrOutput {
	return o
}

func (o SoftwareConfigPtrOutput) ToSoftwareConfigPtrOutputWithContext(ctx context.Context) SoftwareConfigPtrOutput {
	return o
}

func (o SoftwareConfigPtrOutput) Elem() SoftwareConfigOutput {
	return o.ApplyT(func(v *SoftwareConfig) SoftwareConfig {
		if v != nil {
			return *v
		}
		var ret SoftwareConfig
		return ret
	}).(SoftwareConfigOutput)
}

// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
func (o SoftwareConfigPtrOutput) ImageVersion() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SoftwareConfig) *string {
		if v == nil {
			return nil
		}
		return v.ImageVersion
	}).(pulumi.StringPtrOutput)
}

// Optional. The set of components to activate on the cluster.
func (o SoftwareConfigPtrOutput) OptionalComponents() SoftwareConfigOptionalComponentsItemArrayOutput {
	return o.ApplyT(func(v *SoftwareConfig) []SoftwareConfigOptionalComponentsItem {
		if v == nil {
			return nil
		}
		return v.OptionalComponents
	}).(SoftwareConfigOptionalComponentsItemArrayOutput)
}

// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
func (o SoftwareConfigPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SoftwareConfig) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfigResponse struct {
	// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
	ImageVersion string `pulumi:"imageVersion"`
	// Optional. The set of components to activate on the cluster.
	OptionalComponents []string `pulumi:"optionalComponents"`
	// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
	Properties map[string]string `pulumi:"properties"`
}

// Specifies the selection and config of software inside the cluster.
type SoftwareConfigResponseOutput struct{ *pulumi.OutputState }

func (SoftwareConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SoftwareConfigResponse)(nil)).Elem()
}

func (o SoftwareConfigResponseOutput) ToSoftwareConfigResponseOutput() SoftwareConfigResponseOutput {
	return o
}

func (o SoftwareConfigResponseOutput) ToSoftwareConfigResponseOutputWithContext(ctx context.Context) SoftwareConfigResponseOutput {
	return o
}

// Optional. The version of software inside the cluster. It must be one of the supported Dataproc Versions (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions), such as "1.2" (including a subminor version, such as "1.2.29"), or the "preview" version (https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions#other_versions). If unspecified, it defaults to the latest Debian version.
func (o SoftwareConfigResponseOutput) ImageVersion() pulumi.StringOutput {
	return o.ApplyT(func(v SoftwareConfigResponse) string { return v.ImageVersion }).(pulumi.StringOutput)
}

// Optional. The set of components to activate on the cluster.
func (o SoftwareConfigResponseOutput) OptionalComponents() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SoftwareConfigResponse) []string { return v.OptionalComponents }).(pulumi.StringArrayOutput)
}

// Optional. The properties to set on daemon config files.Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. The following are supported prefixes and their mappings: capacity-scheduler: capacity-scheduler.xml core: core-site.xml distcp: distcp-default.xml hdfs: hdfs-site.xml hive: hive-site.xml mapred: mapred-site.xml pig: pig.properties spark: spark-defaults.conf yarn: yarn-site.xmlFor more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
func (o SoftwareConfigResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SoftwareConfigResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatch struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
	MainClass *string `pulumi:"mainClass"`
	// Optional. The HCFS URI of the jar file that contains the main class.
	MainJarFileUri *string `pulumi:"mainJarFileUri"`
}

// SparkBatchInput is an input type that accepts SparkBatchArgs and SparkBatchOutput values.
// You can construct a concrete instance of `SparkBatchInput` via:
//
//          SparkBatchArgs{...}
type SparkBatchInput interface {
	pulumi.Input

	ToSparkBatchOutput() SparkBatchOutput
	ToSparkBatchOutputWithContext(context.Context) SparkBatchOutput
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatchArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
	MainClass pulumi.StringPtrInput `pulumi:"mainClass"`
	// Optional. The HCFS URI of the jar file that contains the main class.
	MainJarFileUri pulumi.StringPtrInput `pulumi:"mainJarFileUri"`
}

func (SparkBatchArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkBatch)(nil)).Elem()
}

func (i SparkBatchArgs) ToSparkBatchOutput() SparkBatchOutput {
	return i.ToSparkBatchOutputWithContext(context.Background())
}

func (i SparkBatchArgs) ToSparkBatchOutputWithContext(ctx context.Context) SparkBatchOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkBatchOutput)
}

func (i SparkBatchArgs) ToSparkBatchPtrOutput() SparkBatchPtrOutput {
	return i.ToSparkBatchPtrOutputWithContext(context.Background())
}

func (i SparkBatchArgs) ToSparkBatchPtrOutputWithContext(ctx context.Context) SparkBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkBatchOutput).ToSparkBatchPtrOutputWithContext(ctx)
}

// SparkBatchPtrInput is an input type that accepts SparkBatchArgs, SparkBatchPtr and SparkBatchPtrOutput values.
// You can construct a concrete instance of `SparkBatchPtrInput` via:
//
//          SparkBatchArgs{...}
//
//  or:
//
//          nil
type SparkBatchPtrInput interface {
	pulumi.Input

	ToSparkBatchPtrOutput() SparkBatchPtrOutput
	ToSparkBatchPtrOutputWithContext(context.Context) SparkBatchPtrOutput
}

type sparkBatchPtrType SparkBatchArgs

func SparkBatchPtr(v *SparkBatchArgs) SparkBatchPtrInput {
	return (*sparkBatchPtrType)(v)
}

func (*sparkBatchPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkBatch)(nil)).Elem()
}

func (i *sparkBatchPtrType) ToSparkBatchPtrOutput() SparkBatchPtrOutput {
	return i.ToSparkBatchPtrOutputWithContext(context.Background())
}

func (i *sparkBatchPtrType) ToSparkBatchPtrOutputWithContext(ctx context.Context) SparkBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkBatchPtrOutput)
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatchOutput struct{ *pulumi.OutputState }

func (SparkBatchOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkBatch)(nil)).Elem()
}

func (o SparkBatchOutput) ToSparkBatchOutput() SparkBatchOutput {
	return o
}

func (o SparkBatchOutput) ToSparkBatchOutputWithContext(ctx context.Context) SparkBatchOutput {
	return o
}

func (o SparkBatchOutput) ToSparkBatchPtrOutput() SparkBatchPtrOutput {
	return o.ToSparkBatchPtrOutputWithContext(context.Background())
}

func (o SparkBatchOutput) ToSparkBatchPtrOutputWithContext(ctx context.Context) SparkBatchPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkBatch) *SparkBatch {
		return &v
	}).(SparkBatchPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkBatchOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatch) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkBatchOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatch) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkBatchOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatch) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o SparkBatchOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatch) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
func (o SparkBatchOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkBatch) *string { return v.MainClass }).(pulumi.StringPtrOutput)
}

// Optional. The HCFS URI of the jar file that contains the main class.
func (o SparkBatchOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkBatch) *string { return v.MainJarFileUri }).(pulumi.StringPtrOutput)
}

type SparkBatchPtrOutput struct{ *pulumi.OutputState }

func (SparkBatchPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkBatch)(nil)).Elem()
}

func (o SparkBatchPtrOutput) ToSparkBatchPtrOutput() SparkBatchPtrOutput {
	return o
}

func (o SparkBatchPtrOutput) ToSparkBatchPtrOutputWithContext(ctx context.Context) SparkBatchPtrOutput {
	return o
}

func (o SparkBatchPtrOutput) Elem() SparkBatchOutput {
	return o.ApplyT(func(v *SparkBatch) SparkBatch {
		if v != nil {
			return *v
		}
		var ret SparkBatch
		return ret
	}).(SparkBatchOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkBatchPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkBatchPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkBatchPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o SparkBatchPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkBatch) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
func (o SparkBatchPtrOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkBatch) *string {
		if v == nil {
			return nil
		}
		return v.MainClass
	}).(pulumi.StringPtrOutput)
}

// Optional. The HCFS URI of the jar file that contains the main class.
func (o SparkBatchPtrOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkBatch) *string {
		if v == nil {
			return nil
		}
		return v.MainJarFileUri
	}).(pulumi.StringPtrOutput)
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatchResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
	MainClass string `pulumi:"mainClass"`
	// Optional. The HCFS URI of the jar file that contains the main class.
	MainJarFileUri string `pulumi:"mainJarFileUri"`
}

// A configuration for running an Apache Spark (https://spark.apache.org/) batch workload.
type SparkBatchResponseOutput struct{ *pulumi.OutputState }

func (SparkBatchResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkBatchResponse)(nil)).Elem()
}

func (o SparkBatchResponseOutput) ToSparkBatchResponseOutput() SparkBatchResponseOutput {
	return o
}

func (o SparkBatchResponseOutput) ToSparkBatchResponseOutputWithContext(ctx context.Context) SparkBatchResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkBatchResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatchResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkBatchResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatchResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkBatchResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatchResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.
func (o SparkBatchResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkBatchResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The name of the driver main class. The jar file that contains the class must be in the classpath or specified in jar_file_uris.
func (o SparkBatchResponseOutput) MainClass() pulumi.StringOutput {
	return o.ApplyT(func(v SparkBatchResponse) string { return v.MainClass }).(pulumi.StringOutput)
}

// Optional. The HCFS URI of the jar file that contains the main class.
func (o SparkBatchResponseOutput) MainJarFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkBatchResponse) string { return v.MainJarFileUri }).(pulumi.StringOutput)
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfig struct {
	// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
	DataprocCluster *string `pulumi:"dataprocCluster"`
}

// SparkHistoryServerConfigInput is an input type that accepts SparkHistoryServerConfigArgs and SparkHistoryServerConfigOutput values.
// You can construct a concrete instance of `SparkHistoryServerConfigInput` via:
//
//          SparkHistoryServerConfigArgs{...}
type SparkHistoryServerConfigInput interface {
	pulumi.Input

	ToSparkHistoryServerConfigOutput() SparkHistoryServerConfigOutput
	ToSparkHistoryServerConfigOutputWithContext(context.Context) SparkHistoryServerConfigOutput
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfigArgs struct {
	// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
	DataprocCluster pulumi.StringPtrInput `pulumi:"dataprocCluster"`
}

func (SparkHistoryServerConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkHistoryServerConfig)(nil)).Elem()
}

func (i SparkHistoryServerConfigArgs) ToSparkHistoryServerConfigOutput() SparkHistoryServerConfigOutput {
	return i.ToSparkHistoryServerConfigOutputWithContext(context.Background())
}

func (i SparkHistoryServerConfigArgs) ToSparkHistoryServerConfigOutputWithContext(ctx context.Context) SparkHistoryServerConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkHistoryServerConfigOutput)
}

func (i SparkHistoryServerConfigArgs) ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput {
	return i.ToSparkHistoryServerConfigPtrOutputWithContext(context.Background())
}

func (i SparkHistoryServerConfigArgs) ToSparkHistoryServerConfigPtrOutputWithContext(ctx context.Context) SparkHistoryServerConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkHistoryServerConfigOutput).ToSparkHistoryServerConfigPtrOutputWithContext(ctx)
}

// SparkHistoryServerConfigPtrInput is an input type that accepts SparkHistoryServerConfigArgs, SparkHistoryServerConfigPtr and SparkHistoryServerConfigPtrOutput values.
// You can construct a concrete instance of `SparkHistoryServerConfigPtrInput` via:
//
//          SparkHistoryServerConfigArgs{...}
//
//  or:
//
//          nil
type SparkHistoryServerConfigPtrInput interface {
	pulumi.Input

	ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput
	ToSparkHistoryServerConfigPtrOutputWithContext(context.Context) SparkHistoryServerConfigPtrOutput
}

type sparkHistoryServerConfigPtrType SparkHistoryServerConfigArgs

func SparkHistoryServerConfigPtr(v *SparkHistoryServerConfigArgs) SparkHistoryServerConfigPtrInput {
	return (*sparkHistoryServerConfigPtrType)(v)
}

func (*sparkHistoryServerConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkHistoryServerConfig)(nil)).Elem()
}

func (i *sparkHistoryServerConfigPtrType) ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput {
	return i.ToSparkHistoryServerConfigPtrOutputWithContext(context.Background())
}

func (i *sparkHistoryServerConfigPtrType) ToSparkHistoryServerConfigPtrOutputWithContext(ctx context.Context) SparkHistoryServerConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkHistoryServerConfigPtrOutput)
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfigOutput struct{ *pulumi.OutputState }

func (SparkHistoryServerConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkHistoryServerConfig)(nil)).Elem()
}

func (o SparkHistoryServerConfigOutput) ToSparkHistoryServerConfigOutput() SparkHistoryServerConfigOutput {
	return o
}

func (o SparkHistoryServerConfigOutput) ToSparkHistoryServerConfigOutputWithContext(ctx context.Context) SparkHistoryServerConfigOutput {
	return o
}

func (o SparkHistoryServerConfigOutput) ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput {
	return o.ToSparkHistoryServerConfigPtrOutputWithContext(context.Background())
}

func (o SparkHistoryServerConfigOutput) ToSparkHistoryServerConfigPtrOutputWithContext(ctx context.Context) SparkHistoryServerConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkHistoryServerConfig) *SparkHistoryServerConfig {
		return &v
	}).(SparkHistoryServerConfigPtrOutput)
}

// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
func (o SparkHistoryServerConfigOutput) DataprocCluster() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkHistoryServerConfig) *string { return v.DataprocCluster }).(pulumi.StringPtrOutput)
}

type SparkHistoryServerConfigPtrOutput struct{ *pulumi.OutputState }

func (SparkHistoryServerConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkHistoryServerConfig)(nil)).Elem()
}

func (o SparkHistoryServerConfigPtrOutput) ToSparkHistoryServerConfigPtrOutput() SparkHistoryServerConfigPtrOutput {
	return o
}

func (o SparkHistoryServerConfigPtrOutput) ToSparkHistoryServerConfigPtrOutputWithContext(ctx context.Context) SparkHistoryServerConfigPtrOutput {
	return o
}

func (o SparkHistoryServerConfigPtrOutput) Elem() SparkHistoryServerConfigOutput {
	return o.ApplyT(func(v *SparkHistoryServerConfig) SparkHistoryServerConfig {
		if v != nil {
			return *v
		}
		var ret SparkHistoryServerConfig
		return ret
	}).(SparkHistoryServerConfigOutput)
}

// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
func (o SparkHistoryServerConfigPtrOutput) DataprocCluster() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkHistoryServerConfig) *string {
		if v == nil {
			return nil
		}
		return v.DataprocCluster
	}).(pulumi.StringPtrOutput)
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfigResponse struct {
	// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
	DataprocCluster string `pulumi:"dataprocCluster"`
}

// Spark History Server configuration for the workload.
type SparkHistoryServerConfigResponseOutput struct{ *pulumi.OutputState }

func (SparkHistoryServerConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkHistoryServerConfigResponse)(nil)).Elem()
}

func (o SparkHistoryServerConfigResponseOutput) ToSparkHistoryServerConfigResponseOutput() SparkHistoryServerConfigResponseOutput {
	return o
}

func (o SparkHistoryServerConfigResponseOutput) ToSparkHistoryServerConfigResponseOutputWithContext(ctx context.Context) SparkHistoryServerConfigResponseOutput {
	return o
}

// Optional. Resource name of an existing Dataproc Cluster to act as a Spark History Server for the workload.Example: projects/[project_id]/regions/[region]/clusters/[cluster_name]
func (o SparkHistoryServerConfigResponseOutput) DataprocCluster() pulumi.StringOutput {
	return o.ApplyT(func(v SparkHistoryServerConfigResponse) string { return v.DataprocCluster }).(pulumi.StringOutput)
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJob struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass *string `pulumi:"mainClass"`
	// The HCFS URI of the jar file that contains the main class.
	MainJarFileUri *string `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// SparkJobInput is an input type that accepts SparkJobArgs and SparkJobOutput values.
// You can construct a concrete instance of `SparkJobInput` via:
//
//          SparkJobArgs{...}
type SparkJobInput interface {
	pulumi.Input

	ToSparkJobOutput() SparkJobOutput
	ToSparkJobOutputWithContext(context.Context) SparkJobOutput
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJobArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass pulumi.StringPtrInput `pulumi:"mainClass"`
	// The HCFS URI of the jar file that contains the main class.
	MainJarFileUri pulumi.StringPtrInput `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
}

func (SparkJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJob)(nil)).Elem()
}

func (i SparkJobArgs) ToSparkJobOutput() SparkJobOutput {
	return i.ToSparkJobOutputWithContext(context.Background())
}

func (i SparkJobArgs) ToSparkJobOutputWithContext(ctx context.Context) SparkJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobOutput)
}

func (i SparkJobArgs) ToSparkJobPtrOutput() SparkJobPtrOutput {
	return i.ToSparkJobPtrOutputWithContext(context.Background())
}

func (i SparkJobArgs) ToSparkJobPtrOutputWithContext(ctx context.Context) SparkJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobOutput).ToSparkJobPtrOutputWithContext(ctx)
}

// SparkJobPtrInput is an input type that accepts SparkJobArgs, SparkJobPtr and SparkJobPtrOutput values.
// You can construct a concrete instance of `SparkJobPtrInput` via:
//
//          SparkJobArgs{...}
//
//  or:
//
//          nil
type SparkJobPtrInput interface {
	pulumi.Input

	ToSparkJobPtrOutput() SparkJobPtrOutput
	ToSparkJobPtrOutputWithContext(context.Context) SparkJobPtrOutput
}

type sparkJobPtrType SparkJobArgs

func SparkJobPtr(v *SparkJobArgs) SparkJobPtrInput {
	return (*sparkJobPtrType)(v)
}

func (*sparkJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkJob)(nil)).Elem()
}

func (i *sparkJobPtrType) ToSparkJobPtrOutput() SparkJobPtrOutput {
	return i.ToSparkJobPtrOutputWithContext(context.Background())
}

func (i *sparkJobPtrType) ToSparkJobPtrOutputWithContext(ctx context.Context) SparkJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkJobPtrOutput)
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJobOutput struct{ *pulumi.OutputState }

func (SparkJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJob)(nil)).Elem()
}

func (o SparkJobOutput) ToSparkJobOutput() SparkJobOutput {
	return o
}

func (o SparkJobOutput) ToSparkJobOutputWithContext(ctx context.Context) SparkJobOutput {
	return o
}

func (o SparkJobOutput) ToSparkJobPtrOutput() SparkJobPtrOutput {
	return o.ToSparkJobPtrOutputWithContext(context.Background())
}

func (o SparkJobOutput) ToSparkJobPtrOutputWithContext(ctx context.Context) SparkJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkJob) *SparkJob {
		return &v
	}).(SparkJobPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkJobOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJob) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkJobOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJob) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkJobOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJob) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
func (o SparkJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v SparkJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o SparkJobOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkJob) *string { return v.MainClass }).(pulumi.StringPtrOutput)
}

// The HCFS URI of the jar file that contains the main class.
func (o SparkJobOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkJob) *string { return v.MainJarFileUri }).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

type SparkJobPtrOutput struct{ *pulumi.OutputState }

func (SparkJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkJob)(nil)).Elem()
}

func (o SparkJobPtrOutput) ToSparkJobPtrOutput() SparkJobPtrOutput {
	return o
}

func (o SparkJobPtrOutput) ToSparkJobPtrOutputWithContext(ctx context.Context) SparkJobPtrOutput {
	return o
}

func (o SparkJobPtrOutput) Elem() SparkJobOutput {
	return o.ApplyT(func(v *SparkJob) SparkJob {
		if v != nil {
			return *v
		}
		var ret SparkJob
		return ret
	}).(SparkJobOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkJobPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkJob) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkJobPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkJob) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkJobPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkJob) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
func (o SparkJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *SparkJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o SparkJobPtrOutput) MainClass() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkJob) *string {
		if v == nil {
			return nil
		}
		return v.MainClass
	}).(pulumi.StringPtrOutput)
}

// The HCFS URI of the jar file that contains the main class.
func (o SparkJobPtrOutput) MainJarFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkJob) *string {
		if v == nil {
			return nil
		}
		return v.MainJarFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJobResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
	MainClass string `pulumi:"mainClass"`
	// The HCFS URI of the jar file that contains the main class.
	MainJarFileUri string `pulumi:"mainJarFileUri"`
	// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// A Dataproc job for running Apache Spark (https://spark.apache.org/) applications on YARN.
type SparkJobResponseOutput struct{ *pulumi.OutputState }

func (SparkJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkJobResponse)(nil)).Elem()
}

func (o SparkJobResponseOutput) ToSparkJobResponseOutput() SparkJobResponseOutput {
	return o
}

func (o SparkJobResponseOutput) ToSparkJobResponseOutputWithContext(ctx context.Context) SparkJobResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkJobResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJobResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkJobResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJobResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkJobResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJobResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of jar files to add to the CLASSPATHs of the Spark driver and tasks.
func (o SparkJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v SparkJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris.
func (o SparkJobResponseOutput) MainClass() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobResponse) string { return v.MainClass }).(pulumi.StringOutput)
}

// The HCFS URI of the jar file that contains the main class.
func (o SparkJobResponseOutput) MainJarFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkJobResponse) string { return v.MainJarFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure Spark. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatch struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
	MainRFileUri string `pulumi:"mainRFileUri"`
}

// SparkRBatchInput is an input type that accepts SparkRBatchArgs and SparkRBatchOutput values.
// You can construct a concrete instance of `SparkRBatchInput` via:
//
//          SparkRBatchArgs{...}
type SparkRBatchInput interface {
	pulumi.Input

	ToSparkRBatchOutput() SparkRBatchOutput
	ToSparkRBatchOutputWithContext(context.Context) SparkRBatchOutput
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatchArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
	MainRFileUri pulumi.StringInput `pulumi:"mainRFileUri"`
}

func (SparkRBatchArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRBatch)(nil)).Elem()
}

func (i SparkRBatchArgs) ToSparkRBatchOutput() SparkRBatchOutput {
	return i.ToSparkRBatchOutputWithContext(context.Background())
}

func (i SparkRBatchArgs) ToSparkRBatchOutputWithContext(ctx context.Context) SparkRBatchOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRBatchOutput)
}

func (i SparkRBatchArgs) ToSparkRBatchPtrOutput() SparkRBatchPtrOutput {
	return i.ToSparkRBatchPtrOutputWithContext(context.Background())
}

func (i SparkRBatchArgs) ToSparkRBatchPtrOutputWithContext(ctx context.Context) SparkRBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRBatchOutput).ToSparkRBatchPtrOutputWithContext(ctx)
}

// SparkRBatchPtrInput is an input type that accepts SparkRBatchArgs, SparkRBatchPtr and SparkRBatchPtrOutput values.
// You can construct a concrete instance of `SparkRBatchPtrInput` via:
//
//          SparkRBatchArgs{...}
//
//  or:
//
//          nil
type SparkRBatchPtrInput interface {
	pulumi.Input

	ToSparkRBatchPtrOutput() SparkRBatchPtrOutput
	ToSparkRBatchPtrOutputWithContext(context.Context) SparkRBatchPtrOutput
}

type sparkRBatchPtrType SparkRBatchArgs

func SparkRBatchPtr(v *SparkRBatchArgs) SparkRBatchPtrInput {
	return (*sparkRBatchPtrType)(v)
}

func (*sparkRBatchPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkRBatch)(nil)).Elem()
}

func (i *sparkRBatchPtrType) ToSparkRBatchPtrOutput() SparkRBatchPtrOutput {
	return i.ToSparkRBatchPtrOutputWithContext(context.Background())
}

func (i *sparkRBatchPtrType) ToSparkRBatchPtrOutputWithContext(ctx context.Context) SparkRBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRBatchPtrOutput)
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatchOutput struct{ *pulumi.OutputState }

func (SparkRBatchOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRBatch)(nil)).Elem()
}

func (o SparkRBatchOutput) ToSparkRBatchOutput() SparkRBatchOutput {
	return o
}

func (o SparkRBatchOutput) ToSparkRBatchOutputWithContext(ctx context.Context) SparkRBatchOutput {
	return o
}

func (o SparkRBatchOutput) ToSparkRBatchPtrOutput() SparkRBatchPtrOutput {
	return o.ToSparkRBatchPtrOutputWithContext(context.Background())
}

func (o SparkRBatchOutput) ToSparkRBatchPtrOutputWithContext(ctx context.Context) SparkRBatchPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkRBatch) *SparkRBatch {
		return &v
	}).(SparkRBatchPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRBatchOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatch) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkRBatchOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatch) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkRBatchOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatch) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
func (o SparkRBatchOutput) MainRFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkRBatch) string { return v.MainRFileUri }).(pulumi.StringOutput)
}

type SparkRBatchPtrOutput struct{ *pulumi.OutputState }

func (SparkRBatchPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkRBatch)(nil)).Elem()
}

func (o SparkRBatchPtrOutput) ToSparkRBatchPtrOutput() SparkRBatchPtrOutput {
	return o
}

func (o SparkRBatchPtrOutput) ToSparkRBatchPtrOutputWithContext(ctx context.Context) SparkRBatchPtrOutput {
	return o
}

func (o SparkRBatchPtrOutput) Elem() SparkRBatchOutput {
	return o.ApplyT(func(v *SparkRBatch) SparkRBatch {
		if v != nil {
			return *v
		}
		var ret SparkRBatch
		return ret
	}).(SparkRBatchOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRBatchPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRBatch) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkRBatchPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRBatch) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkRBatchPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRBatch) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
func (o SparkRBatchPtrOutput) MainRFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkRBatch) *string {
		if v == nil {
			return nil
		}
		return &v.MainRFileUri
	}).(pulumi.StringPtrOutput)
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatchResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor.
	FileUris []string `pulumi:"fileUris"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
	MainRFileUri string `pulumi:"mainRFileUri"`
}

// A configuration for running an Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) batch workload.
type SparkRBatchResponseOutput struct{ *pulumi.OutputState }

func (SparkRBatchResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRBatchResponse)(nil)).Elem()
}

func (o SparkRBatchResponseOutput) ToSparkRBatchResponseOutput() SparkRBatchResponseOutput {
	return o
}

func (o SparkRBatchResponseOutput) ToSparkRBatchResponseOutputWithContext(ctx context.Context) SparkRBatchResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRBatchResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatchResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the Spark driver. Do not include arguments that can be set as batch properties, such as --conf, since a collision can occur that causes an incorrect batch submission.
func (o SparkRBatchResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatchResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor.
func (o SparkRBatchResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRBatchResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R or .r file.
func (o SparkRBatchResponseOutput) MainRFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkRBatchResponse) string { return v.MainRFileUri }).(pulumi.StringOutput)
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJob struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R file.
	MainRFileUri string `pulumi:"mainRFileUri"`
	// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// SparkRJobInput is an input type that accepts SparkRJobArgs and SparkRJobOutput values.
// You can construct a concrete instance of `SparkRJobInput` via:
//
//          SparkRJobArgs{...}
type SparkRJobInput interface {
	pulumi.Input

	ToSparkRJobOutput() SparkRJobOutput
	ToSparkRJobOutputWithContext(context.Context) SparkRJobOutput
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJobArgs struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris pulumi.StringArrayInput `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args pulumi.StringArrayInput `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris pulumi.StringArrayInput `pulumi:"fileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R file.
	MainRFileUri pulumi.StringInput `pulumi:"mainRFileUri"`
	// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties pulumi.StringMapInput `pulumi:"properties"`
}

func (SparkRJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRJob)(nil)).Elem()
}

func (i SparkRJobArgs) ToSparkRJobOutput() SparkRJobOutput {
	return i.ToSparkRJobOutputWithContext(context.Background())
}

func (i SparkRJobArgs) ToSparkRJobOutputWithContext(ctx context.Context) SparkRJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRJobOutput)
}

func (i SparkRJobArgs) ToSparkRJobPtrOutput() SparkRJobPtrOutput {
	return i.ToSparkRJobPtrOutputWithContext(context.Background())
}

func (i SparkRJobArgs) ToSparkRJobPtrOutputWithContext(ctx context.Context) SparkRJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRJobOutput).ToSparkRJobPtrOutputWithContext(ctx)
}

// SparkRJobPtrInput is an input type that accepts SparkRJobArgs, SparkRJobPtr and SparkRJobPtrOutput values.
// You can construct a concrete instance of `SparkRJobPtrInput` via:
//
//          SparkRJobArgs{...}
//
//  or:
//
//          nil
type SparkRJobPtrInput interface {
	pulumi.Input

	ToSparkRJobPtrOutput() SparkRJobPtrOutput
	ToSparkRJobPtrOutputWithContext(context.Context) SparkRJobPtrOutput
}

type sparkRJobPtrType SparkRJobArgs

func SparkRJobPtr(v *SparkRJobArgs) SparkRJobPtrInput {
	return (*sparkRJobPtrType)(v)
}

func (*sparkRJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkRJob)(nil)).Elem()
}

func (i *sparkRJobPtrType) ToSparkRJobPtrOutput() SparkRJobPtrOutput {
	return i.ToSparkRJobPtrOutputWithContext(context.Background())
}

func (i *sparkRJobPtrType) ToSparkRJobPtrOutputWithContext(ctx context.Context) SparkRJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkRJobPtrOutput)
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJobOutput struct{ *pulumi.OutputState }

func (SparkRJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRJob)(nil)).Elem()
}

func (o SparkRJobOutput) ToSparkRJobOutput() SparkRJobOutput {
	return o
}

func (o SparkRJobOutput) ToSparkRJobOutputWithContext(ctx context.Context) SparkRJobOutput {
	return o
}

func (o SparkRJobOutput) ToSparkRJobPtrOutput() SparkRJobPtrOutput {
	return o.ToSparkRJobPtrOutputWithContext(context.Background())
}

func (o SparkRJobOutput) ToSparkRJobPtrOutputWithContext(ctx context.Context) SparkRJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkRJob) *SparkRJob {
		return &v
	}).(SparkRJobPtrOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRJobOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJob) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkRJobOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJob) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkRJobOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJob) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkRJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v SparkRJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R file.
func (o SparkRJobOutput) MainRFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkRJob) string { return v.MainRFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkRJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkRJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

type SparkRJobPtrOutput struct{ *pulumi.OutputState }

func (SparkRJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkRJob)(nil)).Elem()
}

func (o SparkRJobPtrOutput) ToSparkRJobPtrOutput() SparkRJobPtrOutput {
	return o
}

func (o SparkRJobPtrOutput) ToSparkRJobPtrOutputWithContext(ctx context.Context) SparkRJobPtrOutput {
	return o
}

func (o SparkRJobPtrOutput) Elem() SparkRJobOutput {
	return o.ApplyT(func(v *SparkRJob) SparkRJob {
		if v != nil {
			return *v
		}
		var ret SparkRJob
		return ret
	}).(SparkRJobOutput)
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRJobPtrOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRJob) []string {
		if v == nil {
			return nil
		}
		return v.ArchiveUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkRJobPtrOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRJob) []string {
		if v == nil {
			return nil
		}
		return v.Args
	}).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkRJobPtrOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkRJob) []string {
		if v == nil {
			return nil
		}
		return v.FileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkRJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *SparkRJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R file.
func (o SparkRJobPtrOutput) MainRFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkRJob) *string {
		if v == nil {
			return nil
		}
		return &v.MainRFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkRJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkRJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJobResponse struct {
	// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
	ArchiveUris []string `pulumi:"archiveUris"`
	// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
	Args []string `pulumi:"args"`
	// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
	FileUris []string `pulumi:"fileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// The HCFS URI of the main R file to use as the driver. Must be a .R file.
	MainRFileUri string `pulumi:"mainRFileUri"`
	// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
	Properties map[string]string `pulumi:"properties"`
}

// A Dataproc job for running Apache SparkR (https://spark.apache.org/docs/latest/sparkr.html) applications on YARN.
type SparkRJobResponseOutput struct{ *pulumi.OutputState }

func (SparkRJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkRJobResponse)(nil)).Elem()
}

func (o SparkRJobResponseOutput) ToSparkRJobResponseOutput() SparkRJobResponseOutput {
	return o
}

func (o SparkRJobResponseOutput) ToSparkRJobResponseOutputWithContext(ctx context.Context) SparkRJobResponseOutput {
	return o
}

// Optional. HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
func (o SparkRJobResponseOutput) ArchiveUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJobResponse) []string { return v.ArchiveUris }).(pulumi.StringArrayOutput)
}

// Optional. The arguments to pass to the driver. Do not include arguments, such as --conf, that can be set as job properties, since a collision may occur that causes an incorrect job submission.
func (o SparkRJobResponseOutput) Args() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJobResponse) []string { return v.Args }).(pulumi.StringArrayOutput)
}

// Optional. HCFS URIs of files to be placed in the working directory of each executor. Useful for naively parallel tasks.
func (o SparkRJobResponseOutput) FileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkRJobResponse) []string { return v.FileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkRJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v SparkRJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// The HCFS URI of the main R file to use as the driver. Must be a .R file.
func (o SparkRJobResponseOutput) MainRFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkRJobResponse) string { return v.MainRFileUri }).(pulumi.StringOutput)
}

// Optional. A mapping of property names to values, used to configure SparkR. Properties that conflict with values set by the Dataproc API may be overwritten. Can include properties set in /etc/spark/conf/spark-defaults.conf and classes in user code.
func (o SparkRJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkRJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatch struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []string `pulumi:"jarFileUris"`
	// The HCFS URI of the script that contains Spark SQL queries to execute.
	QueryFileUri string `pulumi:"queryFileUri"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	QueryVariables map[string]string `pulumi:"queryVariables"`
}

// SparkSqlBatchInput is an input type that accepts SparkSqlBatchArgs and SparkSqlBatchOutput values.
// You can construct a concrete instance of `SparkSqlBatchInput` via:
//
//          SparkSqlBatchArgs{...}
type SparkSqlBatchInput interface {
	pulumi.Input

	ToSparkSqlBatchOutput() SparkSqlBatchOutput
	ToSparkSqlBatchOutputWithContext(context.Context) SparkSqlBatchOutput
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatchArgs struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// The HCFS URI of the script that contains Spark SQL queries to execute.
	QueryFileUri pulumi.StringInput `pulumi:"queryFileUri"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	QueryVariables pulumi.StringMapInput `pulumi:"queryVariables"`
}

func (SparkSqlBatchArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlBatch)(nil)).Elem()
}

func (i SparkSqlBatchArgs) ToSparkSqlBatchOutput() SparkSqlBatchOutput {
	return i.ToSparkSqlBatchOutputWithContext(context.Background())
}

func (i SparkSqlBatchArgs) ToSparkSqlBatchOutputWithContext(ctx context.Context) SparkSqlBatchOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlBatchOutput)
}

func (i SparkSqlBatchArgs) ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput {
	return i.ToSparkSqlBatchPtrOutputWithContext(context.Background())
}

func (i SparkSqlBatchArgs) ToSparkSqlBatchPtrOutputWithContext(ctx context.Context) SparkSqlBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlBatchOutput).ToSparkSqlBatchPtrOutputWithContext(ctx)
}

// SparkSqlBatchPtrInput is an input type that accepts SparkSqlBatchArgs, SparkSqlBatchPtr and SparkSqlBatchPtrOutput values.
// You can construct a concrete instance of `SparkSqlBatchPtrInput` via:
//
//          SparkSqlBatchArgs{...}
//
//  or:
//
//          nil
type SparkSqlBatchPtrInput interface {
	pulumi.Input

	ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput
	ToSparkSqlBatchPtrOutputWithContext(context.Context) SparkSqlBatchPtrOutput
}

type sparkSqlBatchPtrType SparkSqlBatchArgs

func SparkSqlBatchPtr(v *SparkSqlBatchArgs) SparkSqlBatchPtrInput {
	return (*sparkSqlBatchPtrType)(v)
}

func (*sparkSqlBatchPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkSqlBatch)(nil)).Elem()
}

func (i *sparkSqlBatchPtrType) ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput {
	return i.ToSparkSqlBatchPtrOutputWithContext(context.Background())
}

func (i *sparkSqlBatchPtrType) ToSparkSqlBatchPtrOutputWithContext(ctx context.Context) SparkSqlBatchPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlBatchPtrOutput)
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatchOutput struct{ *pulumi.OutputState }

func (SparkSqlBatchOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlBatch)(nil)).Elem()
}

func (o SparkSqlBatchOutput) ToSparkSqlBatchOutput() SparkSqlBatchOutput {
	return o
}

func (o SparkSqlBatchOutput) ToSparkSqlBatchOutputWithContext(ctx context.Context) SparkSqlBatchOutput {
	return o
}

func (o SparkSqlBatchOutput) ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput {
	return o.ToSparkSqlBatchPtrOutputWithContext(context.Background())
}

func (o SparkSqlBatchOutput) ToSparkSqlBatchPtrOutputWithContext(ctx context.Context) SparkSqlBatchPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkSqlBatch) *SparkSqlBatch {
		return &v
	}).(SparkSqlBatchPtrOutput)
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlBatchOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkSqlBatch) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the script that contains Spark SQL queries to execute.
func (o SparkSqlBatchOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkSqlBatch) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlBatchOutput) QueryVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlBatch) map[string]string { return v.QueryVariables }).(pulumi.StringMapOutput)
}

type SparkSqlBatchPtrOutput struct{ *pulumi.OutputState }

func (SparkSqlBatchPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkSqlBatch)(nil)).Elem()
}

func (o SparkSqlBatchPtrOutput) ToSparkSqlBatchPtrOutput() SparkSqlBatchPtrOutput {
	return o
}

func (o SparkSqlBatchPtrOutput) ToSparkSqlBatchPtrOutputWithContext(ctx context.Context) SparkSqlBatchPtrOutput {
	return o
}

func (o SparkSqlBatchPtrOutput) Elem() SparkSqlBatchOutput {
	return o.ApplyT(func(v *SparkSqlBatch) SparkSqlBatch {
		if v != nil {
			return *v
		}
		var ret SparkSqlBatch
		return ret
	}).(SparkSqlBatchOutput)
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlBatchPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkSqlBatch) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// The HCFS URI of the script that contains Spark SQL queries to execute.
func (o SparkSqlBatchPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkSqlBatch) *string {
		if v == nil {
			return nil
		}
		return &v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlBatchPtrOutput) QueryVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkSqlBatch) map[string]string {
		if v == nil {
			return nil
		}
		return v.QueryVariables
	}).(pulumi.StringMapOutput)
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatchResponse struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []string `pulumi:"jarFileUris"`
	// The HCFS URI of the script that contains Spark SQL queries to execute.
	QueryFileUri string `pulumi:"queryFileUri"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	QueryVariables map[string]string `pulumi:"queryVariables"`
}

// A configuration for running Apache Spark SQL (https://spark.apache.org/sql/) queries as a batch workload.
type SparkSqlBatchResponseOutput struct{ *pulumi.OutputState }

func (SparkSqlBatchResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlBatchResponse)(nil)).Elem()
}

func (o SparkSqlBatchResponseOutput) ToSparkSqlBatchResponseOutput() SparkSqlBatchResponseOutput {
	return o
}

func (o SparkSqlBatchResponseOutput) ToSparkSqlBatchResponseOutputWithContext(ctx context.Context) SparkSqlBatchResponseOutput {
	return o
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlBatchResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkSqlBatchResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// The HCFS URI of the script that contains Spark SQL queries to execute.
func (o SparkSqlBatchResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkSqlBatchResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlBatchResponseOutput) QueryVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlBatchResponse) map[string]string { return v.QueryVariables }).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJob struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig *LoggingConfig `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri *string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList *QueryList `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// SparkSqlJobInput is an input type that accepts SparkSqlJobArgs and SparkSqlJobOutput values.
// You can construct a concrete instance of `SparkSqlJobInput` via:
//
//          SparkSqlJobArgs{...}
type SparkSqlJobInput interface {
	pulumi.Input

	ToSparkSqlJobOutput() SparkSqlJobOutput
	ToSparkSqlJobOutputWithContext(context.Context) SparkSqlJobOutput
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJobArgs struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris pulumi.StringArrayInput `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigPtrInput `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
	Properties pulumi.StringMapInput `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri pulumi.StringPtrInput `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListPtrInput `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	ScriptVariables pulumi.StringMapInput `pulumi:"scriptVariables"`
}

func (SparkSqlJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlJob)(nil)).Elem()
}

func (i SparkSqlJobArgs) ToSparkSqlJobOutput() SparkSqlJobOutput {
	return i.ToSparkSqlJobOutputWithContext(context.Background())
}

func (i SparkSqlJobArgs) ToSparkSqlJobOutputWithContext(ctx context.Context) SparkSqlJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlJobOutput)
}

func (i SparkSqlJobArgs) ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput {
	return i.ToSparkSqlJobPtrOutputWithContext(context.Background())
}

func (i SparkSqlJobArgs) ToSparkSqlJobPtrOutputWithContext(ctx context.Context) SparkSqlJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlJobOutput).ToSparkSqlJobPtrOutputWithContext(ctx)
}

// SparkSqlJobPtrInput is an input type that accepts SparkSqlJobArgs, SparkSqlJobPtr and SparkSqlJobPtrOutput values.
// You can construct a concrete instance of `SparkSqlJobPtrInput` via:
//
//          SparkSqlJobArgs{...}
//
//  or:
//
//          nil
type SparkSqlJobPtrInput interface {
	pulumi.Input

	ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput
	ToSparkSqlJobPtrOutputWithContext(context.Context) SparkSqlJobPtrOutput
}

type sparkSqlJobPtrType SparkSqlJobArgs

func SparkSqlJobPtr(v *SparkSqlJobArgs) SparkSqlJobPtrInput {
	return (*sparkSqlJobPtrType)(v)
}

func (*sparkSqlJobPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkSqlJob)(nil)).Elem()
}

func (i *sparkSqlJobPtrType) ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput {
	return i.ToSparkSqlJobPtrOutputWithContext(context.Background())
}

func (i *sparkSqlJobPtrType) ToSparkSqlJobPtrOutputWithContext(ctx context.Context) SparkSqlJobPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkSqlJobPtrOutput)
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJobOutput struct{ *pulumi.OutputState }

func (SparkSqlJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlJob)(nil)).Elem()
}

func (o SparkSqlJobOutput) ToSparkSqlJobOutput() SparkSqlJobOutput {
	return o
}

func (o SparkSqlJobOutput) ToSparkSqlJobOutputWithContext(ctx context.Context) SparkSqlJobOutput {
	return o
}

func (o SparkSqlJobOutput) ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput {
	return o.ToSparkSqlJobPtrOutputWithContext(context.Background())
}

func (o SparkSqlJobOutput) ToSparkSqlJobPtrOutputWithContext(ctx context.Context) SparkSqlJobPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkSqlJob) *SparkSqlJob {
		return &v
	}).(SparkSqlJobPtrOutput)
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlJobOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkSqlJob) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkSqlJobOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v SparkSqlJob) *LoggingConfig { return v.LoggingConfig }).(LoggingConfigPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
func (o SparkSqlJobOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlJob) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o SparkSqlJobOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v SparkSqlJob) *string { return v.QueryFileUri }).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o SparkSqlJobOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v SparkSqlJob) *QueryList { return v.QueryList }).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlJobOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlJob) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

type SparkSqlJobPtrOutput struct{ *pulumi.OutputState }

func (SparkSqlJobPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkSqlJob)(nil)).Elem()
}

func (o SparkSqlJobPtrOutput) ToSparkSqlJobPtrOutput() SparkSqlJobPtrOutput {
	return o
}

func (o SparkSqlJobPtrOutput) ToSparkSqlJobPtrOutputWithContext(ctx context.Context) SparkSqlJobPtrOutput {
	return o
}

func (o SparkSqlJobPtrOutput) Elem() SparkSqlJobOutput {
	return o.ApplyT(func(v *SparkSqlJob) SparkSqlJob {
		if v != nil {
			return *v
		}
		var ret SparkSqlJob
		return ret
	}).(SparkSqlJobOutput)
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlJobPtrOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *SparkSqlJob) []string {
		if v == nil {
			return nil
		}
		return v.JarFileUris
	}).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkSqlJobPtrOutput) LoggingConfig() LoggingConfigPtrOutput {
	return o.ApplyT(func(v *SparkSqlJob) *LoggingConfig {
		if v == nil {
			return nil
		}
		return v.LoggingConfig
	}).(LoggingConfigPtrOutput)
}

// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
func (o SparkSqlJobPtrOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkSqlJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.Properties
	}).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o SparkSqlJobPtrOutput) QueryFileUri() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkSqlJob) *string {
		if v == nil {
			return nil
		}
		return v.QueryFileUri
	}).(pulumi.StringPtrOutput)
}

// A list of queries.
func (o SparkSqlJobPtrOutput) QueryList() QueryListPtrOutput {
	return o.ApplyT(func(v *SparkSqlJob) *QueryList {
		if v == nil {
			return nil
		}
		return v.QueryList
	}).(QueryListPtrOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlJobPtrOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v *SparkSqlJob) map[string]string {
		if v == nil {
			return nil
		}
		return v.ScriptVariables
	}).(pulumi.StringMapOutput)
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJobResponse struct {
	// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
	JarFileUris []string `pulumi:"jarFileUris"`
	// Optional. The runtime log config for job execution.
	LoggingConfig LoggingConfigResponse `pulumi:"loggingConfig"`
	// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
	Properties map[string]string `pulumi:"properties"`
	// The HCFS URI of the script that contains SQL queries.
	QueryFileUri string `pulumi:"queryFileUri"`
	// A list of queries.
	QueryList QueryListResponse `pulumi:"queryList"`
	// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
	ScriptVariables map[string]string `pulumi:"scriptVariables"`
}

// A Dataproc job for running Apache Spark SQL (https://spark.apache.org/sql/) queries.
type SparkSqlJobResponseOutput struct{ *pulumi.OutputState }

func (SparkSqlJobResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkSqlJobResponse)(nil)).Elem()
}

func (o SparkSqlJobResponseOutput) ToSparkSqlJobResponseOutput() SparkSqlJobResponseOutput {
	return o
}

func (o SparkSqlJobResponseOutput) ToSparkSqlJobResponseOutputWithContext(ctx context.Context) SparkSqlJobResponseOutput {
	return o
}

// Optional. HCFS URIs of jar files to be added to the Spark CLASSPATH.
func (o SparkSqlJobResponseOutput) JarFileUris() pulumi.StringArrayOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) []string { return v.JarFileUris }).(pulumi.StringArrayOutput)
}

// Optional. The runtime log config for job execution.
func (o SparkSqlJobResponseOutput) LoggingConfig() LoggingConfigResponseOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) LoggingConfigResponse { return v.LoggingConfig }).(LoggingConfigResponseOutput)
}

// Optional. A mapping of property names to values, used to configure Spark SQL's SparkConf. Properties that conflict with values set by the Dataproc API may be overwritten.
func (o SparkSqlJobResponseOutput) Properties() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) map[string]string { return v.Properties }).(pulumi.StringMapOutput)
}

// The HCFS URI of the script that contains SQL queries.
func (o SparkSqlJobResponseOutput) QueryFileUri() pulumi.StringOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) string { return v.QueryFileUri }).(pulumi.StringOutput)
}

// A list of queries.
func (o SparkSqlJobResponseOutput) QueryList() QueryListResponseOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) QueryListResponse { return v.QueryList }).(QueryListResponseOutput)
}

// Optional. Mapping of query variable names to values (equivalent to the Spark SQL command: SET name="value";).
func (o SparkSqlJobResponseOutput) ScriptVariables() pulumi.StringMapOutput {
	return o.ApplyT(func(v SparkSqlJobResponse) map[string]string { return v.ScriptVariables }).(pulumi.StringMapOutput)
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfig struct {
	// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decomissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout string `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleDownFactor float64 `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction *float64 `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleUpFactor float64 `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction *float64 `pulumi:"scaleUpMinWorkerFraction"`
}

// SparkStandaloneAutoscalingConfigInput is an input type that accepts SparkStandaloneAutoscalingConfigArgs and SparkStandaloneAutoscalingConfigOutput values.
// You can construct a concrete instance of `SparkStandaloneAutoscalingConfigInput` via:
//
//          SparkStandaloneAutoscalingConfigArgs{...}
type SparkStandaloneAutoscalingConfigInput interface {
	pulumi.Input

	ToSparkStandaloneAutoscalingConfigOutput() SparkStandaloneAutoscalingConfigOutput
	ToSparkStandaloneAutoscalingConfigOutputWithContext(context.Context) SparkStandaloneAutoscalingConfigOutput
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfigArgs struct {
	// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decomissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout pulumi.StringInput `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleDownFactor pulumi.Float64Input `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction pulumi.Float64PtrInput `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleUpFactor pulumi.Float64Input `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction pulumi.Float64PtrInput `pulumi:"scaleUpMinWorkerFraction"`
}

func (SparkStandaloneAutoscalingConfigArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkStandaloneAutoscalingConfig)(nil)).Elem()
}

func (i SparkStandaloneAutoscalingConfigArgs) ToSparkStandaloneAutoscalingConfigOutput() SparkStandaloneAutoscalingConfigOutput {
	return i.ToSparkStandaloneAutoscalingConfigOutputWithContext(context.Background())
}

func (i SparkStandaloneAutoscalingConfigArgs) ToSparkStandaloneAutoscalingConfigOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkStandaloneAutoscalingConfigOutput)
}

func (i SparkStandaloneAutoscalingConfigArgs) ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput {
	return i.ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i SparkStandaloneAutoscalingConfigArgs) ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkStandaloneAutoscalingConfigOutput).ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx)
}

// SparkStandaloneAutoscalingConfigPtrInput is an input type that accepts SparkStandaloneAutoscalingConfigArgs, SparkStandaloneAutoscalingConfigPtr and SparkStandaloneAutoscalingConfigPtrOutput values.
// You can construct a concrete instance of `SparkStandaloneAutoscalingConfigPtrInput` via:
//
//          SparkStandaloneAutoscalingConfigArgs{...}
//
//  or:
//
//          nil
type SparkStandaloneAutoscalingConfigPtrInput interface {
	pulumi.Input

	ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput
	ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(context.Context) SparkStandaloneAutoscalingConfigPtrOutput
}

type sparkStandaloneAutoscalingConfigPtrType SparkStandaloneAutoscalingConfigArgs

func SparkStandaloneAutoscalingConfigPtr(v *SparkStandaloneAutoscalingConfigArgs) SparkStandaloneAutoscalingConfigPtrInput {
	return (*sparkStandaloneAutoscalingConfigPtrType)(v)
}

func (*sparkStandaloneAutoscalingConfigPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkStandaloneAutoscalingConfig)(nil)).Elem()
}

func (i *sparkStandaloneAutoscalingConfigPtrType) ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput {
	return i.ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (i *sparkStandaloneAutoscalingConfigPtrType) ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(SparkStandaloneAutoscalingConfigPtrOutput)
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfigOutput struct{ *pulumi.OutputState }

func (SparkStandaloneAutoscalingConfigOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkStandaloneAutoscalingConfig)(nil)).Elem()
}

func (o SparkStandaloneAutoscalingConfigOutput) ToSparkStandaloneAutoscalingConfigOutput() SparkStandaloneAutoscalingConfigOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigOutput) ToSparkStandaloneAutoscalingConfigOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigOutput) ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput {
	return o.ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(context.Background())
}

func (o SparkStandaloneAutoscalingConfigOutput) ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v SparkStandaloneAutoscalingConfig) *SparkStandaloneAutoscalingConfig {
		return &v
	}).(SparkStandaloneAutoscalingConfigPtrOutput)
}

// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decomissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o SparkStandaloneAutoscalingConfigOutput) GracefulDecommissionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) string { return v.GracefulDecommissionTimeout }).(pulumi.StringOutput)
}

// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigOutput) ScaleDownFactor() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) float64 { return v.ScaleDownFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigOutput) ScaleDownMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) *float64 { return v.ScaleDownMinWorkerFraction }).(pulumi.Float64PtrOutput)
}

// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigOutput) ScaleUpFactor() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) float64 { return v.ScaleUpFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigOutput) ScaleUpMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfig) *float64 { return v.ScaleUpMinWorkerFraction }).(pulumi.Float64PtrOutput)
}

type SparkStandaloneAutoscalingConfigPtrOutput struct{ *pulumi.OutputState }

func (SparkStandaloneAutoscalingConfigPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**SparkStandaloneAutoscalingConfig)(nil)).Elem()
}

func (o SparkStandaloneAutoscalingConfigPtrOutput) ToSparkStandaloneAutoscalingConfigPtrOutput() SparkStandaloneAutoscalingConfigPtrOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigPtrOutput) ToSparkStandaloneAutoscalingConfigPtrOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigPtrOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigPtrOutput) Elem() SparkStandaloneAutoscalingConfigOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) SparkStandaloneAutoscalingConfig {
		if v != nil {
			return *v
		}
		var ret SparkStandaloneAutoscalingConfig
		return ret
	}).(SparkStandaloneAutoscalingConfigOutput)
}

// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decomissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o SparkStandaloneAutoscalingConfigPtrOutput) GracefulDecommissionTimeout() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *string {
		if v == nil {
			return nil
		}
		return &v.GracefulDecommissionTimeout
	}).(pulumi.StringPtrOutput)
}

// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigPtrOutput) ScaleDownFactor() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return &v.ScaleDownFactor
	}).(pulumi.Float64PtrOutput)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigPtrOutput) ScaleDownMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return v.ScaleDownMinWorkerFraction
	}).(pulumi.Float64PtrOutput)
}

// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigPtrOutput) ScaleUpFactor() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return &v.ScaleUpFactor
	}).(pulumi.Float64PtrOutput)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigPtrOutput) ScaleUpMinWorkerFraction() pulumi.Float64PtrOutput {
	return o.ApplyT(func(v *SparkStandaloneAutoscalingConfig) *float64 {
		if v == nil {
			return nil
		}
		return v.ScaleUpMinWorkerFraction
	}).(pulumi.Float64PtrOutput)
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfigResponse struct {
	// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decomissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
	GracefulDecommissionTimeout string `pulumi:"gracefulDecommissionTimeout"`
	// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleDownFactor float64 `pulumi:"scaleDownFactor"`
	// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleDownMinWorkerFraction float64 `pulumi:"scaleDownMinWorkerFraction"`
	// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
	ScaleUpFactor float64 `pulumi:"scaleUpFactor"`
	// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
	ScaleUpMinWorkerFraction float64 `pulumi:"scaleUpMinWorkerFraction"`
}

// Basic autoscaling configurations for Spark Standalone.
type SparkStandaloneAutoscalingConfigResponseOutput struct{ *pulumi.OutputState }

func (SparkStandaloneAutoscalingConfigResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*SparkStandaloneAutoscalingConfigResponse)(nil)).Elem()
}

func (o SparkStandaloneAutoscalingConfigResponseOutput) ToSparkStandaloneAutoscalingConfigResponseOutput() SparkStandaloneAutoscalingConfigResponseOutput {
	return o
}

func (o SparkStandaloneAutoscalingConfigResponseOutput) ToSparkStandaloneAutoscalingConfigResponseOutputWithContext(ctx context.Context) SparkStandaloneAutoscalingConfigResponseOutput {
	return o
}

// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decomissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
func (o SparkStandaloneAutoscalingConfigResponseOutput) GracefulDecommissionTimeout() pulumi.StringOutput {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) string { return v.GracefulDecommissionTimeout }).(pulumi.StringOutput)
}

// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigResponseOutput) ScaleDownFactor() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) float64 { return v.ScaleDownFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigResponseOutput) ScaleDownMinWorkerFraction() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) float64 { return v.ScaleDownMinWorkerFraction }).(pulumi.Float64Output)
}

// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
func (o SparkStandaloneAutoscalingConfigResponseOutput) ScaleUpFactor() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) float64 { return v.ScaleUpFactor }).(pulumi.Float64Output)
}

// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
func (o SparkStandaloneAutoscalingConfigResponseOutput) ScaleUpMinWorkerFraction() pulumi.Float64Output {
	return o.ApplyT(func(v SparkStandaloneAutoscalingConfigResponse) float64 { return v.ScaleUpMinWorkerFraction }).(pulumi.Float64Output)
}

// Historical state information.
type StateHistoryResponse struct {
	// The state of the batch at this point in history.
	State string `pulumi:"state"`
	// Details about the state at this point in history.
	StateMessage string `pulumi:"stateMessage"`
	// The time when the batch entered the historical state.
	StateStartTime string `pulumi:"stateStartTime"`
}

// Historical state information.
type StateHistoryResponseOutput struct{ *pulumi.OutputState }

func (StateHistoryResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*StateHistoryResponse)(nil)).Elem()
}

func (o StateHistoryResponseOutput) ToStateHistoryResponseOutput() StateHistoryResponseOutput {
	return o
}

func (o StateHistoryResponseOutput) ToStateHistoryResponseOutputWithContext(ctx context.Context) StateHistoryResponseOutput {
	return o
}

// The state of the batch at this point in history.
func (o StateHistoryResponseOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v StateHistoryResponse) string { return v.State }).(pulumi.StringOutput)
}

// Details about the state at this point in history.
func (o StateHistoryResponseOutput) StateMessage() pulumi.StringOutput {
	return o.ApplyT(func(v StateHistoryResponse) string { return v.StateMessage }).(pulumi.StringOutput)
}

// The time when the batch entered the historical state.
func (o StateHistoryResponseOutput) StateStartTime() pulumi.StringOutput {
	return o.ApplyT(func(v StateHistoryResponse) string { return v.StateStartTime }).(pulumi.StringOutput)
}

type StateHistoryResponseArrayOutput struct{ *pulumi.OutputState }

func (StateHistoryResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]StateHistoryResponse)(nil)).Elem()
}

func (o StateHistoryResponseArrayOutput) ToStateHistoryResponseArrayOutput() StateHistoryResponseArrayOutput {
	return o
}

func (o StateHistoryResponseArrayOutput) ToStateHistoryResponseArrayOutputWithContext(ctx context.Context) StateHistoryResponseArrayOutput {
	return o
}

func (o StateHistoryResponseArrayOutput) Index(i pulumi.IntInput) StateHistoryResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) StateHistoryResponse {
		return vs[0].([]StateHistoryResponse)[vs[1].(int)]
	}).(StateHistoryResponseOutput)
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameter struct {
	// Optional. Brief description of the parameter. Must not exceed 1024 characters.
	Description *string `pulumi:"description"`
	// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
	Fields []string `pulumi:"fields"`
	// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
	Name string `pulumi:"name"`
	// Optional. Validation rules to be applied to this parameter's value.
	Validation *ParameterValidation `pulumi:"validation"`
}

// TemplateParameterInput is an input type that accepts TemplateParameterArgs and TemplateParameterOutput values.
// You can construct a concrete instance of `TemplateParameterInput` via:
//
//          TemplateParameterArgs{...}
type TemplateParameterInput interface {
	pulumi.Input

	ToTemplateParameterOutput() TemplateParameterOutput
	ToTemplateParameterOutputWithContext(context.Context) TemplateParameterOutput
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameterArgs struct {
	// Optional. Brief description of the parameter. Must not exceed 1024 characters.
	Description pulumi.StringPtrInput `pulumi:"description"`
	// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
	Fields pulumi.StringArrayInput `pulumi:"fields"`
	// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
	Name pulumi.StringInput `pulumi:"name"`
	// Optional. Validation rules to be applied to this parameter's value.
	Validation ParameterValidationPtrInput `pulumi:"validation"`
}

func (TemplateParameterArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*TemplateParameter)(nil)).Elem()
}

func (i TemplateParameterArgs) ToTemplateParameterOutput() TemplateParameterOutput {
	return i.ToTemplateParameterOutputWithContext(context.Background())
}

func (i TemplateParameterArgs) ToTemplateParameterOutputWithContext(ctx context.Context) TemplateParameterOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TemplateParameterOutput)
}

// TemplateParameterArrayInput is an input type that accepts TemplateParameterArray and TemplateParameterArrayOutput values.
// You can construct a concrete instance of `TemplateParameterArrayInput` via:
//
//          TemplateParameterArray{ TemplateParameterArgs{...} }
type TemplateParameterArrayInput interface {
	pulumi.Input

	ToTemplateParameterArrayOutput() TemplateParameterArrayOutput
	ToTemplateParameterArrayOutputWithContext(context.Context) TemplateParameterArrayOutput
}

type TemplateParameterArray []TemplateParameterInput

func (TemplateParameterArray) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TemplateParameter)(nil)).Elem()
}

func (i TemplateParameterArray) ToTemplateParameterArrayOutput() TemplateParameterArrayOutput {
	return i.ToTemplateParameterArrayOutputWithContext(context.Background())
}

func (i TemplateParameterArray) ToTemplateParameterArrayOutputWithContext(ctx context.Context) TemplateParameterArrayOutput {
	return pulumi.ToOutputWithContext(ctx, i).(TemplateParameterArrayOutput)
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameterOutput struct{ *pulumi.OutputState }

func (TemplateParameterOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*TemplateParameter)(nil)).Elem()
}

func (o TemplateParameterOutput) ToTemplateParameterOutput() TemplateParameterOutput {
	return o
}

func (o TemplateParameterOutput) ToTemplateParameterOutputWithContext(ctx context.Context) TemplateParameterOutput {
	return o
}

// Optional. Brief description of the parameter. Must not exceed 1024 characters.
func (o TemplateParameterOutput) Description() pulumi.StringPtrOutput {
	return o.ApplyT(func(v TemplateParameter) *string { return v.Description }).(pulumi.StringPtrOutput)
}

// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
func (o TemplateParameterOutput) Fields() pulumi.StringArrayOutput {
	return o.ApplyT(func(v TemplateParameter) []string { return v.Fields }).(pulumi.StringArrayOutput)
}

// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
func (o TemplateParameterOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v TemplateParameter) string { return v.Name }).(pulumi.StringOutput)
}

// Optional. Validation rules to be applied to this parameter's value.
func (o TemplateParameterOutput) Validation() ParameterValidationPtrOutput {
	return o.ApplyT(func(v TemplateParameter) *ParameterValidation { return v.Validation }).(ParameterValidationPtrOutput)
}

type TemplateParameterArrayOutput struct{ *pulumi.OutputState }

func (TemplateParameterArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TemplateParameter)(nil)).Elem()
}

func (o TemplateParameterArrayOutput) ToTemplateParameterArrayOutput() TemplateParameterArrayOutput {
	return o
}

func (o TemplateParameterArrayOutput) ToTemplateParameterArrayOutputWithContext(ctx context.Context) TemplateParameterArrayOutput {
	return o
}

func (o TemplateParameterArrayOutput) Index(i pulumi.IntInput) TemplateParameterOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) TemplateParameter {
		return vs[0].([]TemplateParameter)[vs[1].(int)]
	}).(TemplateParameterOutput)
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameterResponse struct {
	// Optional. Brief description of the parameter. Must not exceed 1024 characters.
	Description string `pulumi:"description"`
	// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
	Fields []string `pulumi:"fields"`
	// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
	Name string `pulumi:"name"`
	// Optional. Validation rules to be applied to this parameter's value.
	Validation ParameterValidationResponse `pulumi:"validation"`
}

// A configurable parameter that replaces one or more fields in the template. Parameterizable fields: - Labels - File uris - Job properties - Job arguments - Script variables - Main class (in HadoopJob and SparkJob) - Zone (in ClusterSelector)
type TemplateParameterResponseOutput struct{ *pulumi.OutputState }

func (TemplateParameterResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*TemplateParameterResponse)(nil)).Elem()
}

func (o TemplateParameterResponseOutput) ToTemplateParameterResponseOutput() TemplateParameterResponseOutput {
	return o
}

func (o TemplateParameterResponseOutput) ToTemplateParameterResponseOutputWithContext(ctx context.Context) TemplateParameterResponseOutput {
	return o
}

// Optional. Brief description of the parameter. Must not exceed 1024 characters.
func (o TemplateParameterResponseOutput) Description() pulumi.StringOutput {
	return o.ApplyT(func(v TemplateParameterResponse) string { return v.Description }).(pulumi.StringOutput)
}

// Paths to all fields that the parameter replaces. A field is allowed to appear in at most one parameter's list of field paths.A field path is similar in syntax to a google.protobuf.FieldMask. For example, a field path that references the zone field of a workflow template's cluster selector would be specified as placement.clusterSelector.zone.Also, field paths can reference fields using the following syntax: Values in maps can be referenced by key: labels'key' placement.clusterSelector.clusterLabels'key' placement.managedCluster.labels'key' placement.clusterSelector.clusterLabels'key' jobs'step-id'.labels'key' Jobs in the jobs list can be referenced by step-id: jobs'step-id'.hadoopJob.mainJarFileUri jobs'step-id'.hiveJob.queryFileUri jobs'step-id'.pySparkJob.mainPythonFileUri jobs'step-id'.hadoopJob.jarFileUris0 jobs'step-id'.hadoopJob.archiveUris0 jobs'step-id'.hadoopJob.fileUris0 jobs'step-id'.pySparkJob.pythonFileUris0 Items in repeated fields can be referenced by a zero-based index: jobs'step-id'.sparkJob.args0 Other examples: jobs'step-id'.hadoopJob.properties'key' jobs'step-id'.hadoopJob.args0 jobs'step-id'.hiveJob.scriptVariables'key' jobs'step-id'.hadoopJob.mainJarFileUri placement.clusterSelector.zoneIt may not be possible to parameterize maps and repeated fields in their entirety since only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid: placement.clusterSelector.clusterLabels jobs'step-id'.sparkJob.args
func (o TemplateParameterResponseOutput) Fields() pulumi.StringArrayOutput {
	return o.ApplyT(func(v TemplateParameterResponse) []string { return v.Fields }).(pulumi.StringArrayOutput)
}

// Parameter name. The parameter name is used as the key, and paired with the parameter value, which are passed to the template when the template is instantiated. The name must contain only capital letters (A-Z), numbers (0-9), and underscores (_), and must not start with a number. The maximum length is 40 characters.
func (o TemplateParameterResponseOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v TemplateParameterResponse) string { return v.Name }).(pulumi.StringOutput)
}

// Optional. Validation rules to be applied to this parameter's value.
func (o TemplateParameterResponseOutput) Validation() ParameterValidationResponseOutput {
	return o.ApplyT(func(v TemplateParameterResponse) ParameterValidationResponse { return v.Validation }).(ParameterValidationResponseOutput)
}

type TemplateParameterResponseArrayOutput struct{ *pulumi.OutputState }

func (TemplateParameterResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]TemplateParameterResponse)(nil)).Elem()
}

func (o TemplateParameterResponseArrayOutput) ToTemplateParameterResponseArrayOutput() TemplateParameterResponseArrayOutput {
	return o
}

func (o TemplateParameterResponseArrayOutput) ToTemplateParameterResponseArrayOutputWithContext(ctx context.Context) TemplateParameterResponseArrayOutput {
	return o
}

func (o TemplateParameterResponseArrayOutput) Index(i pulumi.IntInput) TemplateParameterResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) TemplateParameterResponse {
		return vs[0].([]TemplateParameterResponse)[vs[1].(int)]
	}).(TemplateParameterResponseOutput)
}

// Validation based on a list of allowed values.
type ValueValidation struct {
	// List of allowed values for the parameter.
	Values []string `pulumi:"values"`
}

// ValueValidationInput is an input type that accepts ValueValidationArgs and ValueValidationOutput values.
// You can construct a concrete instance of `ValueValidationInput` via:
//
//          ValueValidationArgs{...}
type ValueValidationInput interface {
	pulumi.Input

	ToValueValidationOutput() ValueValidationOutput
	ToValueValidationOutputWithContext(context.Context) ValueValidationOutput
}

// Validation based on a list of allowed values.
type ValueValidationArgs struct {
	// List of allowed values for the parameter.
	Values pulumi.StringArrayInput `pulumi:"values"`
}

func (ValueValidationArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*ValueValidation)(nil)).Elem()
}

func (i ValueValidationArgs) ToValueValidationOutput() ValueValidationOutput {
	return i.ToValueValidationOutputWithContext(context.Background())
}

func (i ValueValidationArgs) ToValueValidationOutputWithContext(ctx context.Context) ValueValidationOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ValueValidationOutput)
}

func (i ValueValidationArgs) ToValueValidationPtrOutput() ValueValidationPtrOutput {
	return i.ToValueValidationPtrOutputWithContext(context.Background())
}

func (i ValueValidationArgs) ToValueValidationPtrOutputWithContext(ctx context.Context) ValueValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ValueValidationOutput).ToValueValidationPtrOutputWithContext(ctx)
}

// ValueValidationPtrInput is an input type that accepts ValueValidationArgs, ValueValidationPtr and ValueValidationPtrOutput values.
// You can construct a concrete instance of `ValueValidationPtrInput` via:
//
//          ValueValidationArgs{...}
//
//  or:
//
//          nil
type ValueValidationPtrInput interface {
	pulumi.Input

	ToValueValidationPtrOutput() ValueValidationPtrOutput
	ToValueValidationPtrOutputWithContext(context.Context) ValueValidationPtrOutput
}

type valueValidationPtrType ValueValidationArgs

func ValueValidationPtr(v *ValueValidationArgs) ValueValidationPtrInput {
	return (*valueValidationPtrType)(v)
}

func (*valueValidationPtrType) ElementType() reflect.Type {
	return reflect.TypeOf((**ValueValidation)(nil)).Elem()
}

func (i *valueValidationPtrType) ToValueValidationPtrOutput() ValueValidationPtrOutput {
	return i.ToValueValidationPtrOutputWithContext(context.Background())
}

func (i *valueValidationPtrType) ToValueValidationPtrOutputWithContext(ctx context.Context) ValueValidationPtrOutput {
	return pulumi.ToOutputWithContext(ctx, i).(ValueValidationPtrOutput)
}

// Validation based on a list of allowed values.
type ValueValidationOutput struct{ *pulumi.OutputState }

func (ValueValidationOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ValueValidation)(nil)).Elem()
}

func (o ValueValidationOutput) ToValueValidationOutput() ValueValidationOutput {
	return o
}

func (o ValueValidationOutput) ToValueValidationOutputWithContext(ctx context.Context) ValueValidationOutput {
	return o
}

func (o ValueValidationOutput) ToValueValidationPtrOutput() ValueValidationPtrOutput {
	return o.ToValueValidationPtrOutputWithContext(context.Background())
}

func (o ValueValidationOutput) ToValueValidationPtrOutputWithContext(ctx context.Context) ValueValidationPtrOutput {
	return o.ApplyTWithContext(ctx, func(_ context.Context, v ValueValidation) *ValueValidation {
		return &v
	}).(ValueValidationPtrOutput)
}

// List of allowed values for the parameter.
func (o ValueValidationOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ValueValidation) []string { return v.Values }).(pulumi.StringArrayOutput)
}

type ValueValidationPtrOutput struct{ *pulumi.OutputState }

func (ValueValidationPtrOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**ValueValidation)(nil)).Elem()
}

func (o ValueValidationPtrOutput) ToValueValidationPtrOutput() ValueValidationPtrOutput {
	return o
}

func (o ValueValidationPtrOutput) ToValueValidationPtrOutputWithContext(ctx context.Context) ValueValidationPtrOutput {
	return o
}

func (o ValueValidationPtrOutput) Elem() ValueValidationOutput {
	return o.ApplyT(func(v *ValueValidation) ValueValidation {
		if v != nil {
			return *v
		}
		var ret ValueValidation
		return ret
	}).(ValueValidationOutput)
}

// List of allowed values for the parameter.
func (o ValueValidationPtrOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v *ValueValidation) []string {
		if v == nil {
			return nil
		}
		return v.Values
	}).(pulumi.StringArrayOutput)
}

// Validation based on a list of allowed values.
type ValueValidationResponse struct {
	// List of allowed values for the parameter.
	Values []string `pulumi:"values"`
}

// Validation based on a list of allowed values.
type ValueValidationResponseOutput struct{ *pulumi.OutputState }

func (ValueValidationResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*ValueValidationResponse)(nil)).Elem()
}

func (o ValueValidationResponseOutput) ToValueValidationResponseOutput() ValueValidationResponseOutput {
	return o
}

func (o ValueValidationResponseOutput) ToValueValidationResponseOutputWithContext(ctx context.Context) ValueValidationResponseOutput {
	return o
}

// List of allowed values for the parameter.
func (o ValueValidationResponseOutput) Values() pulumi.StringArrayOutput {
	return o.ApplyT(func(v ValueValidationResponse) []string { return v.Values }).(pulumi.StringArrayOutput)
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacement struct {
	// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
	ClusterSelector *ClusterSelector `pulumi:"clusterSelector"`
	// A cluster that is managed by the workflow.
	ManagedCluster *ManagedCluster `pulumi:"managedCluster"`
}

// WorkflowTemplatePlacementInput is an input type that accepts WorkflowTemplatePlacementArgs and WorkflowTemplatePlacementOutput values.
// You can construct a concrete instance of `WorkflowTemplatePlacementInput` via:
//
//          WorkflowTemplatePlacementArgs{...}
type WorkflowTemplatePlacementInput interface {
	pulumi.Input

	ToWorkflowTemplatePlacementOutput() WorkflowTemplatePlacementOutput
	ToWorkflowTemplatePlacementOutputWithContext(context.Context) WorkflowTemplatePlacementOutput
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacementArgs struct {
	// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
	ClusterSelector ClusterSelectorPtrInput `pulumi:"clusterSelector"`
	// A cluster that is managed by the workflow.
	ManagedCluster ManagedClusterPtrInput `pulumi:"managedCluster"`
}

func (WorkflowTemplatePlacementArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*WorkflowTemplatePlacement)(nil)).Elem()
}

func (i WorkflowTemplatePlacementArgs) ToWorkflowTemplatePlacementOutput() WorkflowTemplatePlacementOutput {
	return i.ToWorkflowTemplatePlacementOutputWithContext(context.Background())
}

func (i WorkflowTemplatePlacementArgs) ToWorkflowTemplatePlacementOutputWithContext(ctx context.Context) WorkflowTemplatePlacementOutput {
	return pulumi.ToOutputWithContext(ctx, i).(WorkflowTemplatePlacementOutput)
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacementOutput struct{ *pulumi.OutputState }

func (WorkflowTemplatePlacementOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*WorkflowTemplatePlacement)(nil)).Elem()
}

func (o WorkflowTemplatePlacementOutput) ToWorkflowTemplatePlacementOutput() WorkflowTemplatePlacementOutput {
	return o
}

func (o WorkflowTemplatePlacementOutput) ToWorkflowTemplatePlacementOutputWithContext(ctx context.Context) WorkflowTemplatePlacementOutput {
	return o
}

// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
func (o WorkflowTemplatePlacementOutput) ClusterSelector() ClusterSelectorPtrOutput {
	return o.ApplyT(func(v WorkflowTemplatePlacement) *ClusterSelector { return v.ClusterSelector }).(ClusterSelectorPtrOutput)
}

// A cluster that is managed by the workflow.
func (o WorkflowTemplatePlacementOutput) ManagedCluster() ManagedClusterPtrOutput {
	return o.ApplyT(func(v WorkflowTemplatePlacement) *ManagedCluster { return v.ManagedCluster }).(ManagedClusterPtrOutput)
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacementResponse struct {
	// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
	ClusterSelector ClusterSelectorResponse `pulumi:"clusterSelector"`
	// A cluster that is managed by the workflow.
	ManagedCluster ManagedClusterResponse `pulumi:"managedCluster"`
}

// Specifies workflow execution target.Either managed_cluster or cluster_selector is required.
type WorkflowTemplatePlacementResponseOutput struct{ *pulumi.OutputState }

func (WorkflowTemplatePlacementResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*WorkflowTemplatePlacementResponse)(nil)).Elem()
}

func (o WorkflowTemplatePlacementResponseOutput) ToWorkflowTemplatePlacementResponseOutput() WorkflowTemplatePlacementResponseOutput {
	return o
}

func (o WorkflowTemplatePlacementResponseOutput) ToWorkflowTemplatePlacementResponseOutputWithContext(ctx context.Context) WorkflowTemplatePlacementResponseOutput {
	return o
}

// Optional. A selector that chooses target cluster for jobs based on metadata.The selector is evaluated at the time each job is submitted.
func (o WorkflowTemplatePlacementResponseOutput) ClusterSelector() ClusterSelectorResponseOutput {
	return o.ApplyT(func(v WorkflowTemplatePlacementResponse) ClusterSelectorResponse { return v.ClusterSelector }).(ClusterSelectorResponseOutput)
}

// A cluster that is managed by the workflow.
func (o WorkflowTemplatePlacementResponseOutput) ManagedCluster() ManagedClusterResponseOutput {
	return o.ApplyT(func(v WorkflowTemplatePlacementResponse) ManagedClusterResponse { return v.ManagedCluster }).(ManagedClusterResponseOutput)
}

// A YARN application created by a job. Application information is a subset of org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Beta Feature: This report is available for testing purposes only. It may be changed before final release.
type YarnApplicationResponse struct {
	// The application name.
	Name string `pulumi:"name"`
	// The numerical progress of the application, from 1 to 100.
	Progress float64 `pulumi:"progress"`
	// The application state.
	State string `pulumi:"state"`
	// Optional. The HTTP URL of the ApplicationMaster, HistoryServer, or TimelineServer that provides application-specific information. The URL uses the internal hostname, and requires a proxy server for resolution and, possibly, access.
	TrackingUrl string `pulumi:"trackingUrl"`
}

// A YARN application created by a job. Application information is a subset of org.apache.hadoop.yarn.proto.YarnProtos.ApplicationReportProto.Beta Feature: This report is available for testing purposes only. It may be changed before final release.
type YarnApplicationResponseOutput struct{ *pulumi.OutputState }

func (YarnApplicationResponseOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*YarnApplicationResponse)(nil)).Elem()
}

func (o YarnApplicationResponseOutput) ToYarnApplicationResponseOutput() YarnApplicationResponseOutput {
	return o
}

func (o YarnApplicationResponseOutput) ToYarnApplicationResponseOutputWithContext(ctx context.Context) YarnApplicationResponseOutput {
	return o
}

// The application name.
func (o YarnApplicationResponseOutput) Name() pulumi.StringOutput {
	return o.ApplyT(func(v YarnApplicationResponse) string { return v.Name }).(pulumi.StringOutput)
}

// The numerical progress of the application, from 1 to 100.
func (o YarnApplicationResponseOutput) Progress() pulumi.Float64Output {
	return o.ApplyT(func(v YarnApplicationResponse) float64 { return v.Progress }).(pulumi.Float64Output)
}

// The application state.
func (o YarnApplicationResponseOutput) State() pulumi.StringOutput {
	return o.ApplyT(func(v YarnApplicationResponse) string { return v.State }).(pulumi.StringOutput)
}

// Optional. The HTTP URL of the ApplicationMaster, HistoryServer, or TimelineServer that provides application-specific information. The URL uses the internal hostname, and requires a proxy server for resolution and, possibly, access.
func (o YarnApplicationResponseOutput) TrackingUrl() pulumi.StringOutput {
	return o.ApplyT(func(v YarnApplicationResponse) string { return v.TrackingUrl }).(pulumi.StringOutput)
}

type YarnApplicationResponseArrayOutput struct{ *pulumi.OutputState }

func (YarnApplicationResponseArrayOutput) ElementType() reflect.Type {
	return reflect.TypeOf((*[]YarnApplicationResponse)(nil)).Elem()
}

func (o YarnApplicationResponseArrayOutput) ToYarnApplicationResponseArrayOutput() YarnApplicationResponseArrayOutput {
	return o
}

func (o YarnApplicationResponseArrayOutput) ToYarnApplicationResponseArrayOutputWithContext(ctx context.Context) YarnApplicationResponseArrayOutput {
	return o
}

func (o YarnApplicationResponseArrayOutput) Index(i pulumi.IntInput) YarnApplicationResponseOutput {
	return pulumi.All(o, i).ApplyT(func(vs []interface{}) YarnApplicationResponse {
		return vs[0].([]YarnApplicationResponse)[vs[1].(int)]
	}).(YarnApplicationResponseOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*AcceleratorConfigInput)(nil)).Elem(), AcceleratorConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*AcceleratorConfigArrayInput)(nil)).Elem(), AcceleratorConfigArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*AutoscalingConfigInput)(nil)).Elem(), AutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*AutoscalingConfigPtrInput)(nil)).Elem(), AutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BasicAutoscalingAlgorithmInput)(nil)).Elem(), BasicAutoscalingAlgorithmArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BasicAutoscalingAlgorithmPtrInput)(nil)).Elem(), BasicAutoscalingAlgorithmArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BasicYarnAutoscalingConfigInput)(nil)).Elem(), BasicYarnAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BasicYarnAutoscalingConfigPtrInput)(nil)).Elem(), BasicYarnAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BindingInput)(nil)).Elem(), BindingArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*BindingArrayInput)(nil)).Elem(), BindingArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterConfigInput)(nil)).Elem(), ClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterConfigPtrInput)(nil)).Elem(), ClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterSelectorInput)(nil)).Elem(), ClusterSelectorArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ClusterSelectorPtrInput)(nil)).Elem(), ClusterSelectorArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ConfidentialInstanceConfigInput)(nil)).Elem(), ConfidentialInstanceConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ConfidentialInstanceConfigPtrInput)(nil)).Elem(), ConfidentialInstanceConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*DiskConfigInput)(nil)).Elem(), DiskConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*DiskConfigPtrInput)(nil)).Elem(), DiskConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EncryptionConfigInput)(nil)).Elem(), EncryptionConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EncryptionConfigPtrInput)(nil)).Elem(), EncryptionConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EndpointConfigInput)(nil)).Elem(), EndpointConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EndpointConfigPtrInput)(nil)).Elem(), EndpointConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EnvironmentConfigInput)(nil)).Elem(), EnvironmentConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*EnvironmentConfigPtrInput)(nil)).Elem(), EnvironmentConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ExecutionConfigInput)(nil)).Elem(), ExecutionConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ExecutionConfigPtrInput)(nil)).Elem(), ExecutionConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ExprInput)(nil)).Elem(), ExprArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ExprPtrInput)(nil)).Elem(), ExprArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GceClusterConfigInput)(nil)).Elem(), GceClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GceClusterConfigPtrInput)(nil)).Elem(), GceClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeClusterConfigInput)(nil)).Elem(), GkeClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*GkeClusterConfigPtrInput)(nil)).Elem(), GkeClusterConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*HadoopJobInput)(nil)).Elem(), HadoopJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*HadoopJobPtrInput)(nil)).Elem(), HadoopJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*HiveJobInput)(nil)).Elem(), HiveJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*HiveJobPtrInput)(nil)).Elem(), HiveJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*IdentityConfigInput)(nil)).Elem(), IdentityConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*IdentityConfigPtrInput)(nil)).Elem(), IdentityConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfigInput)(nil)).Elem(), InstanceGroupAutoscalingPolicyConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*InstanceGroupAutoscalingPolicyConfigPtrInput)(nil)).Elem(), InstanceGroupAutoscalingPolicyConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*InstanceGroupConfigInput)(nil)).Elem(), InstanceGroupConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*InstanceGroupConfigPtrInput)(nil)).Elem(), InstanceGroupConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobPlacementInput)(nil)).Elem(), JobPlacementArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobReferenceInput)(nil)).Elem(), JobReferenceArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobReferencePtrInput)(nil)).Elem(), JobReferenceArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobSchedulingInput)(nil)).Elem(), JobSchedulingArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*JobSchedulingPtrInput)(nil)).Elem(), JobSchedulingArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*KerberosConfigInput)(nil)).Elem(), KerberosConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*KerberosConfigPtrInput)(nil)).Elem(), KerberosConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*LifecycleConfigInput)(nil)).Elem(), LifecycleConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*LifecycleConfigPtrInput)(nil)).Elem(), LifecycleConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*LoggingConfigInput)(nil)).Elem(), LoggingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*LoggingConfigPtrInput)(nil)).Elem(), LoggingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ManagedClusterInput)(nil)).Elem(), ManagedClusterArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ManagedClusterPtrInput)(nil)).Elem(), ManagedClusterArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*MetastoreConfigInput)(nil)).Elem(), MetastoreConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*MetastoreConfigPtrInput)(nil)).Elem(), MetastoreConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NamespacedGkeDeploymentTargetInput)(nil)).Elem(), NamespacedGkeDeploymentTargetArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NamespacedGkeDeploymentTargetPtrInput)(nil)).Elem(), NamespacedGkeDeploymentTargetArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodeGroupAffinityInput)(nil)).Elem(), NodeGroupAffinityArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodeGroupAffinityPtrInput)(nil)).Elem(), NodeGroupAffinityArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodeInitializationActionInput)(nil)).Elem(), NodeInitializationActionArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*NodeInitializationActionArrayInput)(nil)).Elem(), NodeInitializationActionArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*OrderedJobInput)(nil)).Elem(), OrderedJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*OrderedJobArrayInput)(nil)).Elem(), OrderedJobArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*ParameterValidationInput)(nil)).Elem(), ParameterValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ParameterValidationPtrInput)(nil)).Elem(), ParameterValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PeripheralsConfigInput)(nil)).Elem(), PeripheralsConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PeripheralsConfigPtrInput)(nil)).Elem(), PeripheralsConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PigJobInput)(nil)).Elem(), PigJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PigJobPtrInput)(nil)).Elem(), PigJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PrestoJobInput)(nil)).Elem(), PrestoJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PrestoJobPtrInput)(nil)).Elem(), PrestoJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PySparkBatchInput)(nil)).Elem(), PySparkBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PySparkBatchPtrInput)(nil)).Elem(), PySparkBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PySparkJobInput)(nil)).Elem(), PySparkJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*PySparkJobPtrInput)(nil)).Elem(), PySparkJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*QueryListInput)(nil)).Elem(), QueryListArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*QueryListPtrInput)(nil)).Elem(), QueryListArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*RegexValidationInput)(nil)).Elem(), RegexValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*RegexValidationPtrInput)(nil)).Elem(), RegexValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ReservationAffinityInput)(nil)).Elem(), ReservationAffinityArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ReservationAffinityPtrInput)(nil)).Elem(), ReservationAffinityArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*RuntimeConfigInput)(nil)).Elem(), RuntimeConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*RuntimeConfigPtrInput)(nil)).Elem(), RuntimeConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SecurityConfigInput)(nil)).Elem(), SecurityConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SecurityConfigPtrInput)(nil)).Elem(), SecurityConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ShieldedInstanceConfigInput)(nil)).Elem(), ShieldedInstanceConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ShieldedInstanceConfigPtrInput)(nil)).Elem(), ShieldedInstanceConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SoftwareConfigInput)(nil)).Elem(), SoftwareConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SoftwareConfigPtrInput)(nil)).Elem(), SoftwareConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkBatchInput)(nil)).Elem(), SparkBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkBatchPtrInput)(nil)).Elem(), SparkBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkHistoryServerConfigInput)(nil)).Elem(), SparkHistoryServerConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkHistoryServerConfigPtrInput)(nil)).Elem(), SparkHistoryServerConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobInput)(nil)).Elem(), SparkJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkJobPtrInput)(nil)).Elem(), SparkJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkRBatchInput)(nil)).Elem(), SparkRBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkRBatchPtrInput)(nil)).Elem(), SparkRBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkRJobInput)(nil)).Elem(), SparkRJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkRJobPtrInput)(nil)).Elem(), SparkRJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkSqlBatchInput)(nil)).Elem(), SparkSqlBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkSqlBatchPtrInput)(nil)).Elem(), SparkSqlBatchArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkSqlJobInput)(nil)).Elem(), SparkSqlJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkSqlJobPtrInput)(nil)).Elem(), SparkSqlJobArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkStandaloneAutoscalingConfigInput)(nil)).Elem(), SparkStandaloneAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*SparkStandaloneAutoscalingConfigPtrInput)(nil)).Elem(), SparkStandaloneAutoscalingConfigArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*TemplateParameterInput)(nil)).Elem(), TemplateParameterArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*TemplateParameterArrayInput)(nil)).Elem(), TemplateParameterArray{})
	pulumi.RegisterInputType(reflect.TypeOf((*ValueValidationInput)(nil)).Elem(), ValueValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*ValueValidationPtrInput)(nil)).Elem(), ValueValidationArgs{})
	pulumi.RegisterInputType(reflect.TypeOf((*WorkflowTemplatePlacementInput)(nil)).Elem(), WorkflowTemplatePlacementArgs{})
	pulumi.RegisterOutputType(AcceleratorConfigOutput{})
	pulumi.RegisterOutputType(AcceleratorConfigArrayOutput{})
	pulumi.RegisterOutputType(AcceleratorConfigResponseOutput{})
	pulumi.RegisterOutputType(AcceleratorConfigResponseArrayOutput{})
	pulumi.RegisterOutputType(AutoscalingConfigOutput{})
	pulumi.RegisterOutputType(AutoscalingConfigPtrOutput{})
	pulumi.RegisterOutputType(AutoscalingConfigResponseOutput{})
	pulumi.RegisterOutputType(BasicAutoscalingAlgorithmOutput{})
	pulumi.RegisterOutputType(BasicAutoscalingAlgorithmPtrOutput{})
	pulumi.RegisterOutputType(BasicAutoscalingAlgorithmResponseOutput{})
	pulumi.RegisterOutputType(BasicYarnAutoscalingConfigOutput{})
	pulumi.RegisterOutputType(BasicYarnAutoscalingConfigPtrOutput{})
	pulumi.RegisterOutputType(BasicYarnAutoscalingConfigResponseOutput{})
	pulumi.RegisterOutputType(BindingOutput{})
	pulumi.RegisterOutputType(BindingArrayOutput{})
	pulumi.RegisterOutputType(BindingResponseOutput{})
	pulumi.RegisterOutputType(BindingResponseArrayOutput{})
	pulumi.RegisterOutputType(ClusterConfigOutput{})
	pulumi.RegisterOutputType(ClusterConfigPtrOutput{})
	pulumi.RegisterOutputType(ClusterConfigResponseOutput{})
	pulumi.RegisterOutputType(ClusterMetricsResponseOutput{})
	pulumi.RegisterOutputType(ClusterSelectorOutput{})
	pulumi.RegisterOutputType(ClusterSelectorPtrOutput{})
	pulumi.RegisterOutputType(ClusterSelectorResponseOutput{})
	pulumi.RegisterOutputType(ClusterStatusResponseOutput{})
	pulumi.RegisterOutputType(ClusterStatusResponseArrayOutput{})
	pulumi.RegisterOutputType(ConfidentialInstanceConfigOutput{})
	pulumi.RegisterOutputType(ConfidentialInstanceConfigPtrOutput{})
	pulumi.RegisterOutputType(ConfidentialInstanceConfigResponseOutput{})
	pulumi.RegisterOutputType(DiskConfigOutput{})
	pulumi.RegisterOutputType(DiskConfigPtrOutput{})
	pulumi.RegisterOutputType(DiskConfigResponseOutput{})
	pulumi.RegisterOutputType(EncryptionConfigOutput{})
	pulumi.RegisterOutputType(EncryptionConfigPtrOutput{})
	pulumi.RegisterOutputType(EncryptionConfigResponseOutput{})
	pulumi.RegisterOutputType(EndpointConfigOutput{})
	pulumi.RegisterOutputType(EndpointConfigPtrOutput{})
	pulumi.RegisterOutputType(EndpointConfigResponseOutput{})
	pulumi.RegisterOutputType(EnvironmentConfigOutput{})
	pulumi.RegisterOutputType(EnvironmentConfigPtrOutput{})
	pulumi.RegisterOutputType(EnvironmentConfigResponseOutput{})
	pulumi.RegisterOutputType(ExecutionConfigOutput{})
	pulumi.RegisterOutputType(ExecutionConfigPtrOutput{})
	pulumi.RegisterOutputType(ExecutionConfigResponseOutput{})
	pulumi.RegisterOutputType(ExprOutput{})
	pulumi.RegisterOutputType(ExprPtrOutput{})
	pulumi.RegisterOutputType(ExprResponseOutput{})
	pulumi.RegisterOutputType(GceClusterConfigOutput{})
	pulumi.RegisterOutputType(GceClusterConfigPtrOutput{})
	pulumi.RegisterOutputType(GceClusterConfigResponseOutput{})
	pulumi.RegisterOutputType(GkeClusterConfigOutput{})
	pulumi.RegisterOutputType(GkeClusterConfigPtrOutput{})
	pulumi.RegisterOutputType(GkeClusterConfigResponseOutput{})
	pulumi.RegisterOutputType(HadoopJobOutput{})
	pulumi.RegisterOutputType(HadoopJobPtrOutput{})
	pulumi.RegisterOutputType(HadoopJobResponseOutput{})
	pulumi.RegisterOutputType(HiveJobOutput{})
	pulumi.RegisterOutputType(HiveJobPtrOutput{})
	pulumi.RegisterOutputType(HiveJobResponseOutput{})
	pulumi.RegisterOutputType(IdentityConfigOutput{})
	pulumi.RegisterOutputType(IdentityConfigPtrOutput{})
	pulumi.RegisterOutputType(IdentityConfigResponseOutput{})
	pulumi.RegisterOutputType(InstanceGroupAutoscalingPolicyConfigOutput{})
	pulumi.RegisterOutputType(InstanceGroupAutoscalingPolicyConfigPtrOutput{})
	pulumi.RegisterOutputType(InstanceGroupAutoscalingPolicyConfigResponseOutput{})
	pulumi.RegisterOutputType(InstanceGroupConfigOutput{})
	pulumi.RegisterOutputType(InstanceGroupConfigPtrOutput{})
	pulumi.RegisterOutputType(InstanceGroupConfigResponseOutput{})
	pulumi.RegisterOutputType(InstanceReferenceResponseOutput{})
	pulumi.RegisterOutputType(InstanceReferenceResponseArrayOutput{})
	pulumi.RegisterOutputType(JobPlacementOutput{})
	pulumi.RegisterOutputType(JobPlacementResponseOutput{})
	pulumi.RegisterOutputType(JobReferenceOutput{})
	pulumi.RegisterOutputType(JobReferencePtrOutput{})
	pulumi.RegisterOutputType(JobReferenceResponseOutput{})
	pulumi.RegisterOutputType(JobSchedulingOutput{})
	pulumi.RegisterOutputType(JobSchedulingPtrOutput{})
	pulumi.RegisterOutputType(JobSchedulingResponseOutput{})
	pulumi.RegisterOutputType(JobStatusResponseOutput{})
	pulumi.RegisterOutputType(JobStatusResponseArrayOutput{})
	pulumi.RegisterOutputType(KerberosConfigOutput{})
	pulumi.RegisterOutputType(KerberosConfigPtrOutput{})
	pulumi.RegisterOutputType(KerberosConfigResponseOutput{})
	pulumi.RegisterOutputType(LifecycleConfigOutput{})
	pulumi.RegisterOutputType(LifecycleConfigPtrOutput{})
	pulumi.RegisterOutputType(LifecycleConfigResponseOutput{})
	pulumi.RegisterOutputType(LoggingConfigOutput{})
	pulumi.RegisterOutputType(LoggingConfigPtrOutput{})
	pulumi.RegisterOutputType(LoggingConfigResponseOutput{})
	pulumi.RegisterOutputType(ManagedClusterOutput{})
	pulumi.RegisterOutputType(ManagedClusterPtrOutput{})
	pulumi.RegisterOutputType(ManagedClusterResponseOutput{})
	pulumi.RegisterOutputType(ManagedGroupConfigResponseOutput{})
	pulumi.RegisterOutputType(MetastoreConfigOutput{})
	pulumi.RegisterOutputType(MetastoreConfigPtrOutput{})
	pulumi.RegisterOutputType(MetastoreConfigResponseOutput{})
	pulumi.RegisterOutputType(NamespacedGkeDeploymentTargetOutput{})
	pulumi.RegisterOutputType(NamespacedGkeDeploymentTargetPtrOutput{})
	pulumi.RegisterOutputType(NamespacedGkeDeploymentTargetResponseOutput{})
	pulumi.RegisterOutputType(NodeGroupAffinityOutput{})
	pulumi.RegisterOutputType(NodeGroupAffinityPtrOutput{})
	pulumi.RegisterOutputType(NodeGroupAffinityResponseOutput{})
	pulumi.RegisterOutputType(NodeInitializationActionOutput{})
	pulumi.RegisterOutputType(NodeInitializationActionArrayOutput{})
	pulumi.RegisterOutputType(NodeInitializationActionResponseOutput{})
	pulumi.RegisterOutputType(NodeInitializationActionResponseArrayOutput{})
	pulumi.RegisterOutputType(OrderedJobOutput{})
	pulumi.RegisterOutputType(OrderedJobArrayOutput{})
	pulumi.RegisterOutputType(OrderedJobResponseOutput{})
	pulumi.RegisterOutputType(OrderedJobResponseArrayOutput{})
	pulumi.RegisterOutputType(ParameterValidationOutput{})
	pulumi.RegisterOutputType(ParameterValidationPtrOutput{})
	pulumi.RegisterOutputType(ParameterValidationResponseOutput{})
	pulumi.RegisterOutputType(PeripheralsConfigOutput{})
	pulumi.RegisterOutputType(PeripheralsConfigPtrOutput{})
	pulumi.RegisterOutputType(PeripheralsConfigResponseOutput{})
	pulumi.RegisterOutputType(PigJobOutput{})
	pulumi.RegisterOutputType(PigJobPtrOutput{})
	pulumi.RegisterOutputType(PigJobResponseOutput{})
	pulumi.RegisterOutputType(PrestoJobOutput{})
	pulumi.RegisterOutputType(PrestoJobPtrOutput{})
	pulumi.RegisterOutputType(PrestoJobResponseOutput{})
	pulumi.RegisterOutputType(PySparkBatchOutput{})
	pulumi.RegisterOutputType(PySparkBatchPtrOutput{})
	pulumi.RegisterOutputType(PySparkBatchResponseOutput{})
	pulumi.RegisterOutputType(PySparkJobOutput{})
	pulumi.RegisterOutputType(PySparkJobPtrOutput{})
	pulumi.RegisterOutputType(PySparkJobResponseOutput{})
	pulumi.RegisterOutputType(QueryListOutput{})
	pulumi.RegisterOutputType(QueryListPtrOutput{})
	pulumi.RegisterOutputType(QueryListResponseOutput{})
	pulumi.RegisterOutputType(RegexValidationOutput{})
	pulumi.RegisterOutputType(RegexValidationPtrOutput{})
	pulumi.RegisterOutputType(RegexValidationResponseOutput{})
	pulumi.RegisterOutputType(ReservationAffinityOutput{})
	pulumi.RegisterOutputType(ReservationAffinityPtrOutput{})
	pulumi.RegisterOutputType(ReservationAffinityResponseOutput{})
	pulumi.RegisterOutputType(RuntimeConfigOutput{})
	pulumi.RegisterOutputType(RuntimeConfigPtrOutput{})
	pulumi.RegisterOutputType(RuntimeConfigResponseOutput{})
	pulumi.RegisterOutputType(RuntimeInfoResponseOutput{})
	pulumi.RegisterOutputType(SecurityConfigOutput{})
	pulumi.RegisterOutputType(SecurityConfigPtrOutput{})
	pulumi.RegisterOutputType(SecurityConfigResponseOutput{})
	pulumi.RegisterOutputType(ShieldedInstanceConfigOutput{})
	pulumi.RegisterOutputType(ShieldedInstanceConfigPtrOutput{})
	pulumi.RegisterOutputType(ShieldedInstanceConfigResponseOutput{})
	pulumi.RegisterOutputType(SoftwareConfigOutput{})
	pulumi.RegisterOutputType(SoftwareConfigPtrOutput{})
	pulumi.RegisterOutputType(SoftwareConfigResponseOutput{})
	pulumi.RegisterOutputType(SparkBatchOutput{})
	pulumi.RegisterOutputType(SparkBatchPtrOutput{})
	pulumi.RegisterOutputType(SparkBatchResponseOutput{})
	pulumi.RegisterOutputType(SparkHistoryServerConfigOutput{})
	pulumi.RegisterOutputType(SparkHistoryServerConfigPtrOutput{})
	pulumi.RegisterOutputType(SparkHistoryServerConfigResponseOutput{})
	pulumi.RegisterOutputType(SparkJobOutput{})
	pulumi.RegisterOutputType(SparkJobPtrOutput{})
	pulumi.RegisterOutputType(SparkJobResponseOutput{})
	pulumi.RegisterOutputType(SparkRBatchOutput{})
	pulumi.RegisterOutputType(SparkRBatchPtrOutput{})
	pulumi.RegisterOutputType(SparkRBatchResponseOutput{})
	pulumi.RegisterOutputType(SparkRJobOutput{})
	pulumi.RegisterOutputType(SparkRJobPtrOutput{})
	pulumi.RegisterOutputType(SparkRJobResponseOutput{})
	pulumi.RegisterOutputType(SparkSqlBatchOutput{})
	pulumi.RegisterOutputType(SparkSqlBatchPtrOutput{})
	pulumi.RegisterOutputType(SparkSqlBatchResponseOutput{})
	pulumi.RegisterOutputType(SparkSqlJobOutput{})
	pulumi.RegisterOutputType(SparkSqlJobPtrOutput{})
	pulumi.RegisterOutputType(SparkSqlJobResponseOutput{})
	pulumi.RegisterOutputType(SparkStandaloneAutoscalingConfigOutput{})
	pulumi.RegisterOutputType(SparkStandaloneAutoscalingConfigPtrOutput{})
	pulumi.RegisterOutputType(SparkStandaloneAutoscalingConfigResponseOutput{})
	pulumi.RegisterOutputType(StateHistoryResponseOutput{})
	pulumi.RegisterOutputType(StateHistoryResponseArrayOutput{})
	pulumi.RegisterOutputType(TemplateParameterOutput{})
	pulumi.RegisterOutputType(TemplateParameterArrayOutput{})
	pulumi.RegisterOutputType(TemplateParameterResponseOutput{})
	pulumi.RegisterOutputType(TemplateParameterResponseArrayOutput{})
	pulumi.RegisterOutputType(ValueValidationOutput{})
	pulumi.RegisterOutputType(ValueValidationPtrOutput{})
	pulumi.RegisterOutputType(ValueValidationResponseOutput{})
	pulumi.RegisterOutputType(WorkflowTemplatePlacementOutput{})
	pulumi.RegisterOutputType(WorkflowTemplatePlacementResponseOutput{})
	pulumi.RegisterOutputType(YarnApplicationResponseOutput{})
	pulumi.RegisterOutputType(YarnApplicationResponseArrayOutput{})
}
