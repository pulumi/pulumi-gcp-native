// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../../../types/input";
import * as outputs from "../../../types/output";
import * as enums from "../../../types/enums";
import * as utilities from "../../../utilities";

/**
 * The environment values to be set at runtime for a Flex Template.
 */
export interface GoogleCloudDatapipelinesV1FlexTemplateRuntimeEnvironmentResponse {
    /**
     * Additional experiment flags for the job.
     */
    additionalExperiments: string[];
    /**
     * Additional user labels to be specified for the job. Keys and values must follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions). An object containing a list of key/value pairs. Example: `{ "name": "wrench", "mass": "1kg", "count": "3" }`.
     */
    additionalUserLabels: {[key: string]: string};
    /**
     * Whether to enable Streaming Engine for the job.
     */
    enableStreamingEngine: boolean;
    /**
     * Set FlexRS goal for the job. https://cloud.google.com/dataflow/docs/guides/flexrs
     */
    flexrsGoal: string;
    /**
     * Configuration for VM IPs.
     */
    ipConfiguration: string;
    /**
     * Name for the Cloud KMS key for the job. Key format is: projects//locations//keyRings//cryptoKeys/
     */
    kmsKeyName: string;
    /**
     * The machine type to use for the job. Defaults to the value from the template if not specified.
     */
    machineType: string;
    /**
     * The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000.
     */
    maxWorkers: number;
    /**
     * Network to which VMs will be assigned. If empty or unspecified, the service will use the network "default".
     */
    network: string;
    /**
     * The initial number of Compute Engine instances for the job.
     */
    numWorkers: number;
    /**
     * The email address of the service account to run the job as.
     */
    serviceAccountEmail: string;
    /**
     * Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form "https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK" or "regions/REGION/subnetworks/SUBNETWORK". If the subnetwork is located in a Shared VPC network, you must use the complete URL.
     */
    subnetwork: string;
    /**
     * The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with `gs://`.
     */
    tempLocation: string;
    /**
     * The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, defaults to the control plane region.
     */
    workerRegion: string;
    /**
     * The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1-a". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane region is chosen based on available capacity. If both `worker_zone` and `zone` are set, `worker_zone` takes precedence.
     */
    workerZone: string;
    /**
     * The Compute Engine [availability zone](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for launching worker instances to run your pipeline. In the future, worker_zone will take precedence.
     */
    zone: string;
}

/**
 * Launch Flex Template parameter.
 */
export interface GoogleCloudDatapipelinesV1LaunchFlexTemplateParameterResponse {
    /**
     * Cloud Storage path to a file with a JSON-serialized ContainerSpec as content.
     */
    containerSpecGcsPath: string;
    /**
     * The runtime environment for the Flex Template job.
     */
    environment: outputs.datapipelines.v1.GoogleCloudDatapipelinesV1FlexTemplateRuntimeEnvironmentResponse;
    /**
     * The job name to use for the created job. For an update job request, the job name should be the same as the existing running job.
     */
    jobName: string;
    /**
     * Launch options for this Flex Template job. This is a common set of options across languages and templates. This should not be used to pass job parameters.
     */
    launchOptions: {[key: string]: string};
    /**
     * The parameters for the Flex Template. Example: `{"num_workers":"5"}`
     */
    parameters: {[key: string]: string};
    /**
     * Use this to pass transform name mappings for streaming update jobs. Example: `{"oldTransformName":"newTransformName",...}`
     */
    transformNameMappings: {[key: string]: string};
    /**
     * Set this to true if you are sending a request to update a running streaming job. When set, the job name should be the same as the running job.
     */
    update: boolean;
}

/**
 * A request to launch a Dataflow job from a Flex Template.
 */
export interface GoogleCloudDatapipelinesV1LaunchFlexTemplateRequestResponse {
    /**
     * Parameter to launch a job from a Flex Template.
     */
    launchParameter: outputs.datapipelines.v1.GoogleCloudDatapipelinesV1LaunchFlexTemplateParameterResponse;
    /**
     * The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request. For example, `us-central1`, `us-west1`.
     */
    location: string;
    /**
     * The ID of the Cloud Platform project that the job belongs to.
     */
    project: string;
    /**
     * If true, the request is validated but not actually executed. Defaults to false.
     */
    validateOnly: boolean;
}

/**
 * Parameters to provide to the template being launched.
 */
export interface GoogleCloudDatapipelinesV1LaunchTemplateParametersResponse {
    /**
     * The runtime environment for the job.
     */
    environment: outputs.datapipelines.v1.GoogleCloudDatapipelinesV1RuntimeEnvironmentResponse;
    /**
     * The job name to use for the created job.
     */
    jobName: string;
    /**
     * The runtime parameters to pass to the job.
     */
    parameters: {[key: string]: string};
    /**
     * Map of transform name prefixes of the job to be replaced to the corresponding name prefixes of the new job. Only applicable when updating a pipeline.
     */
    transformNameMapping: {[key: string]: string};
    /**
     * If set, replace the existing pipeline with the name specified by jobName with this pipeline, preserving state.
     */
    update: boolean;
}

/**
 * A request to launch a template.
 */
export interface GoogleCloudDatapipelinesV1LaunchTemplateRequestResponse {
    /**
     * A Cloud Storage path to the template from which to create the job. Must be a valid Cloud Storage URL, beginning with 'gs://'.
     */
    gcsPath: string;
    /**
     * The parameters of the template to launch. This should be part of the body of the POST request.
     */
    launchParameters: outputs.datapipelines.v1.GoogleCloudDatapipelinesV1LaunchTemplateParametersResponse;
    /**
     * The [regional endpoint] (https://cloud.google.com/dataflow/docs/concepts/regional-endpoints) to which to direct the request.
     */
    location: string;
    /**
     * The ID of the Cloud Platform project that the job belongs to.
     */
    project: string;
    /**
     * If true, the request is validated but not actually executed. Defaults to false.
     */
    validateOnly: boolean;
}

/**
 * The environment values to set at runtime.
 */
export interface GoogleCloudDatapipelinesV1RuntimeEnvironmentResponse {
    /**
     * Additional experiment flags for the job.
     */
    additionalExperiments: string[];
    /**
     * Additional user labels to be specified for the job. Keys and values should follow the restrictions specified in the [labeling restrictions](https://cloud.google.com/compute/docs/labeling-resources#restrictions) page. An object containing a list of key/value pairs. Example: { "name": "wrench", "mass": "1kg", "count": "3" }.
     */
    additionalUserLabels: {[key: string]: string};
    /**
     * Whether to bypass the safety checks for the job's temporary directory. Use with caution.
     */
    bypassTempDirValidation: boolean;
    /**
     * Whether to enable Streaming Engine for the job.
     */
    enableStreamingEngine: boolean;
    /**
     * Configuration for VM IPs.
     */
    ipConfiguration: string;
    /**
     * Name for the Cloud KMS key for the job. The key format is: projects//locations//keyRings//cryptoKeys/
     */
    kmsKeyName: string;
    /**
     * The machine type to use for the job. Defaults to the value from the template if not specified.
     */
    machineType: string;
    /**
     * The maximum number of Compute Engine instances to be made available to your pipeline during execution, from 1 to 1000.
     */
    maxWorkers: number;
    /**
     * Network to which VMs will be assigned. If empty or unspecified, the service will use the network "default".
     */
    network: string;
    /**
     * The initial number of Compute Engine instances for the job.
     */
    numWorkers: number;
    /**
     * The email address of the service account to run the job as.
     */
    serviceAccountEmail: string;
    /**
     * Subnetwork to which VMs will be assigned, if desired. You can specify a subnetwork using either a complete URL or an abbreviated path. Expected to be of the form "https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK" or "regions/REGION/subnetworks/SUBNETWORK". If the subnetwork is located in a Shared VPC network, you must use the complete URL.
     */
    subnetwork: string;
    /**
     * The Cloud Storage path to use for temporary files. Must be a valid Cloud Storage URL, beginning with `gs://`.
     */
    tempLocation: string;
    /**
     * The Compute Engine region (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1". Mutually exclusive with worker_zone. If neither worker_region nor worker_zone is specified, default to the control plane's region.
     */
    workerRegion: string;
    /**
     * The Compute Engine zone (https://cloud.google.com/compute/docs/regions-zones/regions-zones) in which worker processing should occur, e.g. "us-west1-a". Mutually exclusive with worker_region. If neither worker_region nor worker_zone is specified, a zone in the control plane's region is chosen based on available capacity. If both `worker_zone` and `zone` are set, `worker_zone` takes precedence.
     */
    workerZone: string;
    /**
     * The Compute Engine [availability zone](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for launching worker instances to run your pipeline. In the future, worker_zone will take precedence.
     */
    zone: string;
}

/**
 * Details of the schedule the pipeline runs on.
 */
export interface GoogleCloudDatapipelinesV1ScheduleSpecResponse {
    /**
     * When the next Scheduler job is going to run.
     */
    nextJobTime: string;
    /**
     * Unix-cron format of the schedule. This information is retrieved from the linked Cloud Scheduler.
     */
    schedule: string;
    /**
     * Timezone ID. This matches the timezone IDs used by the Cloud Scheduler API. If empty, UTC time is assumed.
     */
    timeZone: string;
}

/**
 * Workload details for creating the pipeline jobs.
 */
export interface GoogleCloudDatapipelinesV1WorkloadResponse {
    /**
     * Template information and additional parameters needed to launch a Dataflow job using the flex launch API.
     */
    dataflowFlexTemplateRequest: outputs.datapipelines.v1.GoogleCloudDatapipelinesV1LaunchFlexTemplateRequestResponse;
    /**
     * Template information and additional parameters needed to launch a Dataflow job using the standard launch API.
     */
    dataflowLaunchTemplateRequest: outputs.datapipelines.v1.GoogleCloudDatapipelinesV1LaunchTemplateRequestResponse;
}

