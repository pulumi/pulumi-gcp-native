// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../../../types/input";
import * as outputs from "../../../types/output";
import * as enums from "../../../types/enums";
import * as utilities from "../../../utilities";

/**
 * Describe CSV and similar semi-structured data formats.
 */
export interface GoogleCloudDataplexV1AssetDiscoverySpecCsvOptionsResponse {
    /**
     * Optional. The delimiter being used to separate values. This defaults to ','.
     */
    delimiter: string;
    /**
     * Optional. Whether to disable the inference of data type for CSV data. If true, all columns will be registered as strings.
     */
    disableTypeInference: boolean;
    /**
     * Optional. The character encoding of the data. The default is UTF-8.
     */
    encoding: string;
    /**
     * Optional. The number of rows to interpret as header rows that should be skipped when reading data rows.
     */
    headerRows: number;
}

/**
 * Describe JSON data format.
 */
export interface GoogleCloudDataplexV1AssetDiscoverySpecJsonOptionsResponse {
    /**
     * Optional. Whether to disable the inference of data type for Json data. If true, all columns will be registered as their primitive types (strings, number or boolean).
     */
    disableTypeInference: boolean;
    /**
     * Optional. The character encoding of the data. The default is UTF-8.
     */
    encoding: string;
}

/**
 * Settings to manage the metadata discovery and publishing for an asset.
 */
export interface GoogleCloudDataplexV1AssetDiscoverySpecResponse {
    /**
     * Optional. Configuration for CSV data.
     */
    csvOptions: outputs.dataplex.v1.GoogleCloudDataplexV1AssetDiscoverySpecCsvOptionsResponse;
    /**
     * Optional. Whether discovery is enabled.
     */
    enabled: boolean;
    /**
     * Optional. The list of patterns to apply for selecting data to exclude during discovery. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
     */
    excludePatterns: string[];
    /**
     * Optional. The list of patterns to apply for selecting data to include during discovery if only a subset of the data should considered. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
     */
    includePatterns: string[];
    /**
     * Optional. Configuration for Json data.
     */
    jsonOptions: outputs.dataplex.v1.GoogleCloudDataplexV1AssetDiscoverySpecJsonOptionsResponse;
    /**
     * Optional. Cron schedule (https://en.wikipedia.org/wiki/Cron) for running discovery periodically. Successive discovery runs must be scheduled at least 60 minutes apart. The default value is to run discovery every 60 minutes. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
     */
    schedule: string;
}

/**
 * Status of discovery for an asset.
 */
export interface GoogleCloudDataplexV1AssetDiscoveryStatusResponse {
    /**
     * The duration of the last discovery run.
     */
    lastRunDuration: string;
    /**
     * The start time of the last discovery run.
     */
    lastRunTime: string;
    /**
     * Additional information about the current state.
     */
    message: string;
    /**
     * The current status of the discovery feature.
     */
    state: string;
    /**
     * Data Stats of the asset reported by discovery.
     */
    stats: outputs.dataplex.v1.GoogleCloudDataplexV1AssetDiscoveryStatusStatsResponse;
    /**
     * Last update time of the status.
     */
    updateTime: string;
}

/**
 * The aggregated data statistics for the asset reported by discovery.
 */
export interface GoogleCloudDataplexV1AssetDiscoveryStatusStatsResponse {
    /**
     * The count of data items within the referenced resource.
     */
    dataItems: string;
    /**
     * The number of stored data bytes within the referenced resource.
     */
    dataSize: string;
    /**
     * The count of fileset entities within the referenced resource.
     */
    filesets: string;
    /**
     * The count of table entities within the referenced resource.
     */
    tables: string;
}

/**
 * Identifies the cloud resource that is referenced by this asset.
 */
export interface GoogleCloudDataplexV1AssetResourceSpecResponse {
    /**
     * Immutable. Relative name of the cloud resource that contains the data that is being managed within a lake. For example: projects/{project_number}/buckets/{bucket_id} projects/{project_number}/datasets/{dataset_id}
     */
    name: string;
    /**
     * Immutable. Type of resource.
     */
    type: string;
}

/**
 * Status of the resource referenced by an asset.
 */
export interface GoogleCloudDataplexV1AssetResourceStatusResponse {
    /**
     * Additional information about the current state.
     */
    message: string;
    /**
     * The current state of the managed resource.
     */
    state: string;
    /**
     * Last update time of the status.
     */
    updateTime: string;
}

/**
 * Security policy status of the asset. Data security policy, i.e., readers, writers & owners, should be specified in the lake/zone/asset IAM policy.
 */
export interface GoogleCloudDataplexV1AssetSecurityStatusResponse {
    /**
     * Additional information about the current state.
     */
    message: string;
    /**
     * The current state of the security policy applied to the attached resource.
     */
    state: string;
    /**
     * Last update time of the status.
     */
    updateTime: string;
}

/**
 * Aggregated status of the underlying assets of a lake or zone.
 */
export interface GoogleCloudDataplexV1AssetStatusResponse {
    /**
     * Number of active assets.
     */
    activeAssets: number;
    /**
     * Number of assets that are in process of updating the security policy on attached resources.
     */
    securityPolicyApplyingAssets: number;
    /**
     * Last update time of the status.
     */
    updateTime: string;
}

/**
 * Configuration for Notebook content.
 */
export interface GoogleCloudDataplexV1ContentNotebookResponse {
    /**
     * Kernel Type of the notebook.
     */
    kernelType: string;
}

/**
 * Configuration for the Sql Script content.
 */
export interface GoogleCloudDataplexV1ContentSqlScriptResponse {
    /**
     * Query Engine to be used for the Sql Query.
     */
    engine: string;
}

/**
 * Provides compatibility information for a specific metadata store.
 */
export interface GoogleCloudDataplexV1EntityCompatibilityStatusCompatibilityResponse {
    /**
     * Whether the entity is compatible and can be represented in the metadata store.
     */
    compatible: boolean;
    /**
     * Provides additional detail if the entity is incompatible with the metadata store.
     */
    reason: string;
}

/**
 * Provides compatibility information for various metadata stores.
 */
export interface GoogleCloudDataplexV1EntityCompatibilityStatusResponse {
    /**
     * Whether this entity is compatible with BigQuery.
     */
    bigquery: outputs.dataplex.v1.GoogleCloudDataplexV1EntityCompatibilityStatusCompatibilityResponse;
    /**
     * Whether this entity is compatible with Hive Metastore.
     */
    hiveMetastore: outputs.dataplex.v1.GoogleCloudDataplexV1EntityCompatibilityStatusCompatibilityResponse;
}

export interface GoogleCloudDataplexV1EnvironmentEndpointsResponse {
    /**
     * URI to serve notebook APIs
     */
    notebooks: string;
    /**
     * URI to serve SQL APIs
     */
    sql: string;
}

/**
 * Compute resources associated with the analyze interactive workloads.
 */
export interface GoogleCloudDataplexV1EnvironmentInfrastructureSpecComputeResourcesResponse {
    /**
     * Optional. Size in GB of the disk. Default is 100 GB.
     */
    diskSizeGb: number;
    /**
     * Optional. Max configurable nodes. If max_node_count > node_count, then auto-scaling is enabled.
     */
    maxNodeCount: number;
    /**
     * Optional. Total number of nodes in the sessions created for this environment.
     */
    nodeCount: number;
}

/**
 * Software Runtime Configuration to run Analyze.
 */
export interface GoogleCloudDataplexV1EnvironmentInfrastructureSpecOsImageRuntimeResponse {
    /**
     * Dataplex Image version.
     */
    imageVersion: string;
    /**
     * Optional. List of Java jars to be included in the runtime environment. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar
     */
    javaLibraries: string[];
    /**
     * Optional. Spark properties to provide configuration for use in sessions created for this environment. The properties to set on daemon config files. Property keys are specified in prefix:property format. The prefix must be "spark".
     */
    properties: {[key: string]: string};
    /**
     * Optional. A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz
     */
    pythonPackages: string[];
}

/**
 * Configuration for the underlying infrastructure used to run workloads.
 */
export interface GoogleCloudDataplexV1EnvironmentInfrastructureSpecResponse {
    /**
     * Optional. Compute resources needed for analyze interactive workloads.
     */
    compute: outputs.dataplex.v1.GoogleCloudDataplexV1EnvironmentInfrastructureSpecComputeResourcesResponse;
    /**
     * Software Runtime Configuration for analyze interactive workloads.
     */
    osImage: outputs.dataplex.v1.GoogleCloudDataplexV1EnvironmentInfrastructureSpecOsImageRuntimeResponse;
}

export interface GoogleCloudDataplexV1EnvironmentSessionSpecResponse {
    /**
     * Optional. If True, this causes sessions to be pre-created and available for faster startup to enable interactive exploration use-cases. This defaults to False to avoid additional billed charges. These can only be set to True for the environment with name set to "default", and with default configuration.
     */
    enableFastStartup: boolean;
    /**
     * Optional. The idle time configuration of the session. The session will be auto-terminated at the end of this period.
     */
    maxIdleDuration: string;
}

export interface GoogleCloudDataplexV1EnvironmentSessionStatusResponse {
    /**
     * Queries over sessions to mark whether the environment is currently active or not
     */
    active: boolean;
}

/**
 * A job represents an instance of a task.
 */
export interface GoogleCloudDataplexV1JobResponse {
    /**
     * The time when the job ended.
     */
    endTime: string;
    /**
     * Additional information about the current state.
     */
    message: string;
    /**
     * The relative resource name of the job, of the form: projects/{project_number}/locations/{location_id}/lakes/{lake_id}/tasks/{task_id}/jobs/{job_id}.
     */
    name: string;
    /**
     * The number of times the job has been retried (excluding the initial attempt).
     */
    retryCount: number;
    /**
     * The underlying service running a job.
     */
    service: string;
    /**
     * The full resource name for the job run under a particular service.
     */
    serviceJob: string;
    /**
     * The time when the job was started.
     */
    startTime: string;
    /**
     * Execution state for the job.
     */
    state: string;
    /**
     * System generated globally unique ID for the job.
     */
    uid: string;
}

/**
 * Settings to manage association of Dataproc Metastore with a lake.
 */
export interface GoogleCloudDataplexV1LakeMetastoreResponse {
    /**
     * Optional. A relative reference to the Dataproc Metastore (https://cloud.google.com/dataproc-metastore/docs) service associated with the lake: projects/{project_id}/locations/{location_id}/services/{service_id}
     */
    service: string;
}

/**
 * Status of Lake and Dataproc Metastore service instance association.
 */
export interface GoogleCloudDataplexV1LakeMetastoreStatusResponse {
    /**
     * The URI of the endpoint used to access the Metastore service.
     */
    endpoint: string;
    /**
     * Additional information about the current status.
     */
    message: string;
    /**
     * Current state of association.
     */
    state: string;
    /**
     * Last update time of the metastore status of the lake.
     */
    updateTime: string;
}

/**
 * Represents a key field within the entity's partition structure. You could have up to 20 partition fields, but only the first 10 partitions have the filtering ability due to performance consideration. Note: Partition fields are immutable.
 */
export interface GoogleCloudDataplexV1SchemaPartitionFieldResponse {
    /**
     * Partition field name must consist of letters, numbers, and underscores only, with a maximum of length of 256 characters, and must begin with a letter or underscore..
     */
    name: string;
    /**
     * Immutable. The type of field.
     */
    type: string;
}

/**
 * Schema information describing the structure and layout of the data.
 */
export interface GoogleCloudDataplexV1SchemaResponse {
    /**
     * Optional. The sequence of fields describing data in table entities. Note: BigQuery SchemaFields are immutable.
     */
    fields: outputs.dataplex.v1.GoogleCloudDataplexV1SchemaSchemaFieldResponse[];
    /**
     * Optional. The sequence of fields describing the partition structure in entities. If this field is empty, there are no partitions within the data.
     */
    partitionFields: outputs.dataplex.v1.GoogleCloudDataplexV1SchemaPartitionFieldResponse[];
    /**
     * Optional. The structure of paths containing partition data within the entity.
     */
    partitionStyle: string;
    /**
     * Set to true if user-managed or false if managed by Dataplex. The default is false (managed by Dataplex). Set to falseto enable Dataplex discovery to update the schema. including new data discovery, schema inference, and schema evolution. Users retain the ability to input and edit the schema. Dataplex treats schema input by the user as though produced by a previous Dataplex discovery operation, and it will evolve the schema and take action based on that treatment. Set to true to fully manage the entity schema. This setting guarantees that Dataplex will not change schema fields.
     */
    userManaged: boolean;
}

/**
 * Represents a column field within a table schema.
 */
export interface GoogleCloudDataplexV1SchemaSchemaFieldResponse {
    /**
     * Optional. User friendly field description. Must be less than or equal to 1024 characters.
     */
    description: string;
    /**
     * Optional. Any nested field for complex types.
     */
    fields: outputs.dataplex.v1.GoogleCloudDataplexV1SchemaSchemaFieldResponse[];
    /**
     * Additional field semantics.
     */
    mode: string;
    /**
     * The name of the field. Must contain only letters, numbers and underscores, with a maximum length of 767 characters, and must begin with a letter or underscore.
     */
    name: string;
    /**
     * The type of field.
     */
    type: string;
}

/**
 * Describes CSV and similar semi-structured data formats.
 */
export interface GoogleCloudDataplexV1StorageFormatCsvOptionsResponse {
    /**
     * Optional. The delimiter used to separate values. Defaults to ','.
     */
    delimiter: string;
    /**
     * Optional. The character encoding of the data. Accepts "US-ASCII", "UTF-8", and "ISO-8859-1". Defaults to UTF-8 if unspecified.
     */
    encoding: string;
    /**
     * Optional. The number of rows to interpret as header rows that should be skipped when reading data rows. Defaults to 0.
     */
    headerRows: number;
    /**
     * Optional. The character used to quote column values. Accepts '"' (double quotation mark) or ''' (single quotation mark). Defaults to '"' (double quotation mark) if unspecified.
     */
    quote: string;
}

/**
 * Describes JSON data format.
 */
export interface GoogleCloudDataplexV1StorageFormatJsonOptionsResponse {
    /**
     * Optional. The character encoding of the data. Accepts "US-ASCII", "UTF-8" and "ISO-8859-1". Defaults to UTF-8 if not specified.
     */
    encoding: string;
}

/**
 * Describes the format of the data within its storage location.
 */
export interface GoogleCloudDataplexV1StorageFormatResponse {
    /**
     * Optional. The compression type associated with the stored data. If unspecified, the data is uncompressed.
     */
    compressionFormat: string;
    /**
     * Optional. Additional information about CSV formatted data.
     */
    csv: outputs.dataplex.v1.GoogleCloudDataplexV1StorageFormatCsvOptionsResponse;
    /**
     * The data format associated with the stored data, which represents content type values. The value is inferred from mime type.
     */
    format: string;
    /**
     * Optional. Additional information about CSV formatted data.
     */
    json: outputs.dataplex.v1.GoogleCloudDataplexV1StorageFormatJsonOptionsResponse;
    /**
     * The mime type descriptor for the data. Must match the pattern {type}/{subtype}. Supported values: application/x-parquet application/x-avro application/x-orc application/x-tfrecord application/json application/{subtypes} text/csv text/ image/{image subtype} video/{video subtype} audio/{audio subtype}
     */
    mimeType: string;
}

/**
 * Execution related settings, like retry and service_account.
 */
export interface GoogleCloudDataplexV1TaskExecutionSpecResponse {
    /**
     * Optional. The arguments to pass to the task. The args can use placeholders of the format ${placeholder} as part of key/value string. These will be interpolated before passing the args to the driver. Currently supported placeholders: - ${task_id} - ${job_time} To pass positional args, set the key as TASK_ARGS. The value should be a comma-separated string of all the positional arguments. To use a delimiter other than comma, refer to https://cloud.google.com/sdk/gcloud/reference/topic/escaping. In case of other keys being present in the args, then TASK_ARGS will be passed as the last argument.
     */
    args: {[key: string]: string};
    /**
     * Optional. The Cloud KMS key to use for encryption, of the form: projects/{project_number}/locations/{location_id}/keyRings/{key-ring-name}/cryptoKeys/{key-name}.
     */
    kmsKey: string;
    /**
     * Optional. The maximum duration after which the job execution is expired.
     */
    maxJobExecutionLifetime: string;
    /**
     * Optional. The project in which jobs are run. By default, the project containing the Lake is used. If a project is provided, the ExecutionSpec.service_account must belong to this project.
     */
    project: string;
    /**
     * Service account to use to execute a task. If not provided, the default Compute service account for the project is used.
     */
    serviceAccount: string;
}

/**
 * Status of the task execution (e.g. Jobs).
 */
export interface GoogleCloudDataplexV1TaskExecutionStatusResponse {
    /**
     * latest job execution
     */
    latestJob: outputs.dataplex.v1.GoogleCloudDataplexV1JobResponse;
    /**
     * Last update time of the status.
     */
    updateTime: string;
}

/**
 * Batch compute resources associated with the task.
 */
export interface GoogleCloudDataplexV1TaskInfrastructureSpecBatchComputeResourcesResponse {
    /**
     * Optional. Total number of job executors. Executor Count should be between 2 and 100. Default=2
     */
    executorsCount: number;
    /**
     * Optional. Max configurable executors. If max_executors_count > executors_count, then auto-scaling is enabled. Max Executor Count should be between 2 and 1000. Default=1000
     */
    maxExecutorsCount: number;
}

/**
 * Container Image Runtime Configuration used with Batch execution.
 */
export interface GoogleCloudDataplexV1TaskInfrastructureSpecContainerImageRuntimeResponse {
    /**
     * Optional. Container image to use.
     */
    image: string;
    /**
     * Optional. A list of Java JARS to add to the classpath. Valid input includes Cloud Storage URIs to Jar binaries. For example, gs://bucket-name/my/path/to/file.jar
     */
    javaJars: string[];
    /**
     * Optional. Override to common configuration of open source components installed on the Dataproc cluster. The properties to set on daemon config files. Property keys are specified in prefix:property format, for example core:hadoop.tmp.dir. For more information, see Cluster properties (https://cloud.google.com/dataproc/docs/concepts/cluster-properties).
     */
    properties: {[key: string]: string};
    /**
     * Optional. A list of python packages to be installed. Valid formats include Cloud Storage URI to a PIP installable library. For example, gs://bucket-name/my/path/to/lib.tar.gz
     */
    pythonPackages: string[];
}

/**
 * Configuration for the underlying infrastructure used to run workloads.
 */
export interface GoogleCloudDataplexV1TaskInfrastructureSpecResponse {
    /**
     * Compute resources needed for a Task when using Dataproc Serverless.
     */
    batch: outputs.dataplex.v1.GoogleCloudDataplexV1TaskInfrastructureSpecBatchComputeResourcesResponse;
    /**
     * Container Image Runtime Configuration.
     */
    containerImage: outputs.dataplex.v1.GoogleCloudDataplexV1TaskInfrastructureSpecContainerImageRuntimeResponse;
    /**
     * Vpc network.
     */
    vpcNetwork: outputs.dataplex.v1.GoogleCloudDataplexV1TaskInfrastructureSpecVpcNetworkResponse;
}

/**
 * Cloud VPC Network used to run the infrastructure.
 */
export interface GoogleCloudDataplexV1TaskInfrastructureSpecVpcNetworkResponse {
    /**
     * Optional. The Cloud VPC network in which the job is run. By default, the Cloud VPC network named Default within the project is used.
     */
    network: string;
    /**
     * Optional. List of network tags to apply to the job.
     */
    networkTags: string[];
    /**
     * Optional. The Cloud VPC sub-network in which the job is run.
     */
    subNetwork: string;
}

/**
 * Config for running scheduled notebooks.
 */
export interface GoogleCloudDataplexV1TaskNotebookTaskConfigResponse {
    /**
     * Optional. Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
     */
    archiveUris: string[];
    /**
     * Optional. Cloud Storage URIs of files to be placed in the working directory of each executor.
     */
    fileUris: string[];
    /**
     * Optional. Infrastructure specification for the execution.
     */
    infrastructureSpec: outputs.dataplex.v1.GoogleCloudDataplexV1TaskInfrastructureSpecResponse;
    /**
     * Path to input notebook. This can be the Cloud Storage URI of the notebook file or the path to a Notebook Content. The execution args are accessible as environment variables (TASK_key=value).
     */
    notebook: string;
}

/**
 * User-specified config for running a Spark task.
 */
export interface GoogleCloudDataplexV1TaskSparkTaskConfigResponse {
    /**
     * Optional. Cloud Storage URIs of archives to be extracted into the working directory of each executor. Supported file types: .jar, .tar, .tar.gz, .tgz, and .zip.
     */
    archiveUris: string[];
    /**
     * Optional. Cloud Storage URIs of files to be placed in the working directory of each executor.
     */
    fileUris: string[];
    /**
     * Optional. Infrastructure specification for the execution.
     */
    infrastructureSpec: outputs.dataplex.v1.GoogleCloudDataplexV1TaskInfrastructureSpecResponse;
    /**
     * The name of the driver's main class. The jar file that contains the class must be in the default CLASSPATH or specified in jar_file_uris. The execution args are passed in as a sequence of named process arguments (--key=value).
     */
    mainClass: string;
    /**
     * The Cloud Storage URI of the jar file that contains the main class. The execution args are passed in as a sequence of named process arguments (--key=value).
     */
    mainJarFileUri: string;
    /**
     * The Gcloud Storage URI of the main Python file to use as the driver. Must be a .py file. The execution args are passed in as a sequence of named process arguments (--key=value).
     */
    pythonScriptFile: string;
    /**
     * The query text. The execution args are used to declare a set of script variables (set key="value";).
     */
    sqlScript: string;
    /**
     * A reference to a query file. This can be the Cloud Storage URI of the query file or it can the path to a SqlScript Content. The execution args are used to declare a set of script variables (set key="value";).
     */
    sqlScriptFile: string;
}

/**
 * Task scheduling and trigger settings.
 */
export interface GoogleCloudDataplexV1TaskTriggerSpecResponse {
    /**
     * Optional. Prevent the task from executing. This does not cancel already running tasks. It is intended to temporarily disable RECURRING tasks.
     */
    disabled: boolean;
    /**
     * Optional. Number of retry attempts before aborting. Set to zero to never attempt to retry a failed task.
     */
    maxRetries: number;
    /**
     * Optional. Cron schedule (https://en.wikipedia.org/wiki/Cron) for running tasks periodically. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or "TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *". This field is required for RECURRING tasks.
     */
    schedule: string;
    /**
     * Optional. The first run of the task will be after this time. If not specified, the task will run shortly after being submitted if ON_DEMAND and based on the schedule if RECURRING.
     */
    startTime: string;
    /**
     * Immutable. Trigger type of the user-specified Task.
     */
    type: string;
}

/**
 * Describe CSV and similar semi-structured data formats.
 */
export interface GoogleCloudDataplexV1ZoneDiscoverySpecCsvOptionsResponse {
    /**
     * Optional. The delimiter being used to separate values. This defaults to ','.
     */
    delimiter: string;
    /**
     * Optional. Whether to disable the inference of data type for CSV data. If true, all columns will be registered as strings.
     */
    disableTypeInference: boolean;
    /**
     * Optional. The character encoding of the data. The default is UTF-8.
     */
    encoding: string;
    /**
     * Optional. The number of rows to interpret as header rows that should be skipped when reading data rows.
     */
    headerRows: number;
}

/**
 * Describe JSON data format.
 */
export interface GoogleCloudDataplexV1ZoneDiscoverySpecJsonOptionsResponse {
    /**
     * Optional. Whether to disable the inference of data type for Json data. If true, all columns will be registered as their primitive types (strings, number or boolean).
     */
    disableTypeInference: boolean;
    /**
     * Optional. The character encoding of the data. The default is UTF-8.
     */
    encoding: string;
}

/**
 * Settings to manage the metadata discovery and publishing in a zone.
 */
export interface GoogleCloudDataplexV1ZoneDiscoverySpecResponse {
    /**
     * Optional. Configuration for CSV data.
     */
    csvOptions: outputs.dataplex.v1.GoogleCloudDataplexV1ZoneDiscoverySpecCsvOptionsResponse;
    /**
     * Whether discovery is enabled.
     */
    enabled: boolean;
    /**
     * Optional. The list of patterns to apply for selecting data to exclude during discovery. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
     */
    excludePatterns: string[];
    /**
     * Optional. The list of patterns to apply for selecting data to include during discovery if only a subset of the data should considered. For Cloud Storage bucket assets, these are interpreted as glob patterns used to match object names. For BigQuery dataset assets, these are interpreted as patterns to match table names.
     */
    includePatterns: string[];
    /**
     * Optional. Configuration for Json data.
     */
    jsonOptions: outputs.dataplex.v1.GoogleCloudDataplexV1ZoneDiscoverySpecJsonOptionsResponse;
    /**
     * Optional. Cron schedule (https://en.wikipedia.org/wiki/Cron) for running discovery periodically. Successive discovery runs must be scheduled at least 60 minutes apart. The default value is to run discovery every 60 minutes. To explicitly set a timezone to the cron tab, apply a prefix in the cron tab: "CRON_TZ=${IANA_TIME_ZONE}" or TZ=${IANA_TIME_ZONE}". The ${IANA_TIME_ZONE} may only be a valid string from IANA time zone database. For example, "CRON_TZ=America/New_York 1 * * * *", or "TZ=America/New_York 1 * * * *".
     */
    schedule: string;
}

/**
 * Settings for resources attached as assets within a zone.
 */
export interface GoogleCloudDataplexV1ZoneResourceSpecResponse {
    /**
     * Immutable. The location type of the resources that are allowed to be attached to the assets within this zone.
     */
    locationType: string;
}

/**
 * Specifies the audit configuration for a service. The configuration determines which permission types are logged, and what identities, if any, are exempted from logging. An AuditConfig must have one or more AuditLogConfigs.If there are AuditConfigs for both allServices and a specific service, the union of the two AuditConfigs is used for that service: the log_types specified in each AuditConfig are enabled, and the exempted_members in each AuditLogConfig are exempted.Example Policy with multiple AuditConfigs: { "audit_configs": [ { "service": "allServices", "audit_log_configs": [ { "log_type": "DATA_READ", "exempted_members": [ "user:jose@example.com" ] }, { "log_type": "DATA_WRITE" }, { "log_type": "ADMIN_READ" } ] }, { "service": "sampleservice.googleapis.com", "audit_log_configs": [ { "log_type": "DATA_READ" }, { "log_type": "DATA_WRITE", "exempted_members": [ "user:aliya@example.com" ] } ] } ] } For sampleservice, this policy enables DATA_READ, DATA_WRITE and ADMIN_READ logging. It also exempts jose@example.com from DATA_READ logging, and aliya@example.com from DATA_WRITE logging.
 */
export interface GoogleIamV1AuditConfigResponse {
    /**
     * The configuration for logging of each type of permission.
     */
    auditLogConfigs: outputs.dataplex.v1.GoogleIamV1AuditLogConfigResponse[];
    /**
     * Specifies a service that will be enabled for audit logging. For example, storage.googleapis.com, cloudsql.googleapis.com. allServices is a special value that covers all services.
     */
    service: string;
}

/**
 * Provides the configuration for logging a type of permissions. Example: { "audit_log_configs": [ { "log_type": "DATA_READ", "exempted_members": [ "user:jose@example.com" ] }, { "log_type": "DATA_WRITE" } ] } This enables 'DATA_READ' and 'DATA_WRITE' logging, while exempting jose@example.com from DATA_READ logging.
 */
export interface GoogleIamV1AuditLogConfigResponse {
    /**
     * Specifies the identities that do not cause logging for this type of permission. Follows the same format of Binding.members.
     */
    exemptedMembers: string[];
    /**
     * The log type that this config enables.
     */
    logType: string;
}

/**
 * Associates members, or principals, with a role.
 */
export interface GoogleIamV1BindingResponse {
    /**
     * The condition that is associated with this binding.If the condition evaluates to true, then this binding applies to the current request.If the condition evaluates to false, then this binding does not apply to the current request. However, a different role binding might grant the same role to one or more of the principals in this binding.To learn which resources support conditions in their IAM policies, see the IAM documentation (https://cloud.google.com/iam/help/conditions/resource-policies).
     */
    condition: outputs.dataplex.v1.GoogleTypeExprResponse;
    /**
     * Specifies the principals requesting access for a Google Cloud resource. members can have the following values: allUsers: A special identifier that represents anyone who is on the internet; with or without a Google account. allAuthenticatedUsers: A special identifier that represents anyone who is authenticated with a Google account or a service account. Does not include identities that come from external identity providers (IdPs) through identity federation. user:{emailid}: An email address that represents a specific Google account. For example, alice@example.com . serviceAccount:{emailid}: An email address that represents a Google service account. For example, my-other-app@appspot.gserviceaccount.com. serviceAccount:{projectid}.svc.id.goog[{namespace}/{kubernetes-sa}]: An identifier for a Kubernetes service account (https://cloud.google.com/kubernetes-engine/docs/how-to/kubernetes-service-accounts). For example, my-project.svc.id.goog[my-namespace/my-kubernetes-sa]. group:{emailid}: An email address that represents a Google group. For example, admins@example.com. deleted:user:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a user that has been recently deleted. For example, alice@example.com?uid=123456789012345678901. If the user is recovered, this value reverts to user:{emailid} and the recovered user retains the role in the binding. deleted:serviceAccount:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a service account that has been recently deleted. For example, my-other-app@appspot.gserviceaccount.com?uid=123456789012345678901. If the service account is undeleted, this value reverts to serviceAccount:{emailid} and the undeleted service account retains the role in the binding. deleted:group:{emailid}?uid={uniqueid}: An email address (plus unique identifier) representing a Google group that has been recently deleted. For example, admins@example.com?uid=123456789012345678901. If the group is recovered, this value reverts to group:{emailid} and the recovered group retains the role in the binding. domain:{domain}: The G Suite domain (primary) that represents all the users of that domain. For example, google.com or example.com.
     */
    members: string[];
    /**
     * Role that is assigned to the list of members, or principals. For example, roles/viewer, roles/editor, or roles/owner.
     */
    role: string;
}

/**
 * Represents a textual expression in the Common Expression Language (CEL) syntax. CEL is a C-like expression language. The syntax and semantics of CEL are documented at https://github.com/google/cel-spec.Example (Comparison): title: "Summary size limit" description: "Determines if a summary is less than 100 chars" expression: "document.summary.size() < 100" Example (Equality): title: "Requestor is owner" description: "Determines if requestor is the document owner" expression: "document.owner == request.auth.claims.email" Example (Logic): title: "Public documents" description: "Determine whether the document should be publicly visible" expression: "document.type != 'private' && document.type != 'internal'" Example (Data Manipulation): title: "Notification string" description: "Create a notification string with a timestamp." expression: "'New message received at ' + string(document.create_time)" The exact variables and functions that may be referenced within an expression are determined by the service that evaluates it. See the service documentation for additional information.
 */
export interface GoogleTypeExprResponse {
    /**
     * Optional. Description of the expression. This is a longer text which describes the expression, e.g. when hovered over it in a UI.
     */
    description: string;
    /**
     * Textual representation of an expression in Common Expression Language syntax.
     */
    expression: string;
    /**
     * Optional. String indicating the location of the expression for error reporting, e.g. a file name and a position in the file.
     */
    location: string;
    /**
     * Optional. Title for the expression, i.e. a short string describing its purpose. This can be used e.g. in UIs which allow to enter the expression.
     */
    title: string;
}

