// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../../../types/input";
import * as outputs from "../../../types/output";
import * as enums from "../../../types/enums";
import * as utilities from "../../../utilities";

/**
 * Container of information related to one possible annotation that can be used in a labeling task. For example, an image classification task where images are labeled as `dog` or `cat` must reference an AnnotationSpec for `dog` and an AnnotationSpec for `cat`.
 */
export interface GoogleCloudDatalabelingV1beta1AnnotationSpecArgs {
    /**
     * Optional. User-provided description of the annotation specification. The description can be up to 10,000 characters long.
     */
    description?: pulumi.Input<string>;
    /**
     * The display name of the AnnotationSpec. Maximum of 64 characters.
     */
    displayName: pulumi.Input<string>;
}

/**
 * The BigQuery location for input data. If used in an EvaluationJob, this is where the service saves the prediction input and output sampled from the model version.
 */
export interface GoogleCloudDatalabelingV1beta1BigQuerySourceArgs {
    /**
     * BigQuery URI to a table, up to 2,000 characters long. If you specify the URI of a table that does not exist, Data Labeling Service creates a table at the URI with the correct schema when you create your EvaluationJob. If you specify the URI of a table that already exists, it must have the [correct schema](/ml-engine/docs/continuous-evaluation/create-job#table-schema). Provide the table URI in the following format: "bq://{your_project_id}/ {your_dataset_name}/{your_table_name}" [Learn more](/ml-engine/docs/continuous-evaluation/create-job#table-schema).
     */
    inputUri: pulumi.Input<string>;
}

/**
 * Options regarding evaluation between bounding boxes.
 */
export interface GoogleCloudDatalabelingV1beta1BoundingBoxEvaluationOptionsArgs {
    /**
     * Minimum [intersection-over-union (IOU)](/vision/automl/object-detection/docs/evaluate#intersection-over-union) required for 2 bounding boxes to be considered a match. This must be a number between 0 and 1.
     */
    iouThreshold?: pulumi.Input<number>;
}

/**
 * Config for image bounding poly (and bounding box) human labeling task.
 */
export interface GoogleCloudDatalabelingV1beta1BoundingPolyConfigArgs {
    /**
     * Annotation spec set resource name.
     */
    annotationSpecSet: pulumi.Input<string>;
    /**
     * Optional. Instruction message showed on contributors UI.
     */
    instructionMessage?: pulumi.Input<string>;
}

/**
 * Metadata for classification annotations.
 */
export interface GoogleCloudDatalabelingV1beta1ClassificationMetadataArgs {
    /**
     * Whether the classification task is multi-label or not.
     */
    isMultiLabel?: pulumi.Input<boolean>;
}

/**
 * Deprecated: this instruction format is not supported any more. Instruction from a CSV file.
 */
export interface GoogleCloudDatalabelingV1beta1CsvInstructionArgs {
    /**
     * CSV file for the instruction. Only gcs path is allowed.
     */
    gcsFileUri?: pulumi.Input<string>;
}

/**
 * Configuration details used for calculating evaluation metrics and creating an Evaluation.
 */
export interface GoogleCloudDatalabelingV1beta1EvaluationConfigArgs {
    /**
     * Only specify this field if the related model performs image object detection (`IMAGE_BOUNDING_BOX_ANNOTATION`). Describes how to evaluate bounding boxes.
     */
    boundingBoxEvaluationOptions?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1BoundingBoxEvaluationOptionsArgs>;
}

/**
 * Provides details for how an evaluation job sends email alerts based on the results of a run.
 */
export interface GoogleCloudDatalabelingV1beta1EvaluationJobAlertConfigArgs {
    /**
     * An email address to send alerts to.
     */
    email: pulumi.Input<string>;
    /**
     * A number between 0 and 1 that describes a minimum mean average precision threshold. When the evaluation job runs, if it calculates that your model version's predictions from the recent interval have meanAveragePrecision below this threshold, then it sends an alert to your specified email.
     */
    minAcceptableMeanAveragePrecision: pulumi.Input<number>;
}

/**
 * Configures specific details of how a continuous evaluation job works. Provide this configuration when you create an EvaluationJob.
 */
export interface GoogleCloudDatalabelingV1beta1EvaluationJobConfigArgs {
    /**
     * Prediction keys that tell Data Labeling Service where to find the data for evaluation in your BigQuery table. When the service samples prediction input and output from your model version and saves it to BigQuery, the data gets stored as JSON strings in the BigQuery table. These keys tell Data Labeling Service how to parse the JSON. You can provide the following entries in this field: * `data_json_key`: the data key for prediction input. You must provide either this key or `reference_json_key`. * `reference_json_key`: the data reference key for prediction input. You must provide either this key or `data_json_key`. * `label_json_key`: the label key for prediction output. Required. * `label_score_json_key`: the score key for prediction output. Required. * `bounding_box_json_key`: the bounding box key for prediction output. Required if your model version perform image object detection. Learn [how to configure prediction keys](/ml-engine/docs/continuous-evaluation/create-job#prediction-keys).
     */
    bigqueryImportKeys: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * Specify this field if your model version performs image object detection (bounding box detection). `annotationSpecSet` in this configuration must match EvaluationJob.annotationSpecSet.
     */
    boundingPolyConfig?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1BoundingPolyConfigArgs>;
    /**
     * Details for calculating evaluation metrics and creating Evaulations. If your model version performs image object detection, you must specify the `boundingBoxEvaluationOptions` field within this configuration. Otherwise, provide an empty object for this configuration.
     */
    evaluationConfig: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1EvaluationConfigArgs>;
    /**
     * Optional. Configuration details for evaluation job alerts. Specify this field if you want to receive email alerts if the evaluation job finds that your predictions have low mean average precision during a run.
     */
    evaluationJobAlertConfig?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1EvaluationJobAlertConfigArgs>;
    /**
     * The maximum number of predictions to sample and save to BigQuery during each evaluation interval. This limit overrides `example_sample_percentage`: even if the service has not sampled enough predictions to fulfill `example_sample_perecentage` during an interval, it stops sampling predictions when it meets this limit.
     */
    exampleCount: pulumi.Input<number>;
    /**
     * Fraction of predictions to sample and save to BigQuery during each evaluation interval. For example, 0.1 means 10% of predictions served by your model version get saved to BigQuery.
     */
    exampleSamplePercentage: pulumi.Input<number>;
    /**
     * Optional. Details for human annotation of your data. If you set labelMissingGroundTruth to `true` for this evaluation job, then you must specify this field. If you plan to provide your own ground truth labels, then omit this field. Note that you must create an Instruction resource before you can specify this field. Provide the name of the instruction resource in the `instruction` field within this configuration.
     */
    humanAnnotationConfig?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1HumanAnnotationConfigArgs>;
    /**
     * Specify this field if your model version performs image classification or general classification. `annotationSpecSet` in this configuration must match EvaluationJob.annotationSpecSet. `allowMultiLabel` in this configuration must match `classificationMetadata.isMultiLabel` in input_config.
     */
    imageClassificationConfig?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1ImageClassificationConfigArgs>;
    /**
     * Rquired. Details for the sampled prediction input. Within this configuration, there are requirements for several fields: * `dataType` must be one of `IMAGE`, `TEXT`, or `GENERAL_DATA`. * `annotationType` must be one of `IMAGE_CLASSIFICATION_ANNOTATION`, `TEXT_CLASSIFICATION_ANNOTATION`, `GENERAL_CLASSIFICATION_ANNOTATION`, or `IMAGE_BOUNDING_BOX_ANNOTATION` (image object detection). * If your machine learning model performs classification, you must specify `classificationMetadata.isMultiLabel`. * You must specify `bigquerySource` (not `gcsSource`).
     */
    inputConfig?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1InputConfigArgs>;
    /**
     * Specify this field if your model version performs text classification. `annotationSpecSet` in this configuration must match EvaluationJob.annotationSpecSet. `allowMultiLabel` in this configuration must match `classificationMetadata.isMultiLabel` in input_config.
     */
    textClassificationConfig?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1TextClassificationConfigArgs>;
}

/**
 * Source of the Cloud Storage file to be imported.
 */
export interface GoogleCloudDatalabelingV1beta1GcsSourceArgs {
    /**
     * The input URI of source file. This must be a Cloud Storage path (`gs://...`).
     */
    inputUri: pulumi.Input<string>;
    /**
     * The format of the source file. Only "text/csv" is supported.
     */
    mimeType: pulumi.Input<string>;
}

/**
 * Configuration for how human labeling task should be done.
 */
export interface GoogleCloudDatalabelingV1beta1HumanAnnotationConfigArgs {
    /**
     * Optional. A human-readable description for AnnotatedDataset. The description can be up to 10000 characters long.
     */
    annotatedDatasetDescription?: pulumi.Input<string>;
    /**
     * A human-readable name for AnnotatedDataset defined by users. Maximum of 64 characters .
     */
    annotatedDatasetDisplayName: pulumi.Input<string>;
    /**
     * Optional. If you want your own labeling contributors to manage and work on this labeling request, you can set these contributors here. We will give them access to the question types in crowdcompute. Note that these emails must be registered in crowdcompute worker UI: https://crowd-compute.appspot.com/
     */
    contributorEmails?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Instruction resource name.
     */
    instruction: pulumi.Input<string>;
    /**
     * Optional. A human-readable label used to logically group labeling tasks. This string must match the regular expression `[a-zA-Z\\d_-]{0,128}`.
     */
    labelGroup?: pulumi.Input<string>;
    /**
     * Optional. The Language of this question, as a [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt). Default value is en-US. Only need to set this when task is language related. For example, French text classification.
     */
    languageCode?: pulumi.Input<string>;
    /**
     * Optional. Maximum duration for contributors to answer a question. Maximum is 3600 seconds. Default is 3600 seconds.
     */
    questionDuration?: pulumi.Input<string>;
    /**
     * Optional. Replication of questions. Each question will be sent to up to this number of contributors to label. Aggregated answers will be returned. Default is set to 1. For image related labeling, valid values are 1, 3, 5.
     */
    replicaCount?: pulumi.Input<number>;
    /**
     * Email of the user who started the labeling task and should be notified by email. If empty no notification will be sent.
     */
    userEmailAddress?: pulumi.Input<string>;
}

/**
 * Config for image classification human labeling task.
 */
export interface GoogleCloudDatalabelingV1beta1ImageClassificationConfigArgs {
    /**
     * Optional. If allow_multi_label is true, contributors are able to choose multiple labels for one image.
     */
    allowMultiLabel?: pulumi.Input<boolean>;
    /**
     * Annotation spec set resource name.
     */
    annotationSpecSet: pulumi.Input<string>;
    /**
     * Optional. The type of how to aggregate answers.
     */
    answerAggregationType?: pulumi.Input<enums.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1ImageClassificationConfigAnswerAggregationType>;
}

/**
 * The configuration of input data, including data type, location, etc.
 */
export interface GoogleCloudDatalabelingV1beta1InputConfigArgs {
    /**
     * Optional. The type of annotation to be performed on this data. You must specify this field if you are using this InputConfig in an EvaluationJob.
     */
    annotationType?: pulumi.Input<enums.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1InputConfigAnnotationType>;
    /**
     * Source located in BigQuery. You must specify this field if you are using this InputConfig in an EvaluationJob.
     */
    bigquerySource?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1BigQuerySourceArgs>;
    /**
     * Optional. Metadata about annotations for the input. You must specify this field if you are using this InputConfig in an EvaluationJob for a model version that performs classification.
     */
    classificationMetadata?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1ClassificationMetadataArgs>;
    /**
     * Data type must be specifed when user tries to import data.
     */
    dataType: pulumi.Input<enums.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1InputConfigDataType>;
    /**
     * Source located in Cloud Storage.
     */
    gcsSource?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1GcsSourceArgs>;
    /**
     * Required for text import, as language code must be specified.
     */
    textMetadata?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1TextMetadataArgs>;
}

/**
 * Metadata describing the feedback from the operator.
 */
export interface GoogleCloudDatalabelingV1beta1OperatorFeedbackMetadataArgs {
}

/**
 * Instruction from a PDF file.
 */
export interface GoogleCloudDatalabelingV1beta1PdfInstructionArgs {
    /**
     * PDF file for the instruction. Only gcs path is allowed.
     */
    gcsFileUri?: pulumi.Input<string>;
}

/**
 * Metadata describing the feedback from the labeling task requester.
 */
export interface GoogleCloudDatalabelingV1beta1RequesterFeedbackMetadataArgs {
}

/**
 * Config for setting up sentiments.
 */
export interface GoogleCloudDatalabelingV1beta1SentimentConfigArgs {
    /**
     * If set to true, contributors will have the option to select sentiment of the label they selected, to mark it as negative or positive label. Default is false.
     */
    enableLabelSentimentSelection?: pulumi.Input<boolean>;
}

/**
 * Config for text classification human labeling task.
 */
export interface GoogleCloudDatalabelingV1beta1TextClassificationConfigArgs {
    /**
     * Optional. If allow_multi_label is true, contributors are able to choose multiple labels for one text segment.
     */
    allowMultiLabel?: pulumi.Input<boolean>;
    /**
     * Annotation spec set resource name.
     */
    annotationSpecSet: pulumi.Input<string>;
    /**
     * Optional. Configs for sentiment selection. We deprecate sentiment analysis in data labeling side as it is incompatible with uCAIP.
     */
    sentimentConfig?: pulumi.Input<inputs.datalabeling.v1beta1.GoogleCloudDatalabelingV1beta1SentimentConfigArgs>;
}

/**
 * Metadata for the text.
 */
export interface GoogleCloudDatalabelingV1beta1TextMetadataArgs {
    /**
     * The language of this text, as a [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt). Default value is en-US.
     */
    languageCode?: pulumi.Input<string>;
}

