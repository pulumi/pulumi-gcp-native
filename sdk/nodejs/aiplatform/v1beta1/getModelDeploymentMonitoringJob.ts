// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../../types/input";
import * as outputs from "../../types/output";
import * as enums from "../../types/enums";
import * as utilities from "../../utilities";

/**
 * Gets a ModelDeploymentMonitoringJob.
 */
export function getModelDeploymentMonitoringJob(args: GetModelDeploymentMonitoringJobArgs, opts?: pulumi.InvokeOptions): Promise<GetModelDeploymentMonitoringJobResult> {

    opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts || {});
    return pulumi.runtime.invoke("google-native:aiplatform/v1beta1:getModelDeploymentMonitoringJob", {
        "location": args.location,
        "modelDeploymentMonitoringJobId": args.modelDeploymentMonitoringJobId,
        "project": args.project,
    }, opts);
}

export interface GetModelDeploymentMonitoringJobArgs {
    location: string;
    modelDeploymentMonitoringJobId: string;
    project?: string;
}

export interface GetModelDeploymentMonitoringJobResult {
    /**
     * YAML schema file uri describing the format of a single instance that you want Tensorflow Data Validation (TFDV) to analyze. If this field is empty, all the feature data types are inferred from predict_instance_schema_uri, meaning that TFDV will use the data in the exact format(data type) as prediction request/response. If there are any data type differences between predict instance and TFDV instance, this field can be used to override the schema. For models trained with Vertex AI, this field must be set as all the fields in predict instance formatted as string.
     */
    readonly analysisInstanceSchemaUri: string;
    /**
     * The created bigquery tables for the job under customer project. Customer could do their own query & analysis. There could be 4 log tables in maximum: 1. Training data logging predict request/response 2. Serving data logging predict request/response
     */
    readonly bigqueryTables: outputs.aiplatform.v1beta1.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringBigQueryTableResponse[];
    /**
     * Timestamp when this ModelDeploymentMonitoringJob was created.
     */
    readonly createTime: string;
    /**
     * The user-defined name of the ModelDeploymentMonitoringJob. The name can be up to 128 characters long and can consist of any UTF-8 characters. Display name of a ModelDeploymentMonitoringJob.
     */
    readonly displayName: string;
    /**
     * If true, the scheduled monitoring pipeline logs are sent to Google Cloud Logging, including pipeline status and anomalies detected. Please note the logs incur cost, which are subject to [Cloud Logging pricing](https://cloud.google.com/logging#pricing).
     */
    readonly enableMonitoringPipelineLogs: boolean;
    /**
     * Customer-managed encryption key spec for a ModelDeploymentMonitoringJob. If set, this ModelDeploymentMonitoringJob and all sub-resources of this ModelDeploymentMonitoringJob will be secured by this key.
     */
    readonly encryptionSpec: outputs.aiplatform.v1beta1.GoogleCloudAiplatformV1beta1EncryptionSpecResponse;
    /**
     * Endpoint resource name. Format: `projects/{project}/locations/{location}/endpoints/{endpoint}`
     */
    readonly endpoint: string;
    /**
     * Only populated when the job's state is `JOB_STATE_FAILED` or `JOB_STATE_CANCELLED`.
     */
    readonly error: outputs.aiplatform.v1beta1.GoogleRpcStatusResponse;
    /**
     * The labels with user-defined metadata to organize your ModelDeploymentMonitoringJob. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
     */
    readonly labels: {[key: string]: string};
    /**
     * Latest triggered monitoring pipeline metadata.
     */
    readonly latestMonitoringPipelineMetadata: outputs.aiplatform.v1beta1.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringJobLatestMonitoringPipelineMetadataResponse;
    /**
     * The TTL of BigQuery tables in user projects which stores logs. A day is the basic unit of the TTL and we take the ceil of TTL/86400(a day). e.g. { second: 3600} indicates ttl = 1 day.
     */
    readonly logTtl: string;
    /**
     * Sample Strategy for logging.
     */
    readonly loggingSamplingStrategy: outputs.aiplatform.v1beta1.GoogleCloudAiplatformV1beta1SamplingStrategyResponse;
    /**
     * The config for monitoring objectives. This is a per DeployedModel config. Each DeployedModel needs to be configured separately.
     */
    readonly modelDeploymentMonitoringObjectiveConfigs: outputs.aiplatform.v1beta1.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringObjectiveConfigResponse[];
    /**
     * Schedule config for running the monitoring job.
     */
    readonly modelDeploymentMonitoringScheduleConfig: outputs.aiplatform.v1beta1.GoogleCloudAiplatformV1beta1ModelDeploymentMonitoringScheduleConfigResponse;
    /**
     * Alert config for model monitoring.
     */
    readonly modelMonitoringAlertConfig: outputs.aiplatform.v1beta1.GoogleCloudAiplatformV1beta1ModelMonitoringAlertConfigResponse;
    /**
     * Resource name of a ModelDeploymentMonitoringJob.
     */
    readonly name: string;
    /**
     * Timestamp when this monitoring pipeline will be scheduled to run for the next round.
     */
    readonly nextScheduleTime: string;
    /**
     * YAML schema file uri describing the format of a single instance, which are given to format this Endpoint's prediction (and explanation). If not set, we will generate predict schema from collected predict requests.
     */
    readonly predictInstanceSchemaUri: string;
    /**
     * Sample Predict instance, same format as PredictRequest.instances, this can be set as a replacement of ModelDeploymentMonitoringJob.predict_instance_schema_uri. If not set, we will generate predict schema from collected predict requests.
     */
    readonly samplePredictInstance: any;
    /**
     * Schedule state when the monitoring job is in Running state.
     */
    readonly scheduleState: string;
    /**
     * The detailed state of the monitoring job. When the job is still creating, the state will be 'PENDING'. Once the job is successfully created, the state will be 'RUNNING'. Pause the job, the state will be 'PAUSED'. Resume the job, the state will return to 'RUNNING'.
     */
    readonly state: string;
    /**
     * Stats anomalies base folder path.
     */
    readonly statsAnomaliesBaseDirectory: outputs.aiplatform.v1beta1.GoogleCloudAiplatformV1beta1GcsDestinationResponse;
    /**
     * Timestamp when this ModelDeploymentMonitoringJob was updated most recently.
     */
    readonly updateTime: string;
}
/**
 * Gets a ModelDeploymentMonitoringJob.
 */
export function getModelDeploymentMonitoringJobOutput(args: GetModelDeploymentMonitoringJobOutputArgs, opts?: pulumi.InvokeOptions): pulumi.Output<GetModelDeploymentMonitoringJobResult> {
    return pulumi.output(args).apply((a: any) => getModelDeploymentMonitoringJob(a, opts))
}

export interface GetModelDeploymentMonitoringJobOutputArgs {
    location: pulumi.Input<string>;
    modelDeploymentMonitoringJobId: pulumi.Input<string>;
    project?: pulumi.Input<string>;
}
