// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.GoogleNative.Dataproc.V1.Inputs
{

    /// <summary>
    /// Basic autoscaling configurations for Spark Standalone.
    /// </summary>
    public sealed class SparkStandaloneAutoscalingConfigArgs : Pulumi.ResourceArgs
    {
        /// <summary>
        /// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decomissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
        /// </summary>
        [Input("gracefulDecommissionTimeout", required: true)]
        public Input<string> GracefulDecommissionTimeout { get; set; } = null!;

        /// <summary>
        /// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
        /// </summary>
        [Input("scaleDownFactor", required: true)]
        public Input<double> ScaleDownFactor { get; set; } = null!;

        /// <summary>
        /// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
        /// </summary>
        [Input("scaleDownMinWorkerFraction")]
        public Input<double>? ScaleDownMinWorkerFraction { get; set; }

        /// <summary>
        /// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
        /// </summary>
        [Input("scaleUpFactor", required: true)]
        public Input<double> ScaleUpFactor { get; set; } = null!;

        /// <summary>
        /// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
        /// </summary>
        [Input("scaleUpMinWorkerFraction")]
        public Input<double>? ScaleUpMinWorkerFraction { get; set; }

        public SparkStandaloneAutoscalingConfigArgs()
        {
        }
    }
}
