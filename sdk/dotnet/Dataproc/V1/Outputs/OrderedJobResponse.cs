// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.GoogleNative.Dataproc.V1.Outputs
{

    /// <summary>
    /// A job executed by the workflow.
    /// </summary>
    [OutputType]
    public sealed class OrderedJobResponse
    {
        /// <summary>
        /// Optional. Job is a Hadoop job.
        /// </summary>
        public readonly Outputs.HadoopJobResponse HadoopJob;
        /// <summary>
        /// Optional. Job is a Hive job.
        /// </summary>
        public readonly Outputs.HiveJobResponse HiveJob;
        /// <summary>
        /// Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
        /// </summary>
        public readonly ImmutableDictionary<string, string> Labels;
        /// <summary>
        /// Optional. Job is a Pig job.
        /// </summary>
        public readonly Outputs.PigJobResponse PigJob;
        /// <summary>
        /// Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
        /// </summary>
        public readonly ImmutableArray<string> PrerequisiteStepIds;
        /// <summary>
        /// Optional. Job is a Presto job.
        /// </summary>
        public readonly Outputs.PrestoJobResponse PrestoJob;
        /// <summary>
        /// Optional. Job is a PySpark job.
        /// </summary>
        public readonly Outputs.PySparkJobResponse PysparkJob;
        /// <summary>
        /// Optional. Job scheduling configuration.
        /// </summary>
        public readonly Outputs.JobSchedulingResponse Scheduling;
        /// <summary>
        /// Optional. Job is a Spark job.
        /// </summary>
        public readonly Outputs.SparkJobResponse SparkJob;
        /// <summary>
        /// Optional. Job is a SparkR job.
        /// </summary>
        public readonly Outputs.SparkRJobResponse SparkRJob;
        /// <summary>
        /// Optional. Job is a SparkSql job.
        /// </summary>
        public readonly Outputs.SparkSqlJobResponse SparkSqlJob;
        /// <summary>
        /// The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
        /// </summary>
        public readonly string StepId;
        /// <summary>
        /// Optional. Job is a Trino job.
        /// </summary>
        public readonly Outputs.TrinoJobResponse TrinoJob;

        [OutputConstructor]
        private OrderedJobResponse(
            Outputs.HadoopJobResponse hadoopJob,

            Outputs.HiveJobResponse hiveJob,

            ImmutableDictionary<string, string> labels,

            Outputs.PigJobResponse pigJob,

            ImmutableArray<string> prerequisiteStepIds,

            Outputs.PrestoJobResponse prestoJob,

            Outputs.PySparkJobResponse pysparkJob,

            Outputs.JobSchedulingResponse scheduling,

            Outputs.SparkJobResponse sparkJob,

            Outputs.SparkRJobResponse sparkRJob,

            Outputs.SparkSqlJobResponse sparkSqlJob,

            string stepId,

            Outputs.TrinoJobResponse trinoJob)
        {
            HadoopJob = hadoopJob;
            HiveJob = hiveJob;
            Labels = labels;
            PigJob = pigJob;
            PrerequisiteStepIds = prerequisiteStepIds;
            PrestoJob = prestoJob;
            PysparkJob = pysparkJob;
            Scheduling = scheduling;
            SparkJob = sparkJob;
            SparkRJob = sparkRJob;
            SparkSqlJob = sparkSqlJob;
            StepId = stepId;
            TrinoJob = trinoJob;
        }
    }
}
