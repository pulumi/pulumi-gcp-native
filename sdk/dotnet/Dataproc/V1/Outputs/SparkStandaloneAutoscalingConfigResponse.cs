// *** WARNING: this file was generated by the Pulumi SDK Generator. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

using System;
using System.Collections.Generic;
using System.Collections.Immutable;
using System.Threading.Tasks;
using Pulumi.Serialization;

namespace Pulumi.GoogleNative.Dataproc.V1.Outputs
{

    /// <summary>
    /// Basic autoscaling configurations for Spark Standalone.
    /// </summary>
    [OutputType]
    public sealed class SparkStandaloneAutoscalingConfigResponse
    {
        /// <summary>
        /// Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decommissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
        /// </summary>
        public readonly string GracefulDecommissionTimeout;
        /// <summary>
        /// Optional. Remove only idle workers when scaling down cluster
        /// </summary>
        public readonly bool RemoveOnlyIdleWorkers;
        /// <summary>
        /// Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
        /// </summary>
        public readonly double ScaleDownFactor;
        /// <summary>
        /// Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
        /// </summary>
        public readonly double ScaleDownMinWorkerFraction;
        /// <summary>
        /// Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
        /// </summary>
        public readonly double ScaleUpFactor;
        /// <summary>
        /// Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
        /// </summary>
        public readonly double ScaleUpMinWorkerFraction;

        [OutputConstructor]
        private SparkStandaloneAutoscalingConfigResponse(
            string gracefulDecommissionTimeout,

            bool removeOnlyIdleWorkers,

            double scaleDownFactor,

            double scaleDownMinWorkerFraction,

            double scaleUpFactor,

            double scaleUpMinWorkerFraction)
        {
            GracefulDecommissionTimeout = gracefulDecommissionTimeout;
            RemoveOnlyIdleWorkers = removeOnlyIdleWorkers;
            ScaleDownFactor = scaleDownFactor;
            ScaleDownMinWorkerFraction = scaleDownMinWorkerFraction;
            ScaleUpFactor = scaleUpFactor;
            ScaleUpMinWorkerFraction = scaleUpMinWorkerFraction;
        }
    }
}
